
@article{seidel_metadata_2012,
	title = {Metadata {Management} in {Scientific} {Computing}},
	volume = {3},
	url = {http://shodor.org/media/content/jocse/volume3/issue2/seidel_2012},
	abstract = {Complex scientific codes and the datasets they generate are in need of a sophisticated categorization environment that allows the community to store, search, and enhance metadata in an open, dynamic system. Currently, data is often presented in a read-only format, distilled and curated by a select group of researchers. We envision a more open and dynamic system, where authors can publish their data in a writeable format, allowing users to annotate the datasets with their own comments and data. This would enable the scientific community to collaborate on a higher level than before, where researchers could for example annotate a published dataset with their citations. Such a system would require a complete set of permissions to ensure that any individual's data cannot be altered by others unless they specifically allow it. For this reason datasets and codes are generally presented read-only, to protect the author's data; however, this also prevents the type of social revolutions that the private sector has seen with Facebook and Twitter. In this paper, we present an alternative method of publishing codes and datasets, based on Fluidinfo, which is an openly writeable and social metadata engine. We will use the specific example of the Einstein Toolkit, a shared scientific code built using the Cactus Framework, to illustrate how the code's metadata may be published in writeable form via Fluidinfo.},
	number = {2},
	urldate = {2015-01-26},
	journal = {Journal of Computational Science Education},
	author = {Seidel, Eric L.},
	month = dec,
	year = {2012},
	keywords = {Computer Science - Digital Libraries},
	file = {seidel_2012_metadata_management_in_scientific_computing.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/BE9FMWU2/seidel_2012_metadata_management_in_scientific_computing.pdf:application/pdf}
}

@incollection{gast_explaining_2005,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Explaining {ML} {Type} {Errors} by {Data} {Flows}},
	copyright = {©2005 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-26094-3 978-3-540-32038-8},
	url = {http://link.springer.com/chapter/10.1007/11431664_5},
	abstract = {We present a novel approach to explaining ML type errors: Since the type system inhibits data flows that would abort the program at run-time, our type checker identifies as explanations those data flows that violate the typing rules. It also detects the notorious backflows, which are artifacts of unification, and warns the user about the possibly unexpected typing. The generated explanations comprise a detailed textual description and an arrow overlay to the source code, in which each arrowrepresents one data flow. The description refers only to elementary facts about program evaluation, not to the type checking process itself. The method integrates well with unification-based type checking: Type-correct programs incur a modest overhead compared to normal type checking. If a type error occurs, a simple depth-first graph traversal yields the explanation. A proof-of-concept implementation is available.},
	language = {en},
	number = {3474},
	urldate = {2015-05-27},
	booktitle = {Implementation and {Application} of {Functional} {Languages}},
	publisher = {Springer Berlin Heidelberg},
	author = {Gast, Holger},
	editor = {Grelck, Clemens and Huch, Frank and Michaelson, Greg J. and Trinder, Phil},
	year = {2005},
	keywords = {Logics and Meanings of Programs, Programming Languages, Compilers, Interpreters, Programming Techniques},
	pages = {72--89},
	file = {gast_2005_explaining_ml_type_errors_by_data_flows.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/NIV2GECF/gast_2005_explaining_ml_type_errors_by_data_flows.pdf:application/pdf}
}

@inproceedings{jhala_path_2005,
	address = {New York, NY, USA},
	series = {{PLDI} '05},
	title = {Path {Slicing}},
	isbn = {1-59593-056-6},
	url = {http://doi.acm.org/10.1145/1065010.1065016},
	doi = {10.1145/1065010.1065016},
	abstract = {We present a new technique, path slicing, that takes as input a possibly infeasible path to a target location, and eliminates all the operations that are irrelevant towards the reachability of the target location. A path slice is a subsequence of the original path whose infeasibility guarantees the infeasibility of the original path, and whose feasibility guarantees the existence of some feasible variant of the given path that reaches the target location even though the given path may itself be infeasible. Our method combines the ability of program slicing to look at several program paths, with the precision that dynamic slicing enjoys by focusing on a single path. We have implemented Path Slicing to analyze possible counterexamples returned by the software model checker Blast. We show its effectiveness in drastically reducing the size of the counterexamples to less than 1\% of their original size. This enables the precise verification of application programs (upto 100KLOC), by allowing the analysis to focus on the part of the counterexample that is relevant to the property being checked.},
	urldate = {2015-01-24},
	booktitle = {Proceedings of the 2005 {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Jhala, Ranjit and Majumdar, Rupak},
	year = {2005},
	keywords = {counterexample analysis, program slicing},
	pages = {38--47},
	file = {jhala_majumdar_2005_path_slicing.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/VP8QFA53/jhala_majumdar_2005_path_slicing.pdf:application/pdf}
}

@inproceedings{pottier_hindley-milner_2014,
	address = {New York, NY, USA},
	series = {{ICFP} '14},
	title = {Hindley-milner {Elaboration} in {Applicative} {Style}: {Functional} {Pearl}},
	isbn = {978-1-4503-2873-9},
	shorttitle = {Hindley-milner {Elaboration} in {Applicative} {Style}},
	url = {http://doi.acm.org/10.1145/2628136.2628145},
	doi = {10.1145/2628136.2628145},
	abstract = {Type inference - the problem of determining whether a program is well-typed - is well-understood. In contrast, elaboration - the task of constructing an explicitly-typed representation of the program - seems to have received relatively little attention, even though, in a non-local type inference system, it is non-trivial. We show that the constraint-based presentation of Hindley-Milner type inference can be extended to deal with elaboration, while preserving its elegance. This involves introducing a new notion of "constraint with a value", which forms an applicative functor.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 19th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Pottier, François},
	year = {2014},
	keywords = {constraints, elaboration, polymorphism, type inference},
	pages = {203--212},
	file = {pottier_2014_hindley-milner_elaboration_in_applicative_style.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/AJD92TTT/pottier_2014_hindley-milner_elaboration_in_applicative_style.pdf:application/pdf}
}

@inproceedings{visser_test_2004,
	address = {New York, NY, USA},
	series = {{ISSTA} '04},
	title = {Test {Input} {Generation} with {Java} {PathFinder}},
	isbn = {1-58113-820-2},
	url = {http://doi.acm.org/10.1145/1007512.1007526},
	doi = {10.1145/1007512.1007526},
	abstract = {We show how model checking and symbolic execution can be used to generate test inputs to achieve structural coverage of code that manipulates complex data structures. We focus on obtaining branch-coverage during unit testing of some of the core methods of the red-black tree implementation in the Java TreeMap library, using the Java PathFinder model checker. Three different test generation techniques will be introduced and compared, namely, straight model checking of the code, model checking used in a black-box fashion to generate all inputs up to a fixed size, and lastly, model checking used during white-box test input generation. The main contribution of this work is to show how efficient white-box test input generation can be done for code manipulating complex data, taking into account complex method preconditions.},
	urldate = {2015-01-23},
	booktitle = {Proceedings of the 2004 {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Visser, Willem and Pǎsǎreanu, Corina S. and Khurshid, Sarfraz},
	year = {2004},
	keywords = {coverage, model checking, red-black trees, symbolic execution, testing object-oriented programs},
	pages = {97--107},
	file = {visser_et_al_2004_test_input_generation_with_java_pathfinder.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/HQVUM56P/visser_et_al_2004_test_input_generation_with_java_pathfinder.pdf:application/pdf}
}

@inproceedings{sankaranarayanan_static_2013,
	address = {New York, NY, USA},
	series = {{PLDI} '13},
	title = {Static {Analysis} for {Probabilistic} {Programs}: {Inferring} {Whole} {Program} {Properties} from {Finitely} {Many} {Paths}},
	isbn = {978-1-4503-2014-6},
	shorttitle = {Static {Analysis} for {Probabilistic} {Programs}},
	url = {http://doi.acm.org/10.1145/2491956.2462179},
	doi = {10.1145/2491956.2462179},
	abstract = {We propose an approach for the static analysis of probabilistic programs that sense, manipulate, and control based on uncertain data. Examples include programs used in risk analysis, medical decision making and cyber-physical systems. Correctness properties of such programs take the form of queries that seek the probabilities of assertions over program variables. We present a static analysis approach that provides guaranteed interval bounds on the values (assertion probabilities) of such queries. First, we observe that for probabilistic programs, it is possible to conclude facts about the behavior of the entire program by choosing a finite, adequate set of its paths. We provide strategies for choosing such a set of paths and verifying its adequacy. The queries are evaluated over each path by a combination of symbolic execution and probabilistic volume-bound computations. Each path yields interval bounds that can be summed up with a "coverage" bound to yield an interval that encloses the probability of assertion for the program as a whole. We demonstrate promising results on a suite of benchmarks from many different sources including robotic manipulators and medical decision making programs.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 34th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Sankaranarayanan, Sriram and Chakarov, Aleksandar and Gulwani, Sumit},
	year = {2013},
	keywords = {monte-carlo sampling, probabilistic programming, program verification, symbolic execution, volume bounding},
	pages = {447--458},
	file = {sankaranarayanan_et_al_2013_static_analysis_for_probabilistic_programs.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/8KRJ8QB3/sankaranarayanan_et_al_2013_static_analysis_for_probabilistic_programs.pdf:application/pdf}
}

@inproceedings{naylor_finding_2007,
	series = {{SCAM} '07},
	title = {Finding {Inputs} that {Reach} a {Target} {Expression}},
	doi = {10.1109/SCAM.2007.30},
	abstract = {We present an automated program analysis, called Reach, to compute program inputs that cause evaluation of explicitly-marked target expressions. Reach has a range of applications including property refutation, assertion breaking, program crashing, program covering, program understanding, and the development of customised data generators. Reach is based on lazy narrowing, a symbolic evaluation strategy from functional-logic programming. We use Reach to analyse a range of programs, and find it to be a useful tool with clear performance benefits over a method based on exhaustive input generation. We also explore different methods for bounding the search space, the selective use of breadth-first search to find the first solution quickly, and techniques to avoid evaluation that is unnecessary to reach a target.},
	booktitle = {Seventh {IEEE} {International} {Working} {Conference} on {Source} {Code} {Analysis} and {Manipulation}, 2007. {SCAM} 2007},
	author = {Naylor, M. and Runciman, Colin},
	month = sep,
	year = {2007},
	keywords = {\_tablet, Application software, assertion breaking, automatic testing, breadth-first search, Computer crashes, Computer science, Concrete, customised data generator, explicitly-marked target expression, functional-logic programming, functional programming, lazy narrowing, logic programming, Performance analysis, Power generation, program covering, program crashing, program diagnostics, Program processors, program testing, program understanding, property refutation, Reach automated program analysis, research-exam, symbolic evaluation strategy, symbol manipulation, Target recognition},
	pages = {133--142},
	file = {naylor_runciman_2007_finding_inputs_that_reach_a_target_expression.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/DNE432GG/naylor_runciman_2007_finding_inputs_that_reach_a_target_expression.pdf:application/pdf}
}

@inproceedings{saxena_symbolic_2010,
	title = {A {Symbolic} {Execution} {Framework} for {JavaScript}},
	doi = {10.1109/SP.2010.38},
	abstract = {As AJAX applications gain popularity, client-side JavaScript code is becoming increasingly complex. However, few automated vulnerability analysis tools for JavaScript exist. In this paper, we describe the first system for exploring the execution space of JavaScript code using symbolic execution. To handle JavaScript code’s complex use of string operations, we design a new language of string constraints and implement a solver for it. We build an automatic end-to-end tool, Kudzu, and apply it to the problem of finding client-side code injection vulnerabilities. In experiments on 18 live web applications, Kudzu automatically discovers 2 previously unknown vulnerabilities and 9 more that were previously found only with a manually-constructed test suite.},
	booktitle = {2010 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Saxena, P. and Akhawe, D. and Hanna, S. and Mao, Feng and McCamant, S. and Song, D.},
	month = may,
	year = {2010},
	keywords = {Assembly, Computational modeling, Computer architecture, Digital signal processing, Digital signal processing chips, Java, Large scale integration, Logic, Registers, Telecommunication control},
	pages = {513--528},
	file = {saxena_et_al_2010_a_symbolic_execution_framework_for_javascript.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/2M6KNK97/saxena_et_al_2010_a_symbolic_execution_framework_for_javascript.pdf:application/pdf}
}

@inproceedings{barthe_higher-order_2015,
	address = {New York, NY, USA},
	series = {{POPL} '15},
	title = {Higher-{Order} {Approximate} {Relational} {Refinement} {Types} for {Mechanism} {Design} and {Differential} {Privacy}},
	isbn = {978-1-4503-3300-9},
	url = {http://doi.acm.org/10.1145/2676726.2677000},
	doi = {10.1145/2676726.2677000},
	abstract = {Mechanism design is the study of algorithm design where the inputs to the algorithm are controlled by strategic agents, who must be incentivized to faithfully report them. Unlike typical programmatic properties, it is not sufficient for algorithms to merely satisfy the property, incentive properties are only useful if the strategic agents also believe this fact. Verification is an attractive way to convince agents that the incentive properties actually hold, but mechanism design poses several unique challenges: interesting properties can be sophisticated relational properties of probabilistic computations involving expected values, and mechanisms may rely on other probabilistic properties, like differential privacy, to achieve their goals. We introduce a relational refinement type system, called HOARe2, for verifying mechanism design and differential privacy. We show that HOARe2 is sound w.r.t. a denotational semantics, and correctly models (epsilon,delta)-differential privacy; moreover, we show that it subsumes DFuzz, an existing linear dependent type system for differential privacy. Finally, we develop an SMT-based implementation of HOARe2 and use it to verify challenging examples of mechanism design, including auctions and aggregative games, and new proposed examples from differential privacy.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 42Nd {Annual} {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Barthe, Gilles and Gaboardi, Marco and Gallego Arias, Emilio Jesús and Hsu, Justin and Roth, Aaron and Strub, Pierre-Yves},
	year = {2015},
	keywords = {probabilistic programming, program logics},
	pages = {55--68},
	file = {barthe_et_al_2015_higher-order_approximate_relational_refinement_types_for_mechanism_design_and.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/MCT8NREN/barthe_et_al_2015_higher-order_approximate_relational_refinement_types_for_mechanism_design_and.pdf:application/pdf}
}

@inproceedings{lindley_algebraic_2014,
	address = {New York, NY, USA},
	series = {{WGP} '14},
	title = {Algebraic {Effects} and {Effect} {Handlers} for {Idioms} and {Arrows}},
	isbn = {978-1-4503-3042-8},
	url = {http://doi.acm.org/10.1145/2633628.2633636},
	doi = {10.1145/2633628.2633636},
	abstract = {Plotkin and Power's algebraic effects combined with Plotkin and Pretnar's effect handlers provide a foundation for modular programming with effects. We present a generalisation of algebraic effects and effect handlers to support other kinds of effectful computations corresponding to McBride and Paterson's idioms and Hughes' arrows.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 10th {ACM} {SIGPLAN} {Workshop} on {Generic} {Programming}},
	publisher = {ACM},
	author = {Lindley, Sam},
	year = {2014},
	keywords = {algebraic effects, applicative functors, arrows, call-by-push-value, effect handlers, idioms, monads},
	pages = {47--58},
	file = {lindley_2014_algebraic_effects_and_effect_handlers_for_idioms_and_arrows.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/HWN58HAU/lindley_2014_algebraic_effects_and_effect_handlers_for_idioms_and_arrows.pdf:application/pdf}
}

@inproceedings{de_moura_generalized_2009,
	title = {Generalized, efficient array decision procedures},
	doi = {10.1109/FMCAD.2009.5351142},
	abstract = {The theory of arrays is ubiquitous in the context of software and hardware verification and symbolic analysis. The basic array theory was introduced by McCarthy and allows to symbolically representing array updates. In this paper we present combinatory array logic, CAL, using a small, but powerful core of combinators, and reduce it to the theory of uninterpreted functions. CAL allows expressing properties that go well beyond the basic array theory. We provide a new efficient decision procedure for the base theory as well as CAL. The efficient procedure serves a critical role in the performance of the state-of-the-art SMT solver Z3 on array formulas from applications.},
	booktitle = {Formal {Methods} in {Computer}-{Aided} {Design}, 2009. {FMCAD} 2009},
	author = {de Moura, L. and Bjorner, N.},
	month = nov,
	year = {2009},
	keywords = {Arithmetic, Automata, basic array theory, combinatory array logic, Constraint theory, decision theory, Delay, efficient array decision procedure, Equations, Filters, formal logic, Formal verification, Hardware, hardware verification, Logic arrays, satisfiability modulo theory, SMT solver Z3, software verification, Surface-mount technology, symbolic analysis, theorem proving},
	pages = {45--52},
	file = {de_moura_bjorner_2009_generalized,_efficient_array_decision_procedures.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/DZ2V2635/de_moura_bjorner_2009_generalized,_efficient_array_decision_procedures.pdf:application/pdf}
}

@incollection{haack_type_2003,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Type {Error} {Slicing} in {Implicitly} {Typed} {Higher}-{Order} {Languages}},
	copyright = {©2003 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-00886-6 978-3-540-36575-4},
	url = {http://link.springer.com/chapter/10.1007/3-540-36575-3_20},
	abstract = {Previous methods have generally identified the location of a type error as a particular program point or the program subtree rooted at that point. We present a new approach that identifies the location of a type error as a set of program points (a slice) all of which are necessary for the type error. We describe algorithms for finding minimal type error slices for implicitly typed higher-order languages like Standard ML.},
	language = {en},
	number = {2618},
	urldate = {2015-06-26},
	booktitle = {Programming {Languages} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Haack, Christian and Wells, J. B.},
	editor = {Degano, Pierpaolo},
	year = {2003},
	keywords = {data structures, Logics and Meanings of Programs, Mathematical Logic and Formal Languages, Programming Languages, Compilers, Interpreters, Programming Techniques, Software Engineering},
	pages = {284--301},
	file = {haack_wells_2003_type_error_slicing_in_implicitly_typed_higher-order_languages.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/K8AVH5ZV/haack_wells_2003_type_error_slicing_in_implicitly_typed_higher-order_languages.pdf:application/pdf}
}

@inproceedings{christiansen_dependent_2013,
	address = {New York, NY, USA},
	series = {{WGP} '13},
	title = {Dependent {Type} {Providers}},
	isbn = {978-1-4503-2389-5},
	url = {http://doi.acm.org/10.1145/2502488.2502495},
	doi = {10.1145/2502488.2502495},
	abstract = {Type providers, pioneered in the F\# programming language, are a practical and powerful means of gaining the benefits of a modern static type system when working with data schemas that are defined outside of the programming language, such as relational databases. F\# type providers are implemented using a form of compile-time code generation, which requires the compiler to expose an internal API and can undermine type safety. We show that with dependent types it is possible to define a type provider mechanism that does not rely on code generation. Using this mechanism, a type provider becomes a kind of generic program that is instantiated for a particular external schema, which can be represented using an ordinary datatype. Because these dependent type providers use the ordinary abstraction mechanisms of the type system, they preserve its safety properties. We evaluate the practicality of this technique and explore future extensions.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 9th {ACM} {SIGPLAN} {Workshop} on {Generic} {Programming}},
	publisher = {ACM},
	author = {Christiansen, David Raymond},
	year = {2013},
	keywords = {dependent types, generic programming, metaprogramming, type providers},
	pages = {25--34},
	file = {christiansen_2013_dependent_type_providers.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/5MVTHTPP/christiansen_2013_dependent_type_providers.pdf:application/pdf;christiansen_2013_dependent_type_providers.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/AITPJAH9/christiansen_2013_dependent_type_providers.pdf:application/pdf}
}

@article{nilsson_probabilistic_1986,
	title = {Probabilistic logic},
	volume = {28},
	url = {http://www.sciencedirect.com/science/article/pii/0004370286900317},
	number = {1},
	urldate = {2015-05-14},
	journal = {Artificial intelligence},
	author = {Nilsson, Nils J.},
	year = {1986},
	pages = {71--87}
}

@incollection{huet_higher_2002,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Higher {Order} {Unification} 30 {Years} {Later}},
	copyright = {©2002 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-44039-0 978-3-540-45685-8},
	url = {http://link.springer.com/chapter/10.1007/3-540-45685-6_2},
	abstract = {The talk will present a survey of higher order unification, covering an outline of its historical development, a summary of its applications to three fields: automated theorem proving, and more generally engineering of proof assistants, programming environments and software engineering, and finally computational linguistics. It concludes by a presentation of open problems, and a few prospective remarks on promising future directions. This presentation assumes as background the survey by Gilles Dowek in the Handbook of automated theorem proving [28].},
	language = {en},
	number = {2410},
	urldate = {2015-08-18},
	booktitle = {Theorem {Proving} in {Higher} {Order} {Logics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Huet, Gérard},
	editor = {Carreño, Victor A. and Muñoz, César A. and Tahar, Sofiène},
	year = {2002},
	keywords = {Logic Design, Software Engineering},
	pages = {3--12},
	file = {huet_2002_higher_order_unification_30_years_later.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/IWQXW378/huet_2002_higher_order_unification_30_years_later.pdf:application/pdf;Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/SSR5MT8B/3-540-45685-6_2.html:text/html}
}

@inproceedings{demaine_retroactive_2004,
	address = {Philadelphia, PA, USA},
	series = {{SODA} '04},
	title = {Retroactive {Data} {Structures}},
	isbn = {978-0-89871-558-3},
	url = {http://dl.acm.org/citation.cfm?id=982792.982832},
	abstract = {We introduce a new data structuring paradigm in which operations can be performed on a data structure not only in the present but also in the past. In this new paradigm, called retroactive data structures, the historical sequence of operations performed on the data structure is not fixed. The data structure allows arbitrary insertion and deletion of operations at arbitrary times, subject only to consistency requirements. We initiate the study of retroactive data structures by formally defining the model and its variants. We prove that, unlike persistence, efficient retroactivity is not always achievable, so we go on to present several specific retroactive data structures.},
	urldate = {2015-09-25},
	booktitle = {Proceedings of the {Fifteenth} {Annual} {ACM}-{SIAM} {Symposium} on {Discrete} {Algorithms}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Demaine, Erik D. and Iacono, John and Langerman, Stefan},
	year = {2004},
	pages = {281--290},
	file = {demaine_et_al_2004_retroactive_data_structures.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/AD9SSDBI/demaine_et_al_2004_retroactive_data_structures.pdf:application/pdf}
}

@inproceedings{raychev_predicting_2015,
	address = {New York, NY, USA},
	series = {{POPL} '15},
	title = {Predicting {Program} {Properties} from "{Big} {Code}"},
	isbn = {978-1-4503-3300-9},
	url = {http://doi.acm.org/10.1145/2676726.2677009},
	doi = {10.1145/2676726.2677009},
	abstract = {We present a new approach for predicting program properties from massive codebases (aka "Big Code"). Our approach first learns a probabilistic model from existing data and then uses this model to predict properties of new, unseen programs. The key idea of our work is to transform the input program into a representation which allows us to phrase the problem of inferring program properties as structured prediction in machine learning. This formulation enables us to leverage powerful probabilistic graphical models such as conditional random fields (CRFs) in order to perform joint prediction of program properties. As an example of our approach, we built a scalable prediction engine called JSNice for solving two kinds of problems in the context of JavaScript: predicting (syntactic) names of identifiers and predicting (semantic) type annotations of variables. Experimentally, JSNice predicts correct names for 63\% of name identifiers and its type annotation predictions are correct in 81\% of the cases. In the first week since its release, JSNice was used by more than 30,000 developers and in only few months has become a popular tool in the JavaScript developer community. By formulating the problem of inferring program properties as structured prediction and showing how to perform both learning and inference in this context, our work opens up new possibilities for attacking a wide range of difficult problems in the context of "Big Code" including invariant generation, decompilation, synthesis and others.},
	urldate = {2015-04-27},
	booktitle = {Proceedings of the 42Nd {Annual} {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Raychev, Veselin and Vechev, Martin and Krause, Andreas},
	year = {2015},
	keywords = {big code, closure compiler, conditional random fields, javascript, names, program properties, structured prediction, types},
	pages = {111--124},
	file = {raychev_et_al_2015_predicting_program_properties_from_big_code.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/I2F7JC3U/raychev_et_al_2015_predicting_program_properties_from_big_code.pdf:application/pdf}
}

@inproceedings{launchbury_natural_1993,
	address = {New York, NY, USA},
	series = {{POPL} '93},
	title = {A {Natural} {Semantics} for {Lazy} {Evaluation}},
	isbn = {978-0-89791-560-1},
	url = {http://doi.acm.org/10.1145/158511.158618},
	doi = {10.1145/158511.158618},
	abstract = {We define an operational semantics for lazy evaluation which provides an accurate model for sharing. The only computational structure we introduce is a set of bindings which corresponds closely to a heap. The semantics is set at a considerably higher level of abstraction than operational semantics for particular abstract machines, so is more suitable for a variety of proofs. Furthermore, because a heap is explicitly modelled, the semantics provides a suitable framework for studies about space behaviour of terms under lazy evaluation.},
	urldate = {2015-09-25},
	booktitle = {Proceedings of the 20th {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Launchbury, John},
	year = {1993},
	pages = {144--154},
	file = {launchbury_1993_a_natural_semantics_for_lazy_evaluation.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/B8I2HE6R/launchbury_1993_a_natural_semantics_for_lazy_evaluation.pdf:application/pdf}
}

@inproceedings{farmer_hermit_2012,
	address = {New York, NY, USA},
	series = {Haskell '12},
	title = {The {HERMIT} in the {Machine}: {A} {Plugin} for the {Interactive} {Transformation} of {GHC} {Core} {Language} {Programs}},
	isbn = {978-1-4503-1574-6},
	shorttitle = {The {HERMIT} in the {Machine}},
	url = {http://doi.acm.org/10.1145/2364506.2364508},
	doi = {10.1145/2364506.2364508},
	urldate = {2015-05-21},
	booktitle = {Proceedings of the 2012 {Haskell} {Symposium}},
	publisher = {ACM},
	author = {Farmer, Andrew and Gill, Andy and Komp, Ed and Sculthorpe, Neil},
	year = {2012},
	keywords = {dsls, equational reasoning, ghc, optimization, strategic programming},
	pages = {1--12},
	file = {farmer_et_al_2012_the_hermit_in_the_machine.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/C3S7I98H/farmer_et_al_2012_the_hermit_in_the_machine.pdf:application/pdf}
}

@inproceedings{aldrich_power_2013,
	address = {New York, NY, USA},
	series = {Onward! 2013},
	title = {The {Power} of {Interoperability}: {Why} {Objects} {Are} {Inevitable}},
	isbn = {978-1-4503-2472-4},
	shorttitle = {The {Power} of {Interoperability}},
	url = {http://doi.acm.org/10.1145/2509578.2514738},
	doi = {10.1145/2509578.2514738},
	abstract = {Three years ago in this venue, Cook argued that in their essence, objects are what Reynolds called procedural data structures. His observation raises a natural question: if procedural data structures are the essence of objects, has this contributed to the empirical success of objects, and if so, how? This essay attempts to answer that question. After reviewing Cook's definition, I propose the term service abstractions to capture the essential nature of objects. This terminology emphasizes, following Kay, that objects are not primarily about representing and manipulating data, but are more about providing services in support of higher-level goals. Using examples taken from object-oriented frameworks, I illustrate the unique design leverage that service abstractions provide: the ability to define abstractions that can be extended, and whose extensions are interoperable in a first-class way. The essay argues that the form of interoperable extension supported by service abstractions is essential to modern software: many modern frameworks and ecosystems could not have been built without service abstractions. In this sense, the success of objects was not a coincidence: it was an inevitable consequence of their service abstraction nature.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 2013 {ACM} {International} {Symposium} on {New} {Ideas}, {New} {Paradigms}, and {Reflections} on {Programming} \& {Software}},
	publisher = {ACM},
	author = {Aldrich, Jonathan},
	year = {2013},
	keywords = {frameworks, interoperability, object-oriented programming, service abstractions},
	pages = {101--116},
	file = {aldrich_2013_the_power_of_interoperability.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/KD3BBPB9/aldrich_2013_the_power_of_interoperability.pdf:application/pdf}
}

@inproceedings{pavlinovic_practical_2015,
	address = {New York, NY, USA},
	series = {{ICFP} 2015},
	title = {Practical {SMT}-based {Type} {Error} {Localization}},
	isbn = {978-1-4503-3669-7},
	url = {http://doi.acm.org/10.1145/2784731.2784765},
	doi = {10.1145/2784731.2784765},
	abstract = {Compilers for statically typed functional programming languages are notorious for generating confusing type error messages. When the compiler detects a type error, it typically reports the program location where the type checking failed as the source of the error. Since other error sources are not even considered, the actual root cause is often missed. A more adequate approach is to consider all possible error sources and report the most useful one subject to some usefulness criterion. In our previous work, we showed that this approach can be formulated as an optimization problem related to satisfiability modulo theories (SMT). This formulation cleanly separates the heuristic nature of usefulness criteria from the underlying search problem. Unfortunately, algorithms that search for an optimal error source cannot directly use principal types which are crucial for dealing with the exponential-time complexity of the decision problem of polymorphic type checking. In this paper, we present a new algorithm that efficiently finds an optimal error source in a given ill-typed program. Our algorithm uses an improved SMT encoding to cope with the high complexity of polymorphic typing by iteratively expanding the typing constraints from which principal types are derived. The algorithm preserves the clean separation between the heuristics and the actual search. We have implemented our algorithm for OCaml. In our experimental evaluation, we found that the algorithm reduces the running times for optimal type error localization from minutes to seconds and scales better than previous localization algorithms.},
	urldate = {2015-09-03},
	booktitle = {Proceedings of the 20th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Pavlinovic, Zvonimir and King, Tim and Wies, Thomas},
	year = {2015},
	keywords = {Polymorphic Types, satisfiability modulo theories, Type Error Localization},
	pages = {412--423},
	file = {pavlinovic_et_al_2015_practical_smt-based_type_error_localization.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/STX7CR34/pavlinovic_et_al_2015_practical_smt-based_type_error_localization.pdf:application/pdf}
}

@incollection{reinking_type-directed_2015,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Type}-{Directed} {Approach} to {Program} {Repair}},
	copyright = {©2015 Springer International Publishing Switzerland},
	isbn = {978-3-319-21689-8 978-3-319-21690-4},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-21690-4_35},
	abstract = {Developing enterprise software often requires composing several libraries together with a large body of in-house code. Large APIs introduce a steep learning curve for new developers as a result of their complex object-oriented underpinnings. While the written code in general reflects a programmer’s intent, due to evolutions in an API, code can often become ill-typed, yet still syntactically-correct. Such code fragments will no longer compile, and will need to be updated. We describe an algorithm that automatically repairs such errors, and discuss its application to common problems in software engineering.},
	language = {en},
	number = {9206},
	urldate = {2015-11-12},
	booktitle = {Computer {Aided} {Verification}},
	publisher = {Springer International Publishing},
	author = {Reinking, Alex and Piskac, Ruzica},
	editor = {Kroening, Daniel and Păsăreanu, Corina S.},
	month = jul,
	year = {2015},
	note = {DOI: 10.1007/978-3-319-21690-4\_35},
	keywords = {Computer Systems Organization and Communication Networks, Logics and Meanings of Programs, Mathematical Logic and Formal Languages, Software Engineering/Programming and Operating Systems},
	pages = {511--517},
	file = {reinking_piskac_2015_a_type-directed_approach_to_program_repair.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/GFDJEUZ3/reinking_piskac_2015_a_type-directed_approach_to_program_repair.pdf:application/pdf;Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/X5RWHSMD/978-3-319-21690-4_35.html:text/html}
}

@article{king_symbolic_1976,
	title = {Symbolic {Execution} and {Program} {Testing}},
	volume = {19},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/360248.360252},
	doi = {10.1145/360248.360252},
	abstract = {This paper describes the symbolic execution of programs. Instead of supplying the normal inputs to a program (e.g. numbers) one supplies symbols representing arbitrary values. The execution proceeds as in a normal execution except that values may be symbolic formulas over the input symbols. The difficult, yet interesting issues arise during the symbolic execution of conditional branch type statements. A particular system called EFFIGY which provides symbolic execution for program testing and debugging is also described. It interpretively executes programs written in a simple PL/I style programming language. It includes many standard debugging features, the ability to manage and to prove things about symbolic expressions, a simple program testing manager, and a program verifier. A brief discussion of the relationship between symbolic execution and program proving is also included.},
	number = {7},
	urldate = {2015-06-26},
	journal = {Commun. ACM},
	author = {King, James C.},
	month = jul,
	year = {1976},
	keywords = {program debugging, program proving, program testing, program verification, symbolic execution, symbolic interpretation},
	pages = {385--394},
	file = {king_1976_symbolic_execution_and_program_testing.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/82JIPCMD/king_1976_symbolic_execution_and_program_testing.pdf:application/pdf}
}

@inproceedings{xi_eliminating_1998,
	address = {New York, NY, USA},
	series = {{PLDI} '98},
	title = {Eliminating {Array} {Bound} {Checking} {Through} {Dependent} {Types}},
	isbn = {0-89791-987-4},
	url = {http://doi.acm.org/10.1145/277650.277732},
	doi = {10.1145/277650.277732},
	abstract = {We present a type-based approach to eliminating array bound checking and list tag checking by conservatively extending Standard ML with a restricted form of dependent types. This enables the programmer to capture more invariants through types while type-checking remains decidable in theory and can still be performed efficiently in practice. We illustrate our approach through concrete examples and present the result of our preliminary experiments which support support the feasibility and effectiveness of our approach.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the {ACM} {SIGPLAN} 1998 {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Xi, Hongwei and Pfenning, Frank},
	year = {1998},
	pages = {249--257},
	file = {xi_pfenning_1998_eliminating_array_bound_checking_through_dependent_types.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/5I8B7JF8/xi_pfenning_1998_eliminating_array_bound_checking_through_dependent_types.pdf:application/pdf}
}

@article{abramson_when_2011,
	title = {When {Formal} {Systems} {Kill}: {Computer} {Ethics} and {Formal} {Methods}},
	volume = {11},
	shorttitle = {When {Formal} {Systems} {Kill}},
	url = {https://www.cs.indiana.edu/~lepike/pubs/fm-ethics.pdf},
	number = {1},
	urldate = {2014-06-16},
	journal = {APA Newsletter on Philosophy and Computers},
	author = {Abramson, Darren and Pike, Lee},
	year = {2011},
	keywords = {\_tablet},
	file = {abramson_pike_2011_when_formal_systems_kill.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/34J32B66/abramson_pike_2011_when_formal_systems_kill.pdf:application/pdf}
}

@article{wadler_critique_1987,
	title = {A {Critique} of {Abelson} and {Sussman} or {Why} {Calculating} is {Better} {Than} {Scheming}},
	volume = {22},
	issn = {0362-1340},
	url = {http://doi.acm.org/10.1145/24697.24706},
	doi = {10.1145/24697.24706},
	number = {3},
	urldate = {2015-03-04},
	journal = {SIGPLAN Not.},
	author = {Wadler, P},
	month = mar,
	year = {1987},
	keywords = {education, functional programming},
	pages = {83--94},
	file = {wadler_1987_a_critique_of_abelson_and_sussman_or_why_calculating_is_better_than_scheming.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/SWPW3E29/wadler_1987_a_critique_of_abelson_and_sussman_or_why_calculating_is_better_than_scheming.pdf:application/pdf}
}

@inproceedings{seidel_simplifying_2010,
	address = {New York, NY, USA},
	series = {{TG} '10},
	title = {Simplifying {Complex} {Software} {Assembly}: {The} {Component} {Retrieval} {Language} and {Implementation}},
	isbn = {978-1-60558-818-6},
	shorttitle = {Simplifying {Complex} {Software} {Assembly}},
	url = {http://doi.acm.org/10.1145/1838574.1838592},
	doi = {10.1145/1838574.1838592},
	abstract = {Assembling simulation software along with the associated tools and utilities is a challenging endeavor, particularly when the components are distributed across multiple source code versioning systems. It is problematic for researchers compiling and running the software across many different supercomputers, as well as for novices in a field who are often presented with a bewildering list of software to collect and install. In this paper, we describe a language (CRL) for specifying software components with the details needed to obtain them from source code repositories. The language supports public and private access. We describe a tool called Get Components which implements CRL and can be used to assemble software. We demonstrate the tool for application scenarios with the Cactus Framework on the NSF TeraGrid resources. The tool itself is distributed with an open source license and freely available from our web page.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 2010 {TeraGrid} {Conference}},
	publisher = {ACM},
	author = {Seidel, Eric L. and Allen, Gabrielle and Brandt, Steven and Löffler, Frank and Schnetter, Erik},
	year = {2010},
	pages = {18:1--18:8},
	file = {seidel_et_al_2010_simplifying_complex_software_assembly.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/PJKBBAXN/seidel_et_al_2010_simplifying_complex_software_assembly.pdf:application/pdf}
}

@inproceedings{pombrio_resugaring:_2014,
	address = {New York, NY, USA},
	series = {{PLDI} '14},
	title = {Resugaring: {Lifting} {Evaluation} {Sequences} {Through} {Syntactic} {Sugar}},
	isbn = {978-1-4503-2784-8},
	shorttitle = {Resugaring},
	url = {http://doi.acm.org/10.1145/2594291.2594319},
	doi = {10.1145/2594291.2594319},
	abstract = {Syntactic sugar is pervasive in language technology. It is used to shrink the size of a core language; to define domain-specific languages; and even to let programmers extend their language. Unfortunately, syntactic sugar is eliminated by transformation, so the resulting programs become unfamiliar to authors. Thus, it comes at a price: it obscures the relationship between the user's source program and the program being evaluated. We address this problem by showing how to compute reduction steps in terms of the surface syntax. Each step in the surface language emulates one or more steps in the core language. The computed steps hide the transformation, thus maintaining the abstraction provided by the surface language. We make these statements about emulation and abstraction precise, prove that they hold in our formalism, and verify part of the system in Coq. We have implemented this work and applied it to three very different languages.},
	urldate = {2015-08-31},
	booktitle = {Proceedings of the 35th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Pombrio, Justin and Krishnamurthi, Shriram},
	year = {2014},
	keywords = {debugging, evaluation, macros, programming languages, resugaring, syntactic sugar},
	pages = {361--371},
	file = {pombrio_krishnamurthi_2014_resugaring.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/436BZAP9/pombrio_krishnamurthi_2014_resugaring.pdf:application/pdf}
}

@inproceedings{jadud_methods_2006,
	address = {New York, NY, USA},
	series = {{ICER} '06},
	title = {Methods and {Tools} for {Exploring} {Novice} {Compilation} {Behaviour}},
	isbn = {978-1-59593-494-9},
	url = {http://doi.acm.org/10.1145/1151588.1151600},
	doi = {10.1145/1151588.1151600},
	abstract = {Our research explores what we call compilation behaviour: the programming behaviour a student engages in while repeatedly editing and compiling their programs. This edit-compile cycle often represents students' attempts to make their programs syntactically, as opposed to semantically, correct. Over the course of two years, we have observed first-year university students learning to program in Java, collecting and studying thousands of snapshots of their programs from one compilation to the next. At the University of Kent, students are introduced to programming in an objects-first style using BlueJ, an environment intended for use by novice programmers.},
	urldate = {2015-09-24},
	booktitle = {Proceedings of the {Second} {International} {Workshop} on {Computing} {Education} {Research}},
	publisher = {ACM},
	author = {Jadud, Matthew C.},
	year = {2006},
	keywords = {behavior, BlueJ, compilation, compiler, Java, novice},
	pages = {73--84},
	file = {jadud_2006_methods_and_tools_for_exploring_novice_compilation_behaviour.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/9ZVE24B5/jadud_2006_methods_and_tools_for_exploring_novice_compilation_behaviour.pdf:application/pdf}
}

@incollection{claessen_quickspec:_2010,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{QuickSpec}: {Guessing} {Formal} {Specifications} {Using} {Testing}},
	copyright = {©2010 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-13976-5 978-3-642-13977-2},
	shorttitle = {{QuickSpec}},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-13977-2_3},
	abstract = {We present QuickSpec, a tool that automatically generates algebraic specifications for sets of pure functions. The tool is based on testing, rather than static analysis or theorem proving. The main challenge QuickSpec faces is to keep the number of generated equations to a minimum while maintaining completeness. We demonstrate how QuickSpec can improve one’s understanding of a program module by exploring the laws that are generated using two case studies: a heap library for Haskell and a fixed-point arithmetic library for Erlang.},
	language = {en},
	number = {6143},
	urldate = {2015-06-26},
	booktitle = {Tests and {Proofs}},
	publisher = {Springer Berlin Heidelberg},
	author = {Claessen, Koen and Smallbone, Nicholas and Hughes, John},
	editor = {Fraser, Gordon and Gargantini, Angelo},
	year = {2010},
	keywords = {Algorithm Analysis and Problem Complexity, Logics and Meanings of Programs, Mathematical Logic and Formal Languages, Programming Languages, Compilers, Interpreters, Programming Techniques, Software Engineering},
	pages = {6--21},
	file = {claessen_et_al_2010_quickspec.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/FNXCP9BA/claessen_et_al_2010_quickspec.pdf:application/pdf}
}

@book{godefroid_automated_????,
	title = {Automated {Whitebox} {Fuzz} {Testing}},
	abstract = {Fuzz testing is an effective technique for finding security vulnerabilities in software. Traditionally, fuzz testing tools apply random mutations to well-formed inputs of a program and test the resulting values. We present an alternative whitebox fuzz testing approach inspired by recent advances in symbolic execution and dynamic test generation. Our approach records an actual run of the program under test on a well-formed input, symbolically evaluates the recorded trace, and gathers constraints on inputs capturing how the program uses these. The collected constraints are then negated one by one and solved with a constraint solver, producing new inputs that exercise different control paths in the program. This process is repeated with the help of a code-coverage maximizing heuristic designed to find defects as fast as possible. We have implemented this algorithm in SAGE (Scalable, Automated, Guided Execution), a new tool employing x86 instruction-level tracing and emulation for whitebox fuzzing of arbitrary file-reading Windows applications. We describe key optimizations needed to make dynamic test generation scale to large input files and long execution traces with hundreds of millions of instructions. We then present detailed experiments with several Windows applications. Notably, without any format-specific knowledge, SAGE detects the MS07-017 ANI vulnerability, which was missed by extensive blackbox fuzzing and static analysis tools. Furthermore, while still in an early stage of development, SAGE has already discovered 30+ new bugs in large shipped Windows applications including image processors, media players, and file decoders. Several of these bugs are potentially exploitable memory access violations.},
	author = {Godefroid, Patrice and Levin, Michael Y. and Molnar, David},
	file = {godefroid_et_al_automated_whitebox_fuzz_testing.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/6GBVQEIA/godefroid_et_al_automated_whitebox_fuzz_testing.pdf:application/pdf}
}

@article{beyer_software_2007,
	title = {The software model checker {Blast}},
	volume = {9},
	issn = {1433-2779, 1433-2787},
	url = {http://link.springer.com/article/10.1007/s10009-007-0044-z},
	doi = {10.1007/s10009-007-0044-z},
	abstract = {Blast is an automatic verification tool for checking temporal safety properties of C programs. Given a C program and a temporal safety property, Blast either statically proves that the program satisfies the safety property, or provides an execution path that exhibits a violation of the property (or, since the problem is undecidable, does not terminate). Blast constructs, explores, and refines abstractions of the program state space based on lazy predicate abstraction and interpolation-based predicate discovery. This paper gives an introduction to Blast and demonstrates, through two case studies, how it can be applied to program verification and test-case generation. In the first case study, we use Blast to statically prove memory safety for C programs. We use CCured, a type-based memory-safety analyzer, to annotate a program with run-time assertions that check for safe memory operations. Then, we use Blast to remove as many of the run-time checks as possible (by proving that these checks never fail), and to generate execution scenarios that violate the assertions for the remaining run-time checks. In our second case study, we use Blast to automatically generate test suites that guarantee full coverage with respect to a given predicate. Given a C program and a target predicate p, Blast determines the program locations q for which there exists a program execution that reaches q with p true, and automatically generates a set of test vectors that cause such executions. Our experiments show that Blast can provide automated, precise, and scalable analysis for C programs.},
	language = {en},
	number = {5-6},
	urldate = {2015-01-23},
	journal = {International Journal on Software Tools for Technology Transfer},
	author = {Beyer, Dirk and Henzinger, Thomas A. and Jhala, Ranjit and Majumdar, Rupak},
	month = sep,
	year = {2007},
	keywords = {Memory safety, model checking, Software Engineering, Software Engineering/Programming and Operating Systems, Software specification, software verification, Test-case generation, Theory of Computation},
	pages = {505--525},
	file = {beyer_et_al_2007_the_software_model_checker_blast.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/GF9MUEC2/beyer_et_al_2007_the_software_model_checker_blast.pdf:application/pdf}
}

@incollection{sestoft_demonstrating_2002,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Demonstrating {Lambda} {Calculus} {Reduction}},
	copyright = {©2002 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-00326-7 978-3-540-36377-4},
	url = {http://link.springer.com/chapter/10.1007/3-540-36377-7_19},
	language = {en},
	number = {2566},
	urldate = {2015-09-25},
	booktitle = {The {Essence} of {Computation}},
	publisher = {Springer Berlin Heidelberg},
	author = {Sestoft, Peter},
	editor = {Mogensen, Torben Æ and Schmidt, David A. and Sudborough, I. Hal},
	year = {2002},
	keywords = {Computation by Abstract Devices, Logics and Meanings of Programs, Software Engineering},
	pages = {420--435},
	file = {sestoft_2002_demonstrating_lambda_calculus_reduction.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/GNMATTT3/sestoft_2002_demonstrating_lambda_calculus_reduction.pdf:application/pdf;Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/3DV78B3S/10.html:text/html}
}

@inproceedings{vazou_liquidhaskell:_2014,
	address = {New York, NY, USA},
	series = {Haskell '14},
	title = {{LiquidHaskell}: {Experience} with {Refinement} {Types} in the {Real} {World}},
	isbn = {978-1-4503-3041-1},
	shorttitle = {{LiquidHaskell}},
	url = {http://doi.acm.org/10.1145/2633357.2633366},
	doi = {10.1145/2633357.2633366},
	abstract = {Haskell has many delightful features. Perhaps the one most beloved by its users is its type system that allows developers to specify and verify a variety of program properties at compile time. However, many properties, typically those that depend on relationships between program values are impossible, or at the very least, cumbersome to encode within the existing type system. Many such properties can be verified using a combination of Refinement Types and external SMT solvers. We describe the refinement type checker liquidHaskell, which we have used to specify and verify a variety of properties of over 10,000 lines of Haskell code from various popular libraries, including containers, hscolour, bytestring, text, vector-algorithms and xmonad. First, we present a high-level overview of liquidHaskell, through a tour of its features. Second, we present a qualitative discussion of the kinds of properties that can be checked -- ranging from generic application independent criteria like totality and termination, to application specific concerns like memory safety and data structure correctness invariants. Finally, we present a quantitative evaluation of the approach, with a view towards measuring the efficiency and programmer effort required for verification, and discuss the limitations of the approach.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 2014 {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Vazou, Niki and Seidel, Eric L. and Jhala, Ranjit},
	year = {2014},
	keywords = {haskell, refinement types, smt-based verification, verification},
	pages = {39--51},
	file = {vazou_et_al_2014_liquidhaskell.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/GFSJ55R2/vazou_et_al_2014_liquidhaskell.pdf:application/pdf}
}

@article{tassey_economic_2002,
	title = {The economic impacts of inadequate infrastructure for software testing},
	volume = {7007},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.122.3316&rep=rep1&type=pdf},
	number = {011},
	urldate = {2015-01-30},
	journal = {National Institute of Standards and Technology, RTI Project},
	author = {Tassey, Gregory},
	year = {2002},
	file = {tassey_2002_the_economic_impacts_of_inadequate_infrastructure_for_software_testing.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/RGBRVMR8/tassey_2002_the_economic_impacts_of_inadequate_infrastructure_for_software_testing.pdf:application/pdf}
}

@incollection{turner_elementary_1995,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Elementary strong functional programming},
	copyright = {©1995 Springer-Verlag},
	isbn = {978-3-540-60675-8 978-3-540-49252-8},
	url = {http://link.springer.com/chapter/10.1007/3-540-60675-0_35},
	abstract = {Functional programming is a good idea, but we haven't got it quite right yet. What we have been doing up to now is weak (or partial) functional programming. What we should be doing is strong (or total) functional programming — in which all computations terminate. We propose an elementary discipline of strong functional programming. A key feature of the discipline is that we introduce a type distinction between data, which is known to be finite, and codata, which is (potentially) infinite.},
	language = {en},
	number = {1022},
	urldate = {2015-03-04},
	booktitle = {Funtional {Programming} {Languages} in {Education}},
	publisher = {Springer Berlin Heidelberg},
	author = {Turner, D. A.},
	editor = {Hartel, Pieter H. and Plasmeijer, Rinus},
	year = {1995},
	keywords = {Logics and Meanings of Programs, Programming Techniques},
	pages = {1--13},
	file = {turner_1995_elementary_strong_functional_programming.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/2FWQ5RJA/turner_1995_elementary_strong_functional_programming.pdf:application/pdf}
}

@inproceedings{morris_variations_2015,
	address = {New York, NY, USA},
	series = {Haskell 2015},
	title = {Variations on {Variants}},
	isbn = {978-1-4503-3808-0},
	url = {http://doi.acm.org/10.1145/2804302.2804320},
	doi = {10.1145/2804302.2804320},
	abstract = {Extensible variants improve the modularity and expressiveness of programming languages: they allow program functionality to be decomposed into independent blocks, and allow seamless extension of existing code with both new cases of existing data types and new operations over those data types. This paper considers three approaches to providing extensible variants in Haskell. Row typing is a long understood mechanism for typing extensible records and variants, but its adoption would require extension of Haskell's core type system. Alternatively, we might hope to encode extensible variants in terms of existing mechanisms, such as type classes. We describe an encoding of extensible variants using instance chains, a proposed extension of the class system. Unlike many previous encodings of extensible variants, ours does not require the definition of a new type class for each function that consumes variants. Finally, we translate our encoding to use closed type families, an existing feature of GHC. Doing so demonstrates the interpretation of instances chains and functional dependencies in closed type families. One concern with encodings like ours is how completely they match the encoded system. We compare the expressiveness of our encodings with each other and with systems based on row types. We find that, while equivalent terms are typable in each system, both encodings require explicit type annotations to resolve ambiguities in typing not present in row type systems, and the type family implementation retains more constraints in principal types than does the instance chain implementation. We propose a general mechanism to guide the instantiation of ambiguous type variables, show that it eliminates the need for type annotations in our encodings, and discuss conditions under which it preserves coherence.},
	urldate = {2015-09-03},
	booktitle = {Proceedings of the 8th {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Morris, J. Garrett},
	year = {2015},
	keywords = {expression problem, extensible variants, row types},
	pages = {71--81},
	file = {morris_2015_variations_on_variants.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/6XC9KT25/morris_2015_variations_on_variants.pdf:application/pdf}
}

@inproceedings{costa_vigilante:_2005,
	address = {New York, NY, USA},
	series = {{SOSP} '05},
	title = {Vigilante: {End}-to-end {Containment} of {Internet} {Worms}},
	isbn = {1-59593-079-5},
	shorttitle = {Vigilante},
	url = {http://doi.acm.org/10.1145/1095810.1095824},
	doi = {10.1145/1095810.1095824},
	abstract = {Worm containment must be automatic because worms can spread too fast for humans to respond. Recent work has proposed network-level techniques to automate worm containment; these techniques have limitations because there is no information about the vulnerabilities exploited by worms at the network level. We propose Vigilante, a new end-to-end approach to contain worms automatically that addresses these limitations. Vigilante relies on collaborative worm detection at end hosts, but does not require hosts to trust each other. Hosts run instrumented software to detect worms and broadcast self-certifying alerts (SCAs) upon worm detection. SCAs are proofs of vulnerability that can be inexpensively verified by any vulnerable host. When hosts receive an SCA, they generate filters that block infection by analysing the SCA-guided execution of the vulnerable software. We show that Vigilante can automatically contain fast-spreading worms that exploit unknown vulnerabilities without blocking innocuous traffic.},
	urldate = {2015-01-25},
	booktitle = {Proceedings of the {Twentieth} {ACM} {Symposium} on {Operating} {Systems} {Principles}},
	publisher = {ACM},
	author = {Costa, Manuel and Crowcroft, Jon and Castro, Miguel and Rowstron, Antony and Zhou, Lidong and Zhang, Lintao and Barham, Paul},
	year = {2005},
	keywords = {control flow analysis, data flow analysis, self-certifying alerts, worm containment},
	pages = {133--147},
	file = {costa_et_al_2005_vigilante.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/9AIRHHBH/costa_et_al_2005_vigilante.pdf:application/pdf}
}

@inproceedings{duregard_feat:_2012,
	address = {New York, NY, USA},
	series = {Haskell '12},
	title = {Feat: {Functional} {Enumeration} of {Algebraic} {Types}},
	isbn = {978-1-4503-1574-6},
	shorttitle = {Feat},
	url = {http://doi.acm.org/10.1145/2364506.2364515},
	doi = {10.1145/2364506.2364515},
	abstract = {In mathematics, an enumeration of a set S is a bijective function from (an initial segment of) the natural numbers to S. We define "functional enumerations" as efficiently computable such bijections. This paper describes a theory of functional enumeration and provides an algebra of enumerations closed under sums, products, guarded recursion and bijections. We partition each enumerated set into numbered, finite subsets. We provide a generic enumeration such that the number of each part corresponds to the size of its values (measured in the number of constructors). We implement our ideas in a Haskell library called testing-feat, and make the source code freely available. Feat provides efficient "random access" to enumerated values. The primary application is property-based testing, where it is used to define both random sampling (for example QuickCheck generators) and exhaustive enumeration (in the style of SmallCheck). We claim that functional enumeration is the best option for automatically generating test cases from large groups of mutually recursive syntax tree types. As a case study we use Feat to test the pretty-printer of the Template Haskell library (uncovering several bugs).},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 2012 {Haskell} {Symposium}},
	publisher = {ACM},
	author = {Duregård, Jonas and Jansson, Patrik and Wang, Meng},
	year = {2012},
	keywords = {enumeration, memoisation, property-based testing},
	pages = {61--72},
	file = {duregård_et_al_2012_feat.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/7HE7BKK2/duregård_et_al_2012_feat.pdf:application/pdf}
}

@techreport{hayes_specifying_1994,
	address = {Mountain View, CA, USA},
	title = {Specifying and {Testing} {Software} {Components} {Using} {ADL}},
	abstract = {This paper presents a novel approach to unit testing of software components. This approach uses the specification language ADL, that is particularly well-suited for testing, to formally document the intended behavior of software components. Another related language, TDD, is used to systematically describe the test-data on which the software components will be tested. This paper gives a detailed overview of the ADL language, and a brief presentation of the TDD language. Some details of the actual test system are also presented, along with some significant results.},
	institution = {Sun Microsystems, Inc.},
	author = {Hayes, Roger and Sankar, Sriram},
	year = {1994},
	file = {hayes_sankar_1994_specifying_and_testing_software_components_using_adl.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/964AZVZC/hayes_sankar_1994_specifying_and_testing_software_components_using_adl.pdf:application/pdf}
}

@techreport{chitil_strictcheck:_2011,
	title = {{StrictCheck}: a tool for testing whether a function is unnecessarily strict},
	shorttitle = {{StrictCheck}},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.414.9159&rep=rep1&type=pdf},
	urldate = {2015-08-06},
	institution = {Citeseer},
	author = {Chitil, Olaf},
	year = {2011},
	file = {chitil_2011_strictcheck.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/V5X2A4XQ/chitil_2011_strictcheck.pdf:application/pdf}
}

@inproceedings{gill_introducing_2006,
	address = {New York, NY, USA},
	series = {Haskell '06},
	title = {Introducing the {Haskell} {Equational} {Reasoning} {Assistant}},
	isbn = {1-59593-489-8},
	url = {http://doi.acm.org/10.1145/1159842.1159856},
	doi = {10.1145/1159842.1159856},
	abstract = {We introduce the new, improved version of the Haskell Equational Reasoning Assistant, which consists of an Ajax application for rewriting Haskell fragments in their context, and an API for scripting non-trivial rewrites.},
	urldate = {2015-05-21},
	booktitle = {Proceedings of the 2006 {ACM} {SIGPLAN} {Workshop} on {Haskell}},
	publisher = {ACM},
	author = {Gill, Andy},
	year = {2006},
	keywords = {ajax applications, equational reasoning, transformation tools},
	pages = {108--109},
	file = {gill_2006_introducing_the_haskell_equational_reasoning_assistant.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/3B6T8H2N/gill_2006_introducing_the_haskell_equational_reasoning_assistant.pdf:application/pdf}
}

@inproceedings{tomb_variably_2007,
	address = {New York, NY, USA},
	series = {{ISSTA} '07},
	title = {Variably {Interprocedural} {Program} {Analysis} for {Runtime} {Error} {Detection}},
	isbn = {978-1-59593-734-6},
	url = {http://doi.acm.org/10.1145/1273463.1273478},
	doi = {10.1145/1273463.1273478},
	abstract = {This paper describes an analysis approach based on a of static and dynamic techniques to ?nd run-time errors in Java code. It uses symbolic execution to ?nd constraints under which an error (e.g. a null pointer dereference, array out of bounds access, or assertion violation) may occur and then solves these constraints to ?nd test inputs that may expose the error. It only alerts the user to the possibility of a real error when it detects the expected exception during a program run. The analysis is customizable in two important ways. First, we can adjust how deeply to follow calls from each top-level method. Second, we can adjust the path termination tion for the symbolic execution engine to be either a bound on the path condition length or a bound on the number of times each instruction can be revisited. We evaluated the tool on a set of benchmarks from the literature as well as a number of real-world systems that range in size from a few thousand to 50,000 lines of code. The tool discovered all known errors in the benchmarks (as well as some not previously known) and reported on average 8 errors per 1000 lines of code for the industrial examples. In both cases the interprocedural call depth played little role in the error detection. That is, an intraprocedural analysis seems adequate for the class of errors we detect.},
	urldate = {2015-01-24},
	booktitle = {Proceedings of the 2007 {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Tomb, Aaron and Brat, Guillaume and Visser, Willem},
	year = {2007},
	keywords = {can-test, defect detection, generation, symbolic execution},
	pages = {97--107},
	file = {tomb_et_al_2007_variably_interprocedural_program_analysis_for_runtime_error_detection.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/INQTJAQ9/tomb_et_al_2007_variably_interprocedural_program_analysis_for_runtime_error_detection.pdf:application/pdf}
}

@article{jhala_software_2009,
	title = {Software {Model} {Checking}},
	volume = {41},
	issn = {0360-0300},
	url = {http://doi.acm.org/10.1145/1592434.1592438},
	doi = {10.1145/1592434.1592438},
	abstract = {We survey recent progress in software model checking.},
	number = {4},
	urldate = {2015-01-23},
	journal = {ACM Comput. Surv.},
	author = {Jhala, Ranjit and Majumdar, Rupak},
	month = oct,
	year = {2009},
	keywords = {abstraction, counterexample-guided refinement, enumerative and symbolic model checking, liveness, safety, Software model checking},
	pages = {21:1--21:54},
	file = {jhala_majumdar_2009_software_model_checking.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/FZ2GJ2VS/jhala_majumdar_2009_software_model_checking.pdf:application/pdf}
}

@book{burnstein_practical_2003,
	title = {Practical {Software} {Testing}: {A} {Process}-{Oriented} {Approach}},
	isbn = {978-0-387-95131-7},
	shorttitle = {Practical {Software} {Testing}},
	abstract = {Based on the needs of the educational community, and the software professional, this book takes a unique approach to teaching software testing. It introduces testing concepts that are managerial, technical, and process oriented, using the Testing Maturity Model (TMM) as a guiding framework. The TMM levels and goals support a structured presentation of fundamental and advanced test-related concepts to the reader. In this context, the interrelationships between theoretical, technical, and managerial concepts become more apparent. In addition, relationships between the testing process, maturity goals, and such key players as managers, testers and client groups are introduced. Topics and features:- Process/engineering-oriented text- Promotes the growth and value of software testing as a profession- Introduces both technical and managerial aspects of testing in a clear and precise style- Uses the TMM framework to introduce testing concepts in a systemmatic, evolutionary way to faciliate understanding- Describes the role of testing tools and measurements, and how to integrate them into the testing process Graduate students and industry professionals will benefit from the book, which is designed for a graduate course in software testing, software quality assurance, or software validation and verification Moreover, the number of universities with graduate courses that cover this material will grow, given the evoluation in software development as an engineering discipline and the creation of degree programs in software engineering.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Burnstein, Ilene},
	month = jun,
	year = {2003},
	keywords = {Computers / Information Technology, Computers / Programming / General, Computers / Programming Languages / General, Computers / Software Development \& Engineering / General, Computers / Software Development \& Engineering / Systems Analysis \& Design}
}

@inproceedings{marceau_measuring_2011,
	address = {New York, NY, USA},
	series = {{SIGCSE} '11},
	title = {Measuring the {Effectiveness} of {Error} {Messages} {Designed} for {Novice} {Programmers}},
	isbn = {978-1-4503-0500-6},
	url = {http://doi.acm.org/10.1145/1953163.1953308},
	doi = {10.1145/1953163.1953308},
	abstract = {Good error messages are critical for novice programmers. Re-cognizing this, the DrRacket programming environment provides a series of pedagogically-inspired language subsets with error messages customized to each subset. We apply human-factors research methods to explore the effectiveness of these messages. Unlike existing work in this area, we study messages at a fine-grained level by analyzing the edits students make in response to various classes of errors. We present a rubric (which is not language specific) to evaluate student responses, apply it to a course-worth of student lab work, and describe what we have learned about using the rubric effectively. We also discuss some concrete observations on the effectiveness of these messages.},
	urldate = {2015-09-24},
	booktitle = {Proceedings of the 42Nd {ACM} {Technical} {Symposium} on {Computer} {Science} {Education}},
	publisher = {ACM},
	author = {Marceau, Guillaume and Fisler, Kathi and Krishnamurthi, Shriram},
	year = {2011},
	keywords = {error messages, novice programmers, user-studies},
	pages = {499--504},
	file = {marceau_et_al_2011_measuring_the_effectiveness_of_error_messages_designed_for_novice_programmers.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/25Z35SRR/marceau_et_al_2011_measuring_the_effectiveness_of_error_messages_designed_for_novice_programmers.pdf:application/pdf}
}

@inproceedings{vytiniotis_halo:_2013,
	address = {New York, NY, USA},
	series = {{POPL} '13},
	title = {{HALO}: {Haskell} to {Logic} {Through} {Denotational} {Semantics}},
	isbn = {978-1-4503-1832-7},
	shorttitle = {{HALO}},
	url = {http://doi.acm.org/10.1145/2429069.2429121},
	doi = {10.1145/2429069.2429121},
	abstract = {Even well-typed programs can go wrong in modern functional languages, by encountering a pattern-match failure, or simply returning the wrong answer. An increasingly-popular response is to allow programmers to write contracts that express semantic properties, such as crash-freedom or some useful post-condition. We study the static verification of such contracts. Our main contribution is a novel translation to first-order logic of both Haskell programs, and contracts written in Haskell, all justified by denotational semantics. This translation enables us to prove that functions satisfy their contracts using an off-the-shelf first-order logic theorem prover.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 40th {Annual} {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Vytiniotis, Dimitrios and Peyton Jones, Simon and Claessen, Koen and Rosén, Dan},
	year = {2013},
	keywords = {first-order logic, static contract checking},
	pages = {431--442},
	file = {vytiniotis_et_al_2013_halo.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/AE8UKACF/vytiniotis_et_al_2013_halo.pdf:application/pdf}
}

@inproceedings{allwood_high_2011,
	address = {New York, NY, USA},
	series = {{ISSTA} '11},
	title = {High {Coverage} {Testing} of {Haskell} {Programs}},
	isbn = {978-1-4503-0562-4},
	url = {http://doi.acm.org/10.1145/2001420.2001465},
	doi = {10.1145/2001420.2001465},
	abstract = {This paper presents a new lightweight technique for automatically generating high coverage test suites for Haskell library code. Our approach combines four main features to increase test coverage: (1) automatically inferring the constructors and functions needed to generate test data; (2) using needed narrowing to take advantage of Haskell's lazy evaluation semantics; (3) inspecting elements inside returned data structures through the use of case statements, and (4) efficiently handling polymorphism by lazily instantiating all possible instances. We have implemented this technique in IRULAN, a fully automatic tool for systematic black-box unit testing of Haskell library code. We have designed IRULAN to generate high coverage test suites and detect common programming errors in the process. We have applied IRULAN to over 50 programs from the spectral and real suites of the nofib benchmark and show that it can effectively generate high-coverage test suites---exhibiting 70.83\% coverage for spectral and 59.78\% coverage for real---and find errors in these programs. Our techniques are general enough to be useful for several other types of testing, and we also discuss our experience of using IRULAN for property and regression testing.},
	urldate = {2015-06-05},
	booktitle = {Proceedings of the 2011 {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Allwood, Tristan and Cadar, Cristian and Eisenbach, Susan},
	year = {2011},
	keywords = {black-box testing, haskell, IRULAN, needed narrowing},
	pages = {375--385},
	file = {allwood_et_al_2011_high_coverage_testing_of_haskell_programs.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/IMW6W2QX/allwood_et_al_2011_high_coverage_testing_of_haskell_programs.pdf:application/pdf}
}

@inproceedings{st-amour_experience_2013,
	address = {New York, NY, USA},
	series = {{ICFP} '13},
	title = {Experience {Report}: {Applying} {Random} {Testing} to a {Base} {Type} {Environment}},
	isbn = {978-1-4503-2326-0},
	shorttitle = {Experience {Report}},
	url = {http://doi.acm.org/10.1145/2500365.2500616},
	doi = {10.1145/2500365.2500616},
	abstract = {As programmers, programming in typed languages increases our confidence in the correctness of our programs. As type system designers, soundness proofs increase our confidence in the correctness of our type systems. There is more to typed languages than their typing rules, however. To be usable, a typed language needs to provide a well-furnished standard library and to specify types for its exports. As software artifacts, these base type environments can rival typecheckers in complexity. Our experience with the Typed Racket base environment---which accounts for 31\% of the code in the Typed Racket implementation---teaches us that writing type environments can be just as error-prone as writing typecheckers. We report on our experience over the past two years of using random testing to increase our confidence in the correctness of the Typed Racket base environment.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 18th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {St-Amour, Vincent and Toronto, Neil},
	year = {2013},
	keywords = {numeric towers, random testing, type environments},
	pages = {351--356},
	file = {st-amour_toronto_2013_experience_report.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/JZAE2ER5/st-amour_toronto_2013_experience_report.pdf:application/pdf}
}

@article{brady_idris_2013,
	title = {Idris, a general-purpose dependently typed programming language: {Design} and implementation},
	volume = {23},
	issn = {1469-7653},
	shorttitle = {Idris, a general-purpose dependently typed programming language},
	url = {http://journals.cambridge.org/article_S095679681300018X},
	doi = {10.1017/S095679681300018X},
	abstract = {Many components of a dependently typed programming language are by now well understood, for example, the underlying type theory, type checking, unification and evaluation. How to combine these components into a realistic and usable high-level language is, however, folklore, discovered anew by successive language implementors. In this paper, I describe the implementation of Idris, a new dependently typed functional programming language. Idris is intended to be a general-purpose programming language and as such provides high-level concepts such as implicit syntax, type classes and do notation. I describe the high-level language and the underlying type theory, and present a tactic-based method for elaborating concrete high-level syntax with implicit arguments and type classes into a fully explicit type theory. Furthermore, I show how this method facilitates the implementation of new high-level language constructs.},
	number = {05},
	urldate = {2015-09-02},
	journal = {Journal of Functional Programming},
	author = {Brady, Edwin},
	year = {2013},
	pages = {552--593},
	file = {brady_2013_idris,_a_general-purpose_dependently_typed_programming_language.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/89Z55HTS/brady_2013_idris,_a_general-purpose_dependently_typed_programming_language.pdf:application/pdf;Cambridge Journals Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/BSVU66CJ/displayAbstract.html:text/html}
}

@article{ernst_dynamically_2001,
	title = {Dynamically discovering likely program invariants to support program evolution},
	volume = {27},
	issn = {0098-5589},
	doi = {10.1109/32.908957},
	abstract = {Explicitly stated program invariants can help programmers by identifying program properties that must be preserved when modifying code. In practice, however, these invariants are usually implicit. An alternative to expecting programmers to fully annotate code with invariants is to automatically infer likely invariants from the program itself. This research focuses on dynamic techniques for discovering invariants from execution traces. This article reports three results. First, it describes techniques for dynamically discovering invariants, along with an implementation, named Daikon, that embodies these techniques. Second, it reports on the application of Daikon to two sets of target programs. In programs from Gries's work (1981) on program derivation, the system rediscovered predefined invariants. In a C program lacking explicit invariants, the system discovered invariants that assisted a software evolution task. These experiments demonstrate that, at least for small programs, invariant inference is both accurate and useful. Third, it analyzes scalability issues, such as invariant detection runtime and accuracy, as functions of test suites and program points instrumented},
	number = {2},
	journal = {IEEE Transactions on Software Engineering},
	author = {Ernst, M.D. and Cockrell, J. and Griswold, William G. and Notkin, D.},
	month = feb,
	year = {2001},
	keywords = {Application software, Computer Society, Daikon, Detectors, execution traces, explicitly stated program invariants, Formal specifications, Instruments, invariant inference, likely program invariants, modifying code, Pattern analysis, program derivation, program evolution, Programming profession, program properties, reverse engineering, Runtime, scalability, small programs, software evolution, software maintenance, Testing},
	pages = {99--123},
	file = {ernst_et_al_2001_dynamically_discovering_likely_program_invariants_to_support_program_evolution.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/XHA2TMCQ/ernst_et_al_2001_dynamically_discovering_likely_program_invariants_to_support_program_evolution.pdf:application/pdf}
}

@inproceedings{de_vries_true_2014,
	address = {New York, NY, USA},
	series = {{WGP} '14},
	title = {True {Sums} of {Products}},
	isbn = {978-1-4503-3042-8},
	url = {http://doi.acm.org/10.1145/2633628.2633634},
	doi = {10.1145/2633628.2633634},
	abstract = {We introduce the sum-of-products (SOP) view for datatype-generic programming (in Haskell). While many of the libraries that are commonly in use today represent datatypes as arbitrary combinations of binary sums and products, SOP reflects the structure of datatypes more faithfully: each datatype is a single n-ary sum, where each component of the sum is a single n-ary product. This representation turns out to be expressible accurately in GHC with today's extensions. The resulting list-like structure of datatypes allows for the definition of powerful high-level traversal combinators, which in turn encourage the definition of generic functions in a compositional and concise style. A major plus of the SOP view is that it allows to separate function-specific metadata from the main structural representation and recombining this information later.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 10th {ACM} {SIGPLAN} {Workshop} on {Generic} {Programming}},
	publisher = {ACM},
	author = {de Vries, Edsko and Löh, Andres},
	year = {2014},
	keywords = {datatype-generic programming, generic views, json, lenses, metadata, sums of products, universes},
	pages = {83--94},
	file = {de_vries_löh_2014_true_sums_of_products.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/7XR44SKD/de_vries_löh_2014_true_sums_of_products.pdf:application/pdf}
}

@inproceedings{zhang_toward_2014,
	address = {New York, NY, USA},
	series = {{POPL} '14},
	title = {Toward {General} {Diagnosis} of {Static} {Errors}},
	isbn = {978-1-4503-2544-8},
	url = {http://doi.acm.org/10.1145/2535838.2535870},
	doi = {10.1145/2535838.2535870},
	abstract = {We introduce a general way to locate programmer mistakes that are detected by static analyses such as type checking. The program analysis is expressed in a constraint language in which mistakes result in unsatisfiable constraints. Given an unsatisfiable system of constraints, both satisfiable and unsatisfiable constraints are analyzed, to identify the program expressions most likely to be the cause of unsatisfiability. The likelihood of different error explanations is evaluated under the assumption that the programmer's code is mostly correct, so the simplest explanations are chosen, following Bayesian principles. For analyses that rely on programmer-stated assumptions, the diagnosis also identifies assumptions likely to have been omitted. The new error diagnosis approach has been implemented for two very different program analyses: type inference in OCaml and information flow checking in Jif. The effectiveness of the approach is evaluated using previously collected programs containing errors. The results show that when compared to existing compilers and other tools, the general technique identifies the location of programmer errors significantly more accurately.},
	urldate = {2015-01-21},
	booktitle = {Proceedings of the 41st {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Zhang, Danfeng and Myers, Andrew C.},
	year = {2014},
	keywords = {error diagnosis, information flow, static program analysis, type inference},
	pages = {569--581},
	file = {zhang_myers_2014_toward_general_diagnosis_of_static_errors.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/8WP6HDZC/zhang_myers_2014_toward_general_diagnosis_of_static_errors.pdf:application/pdf}
}

@inproceedings{angiuli_homotopical_2014,
	address = {New York, NY, USA},
	series = {{ICFP} '14},
	title = {Homotopical {Patch} {Theory}},
	isbn = {978-1-4503-2873-9},
	url = {http://doi.acm.org/10.1145/2628136.2628158},
	doi = {10.1145/2628136.2628158},
	abstract = {Homotopy type theory is an extension of Martin-Löf type theory, based on a correspondence with homotopy theory and higher category theory. In homotopy type theory, the propositional equality type becomes proof-relevant, and corresponds to paths in a space. This allows for a new class of datatypes, called higher inductive types, which are specified by constructors not only for points but also for paths. In this paper, we consider a programming application of higher inductive types. Version control systems such as Darcs are based on the notion of patches - syntactic representations of edits to a repository. We show how patch theory can be developed in homotopy type theory. Our formulation separates formal theories of patches from their interpretation as edits to repositories. A patch theory is presented as a higher inductive type. Models of a patch theory are given by maps out of that type, which, being functors, automatically preserve the structure of patches. Several standard tools of homotopy theory come into play, demonstrating the use of these methods in a practical programming context.},
	urldate = {2015-02-05},
	booktitle = {Proceedings of the 19th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Angiuli, Carlo and Morehouse, Edward and Licata, Daniel R. and Harper, Robert},
	year = {2014},
	keywords = {languages, theory},
	pages = {243--256},
	file = {angiuli_et_al_2014_homotopical_patch_theory.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/9WVR7VH8/angiuli_et_al_2014_homotopical_patch_theory.pdf:application/pdf}
}

@inproceedings{raychev_refactoring_2013,
	address = {New York, NY, USA},
	series = {{OOPSLA} '13},
	title = {Refactoring with {Synthesis}},
	isbn = {978-1-4503-2374-1},
	url = {http://doi.acm.org/10.1145/2509136.2509544},
	doi = {10.1145/2509136.2509544},
	abstract = {Refactoring has become an integral part of modern software development, with wide support in popular integrated development environments (IDEs). Modern IDEs provide a fixed set of supported refactorings, listed in a refactoring menu. But with IDEs supporting more and more refactorings, it is becoming increasingly difficult for programmers to discover and memorize all their names and meanings. Also, since the set of refactorings is hard-coded, if a programmer wants to achieve a slightly different code transformation, she has to either apply a (possibly non-obvious) sequence of several built-in refactorings, or just perform the transformation by hand. We propose a novel approach to refactoring, based on synthesis from examples, which addresses these limitations. With our system, the programmer need not worry how to invoke individual refactorings or the order in which to apply them. Instead, a transformation is achieved via three simple steps: the programmer first indicates the start of a code refactoring phase; then she performs some of the desired code changes manually; and finally, she asks the tool to complete the refactoring. Our system completes the refactoring by first extracting the difference between the starting program and the modified version, and then synthesizing a sequence of refactorings that achieves (at least) the desired changes. To enable scalable synthesis, we introduce local refactorings, which allow for first discovering a refactoring sequence on small program fragments and then extrapolating it to a full refactoring sequence. We implemented our approach as an Eclipse plug-in, with an architecture that is easily extendable with new refactorings. The experimental results are encouraging: with only minimal user input, the synthesizer was able to quickly discover complex refactoring sequences for several challenging realistic examples.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 2013 {ACM} {SIGPLAN} {International} {Conference} on {Object} {Oriented} {Programming} {Systems} {Languages} \& {Applications}},
	publisher = {ACM},
	author = {Raychev, Veselin and Schäfer, Max and Sridharan, Manu and Vechev, Martin},
	year = {2013},
	keywords = {refactoring, synthesis},
	pages = {339--354},
	file = {raychev_et_al_2013_refactoring_with_synthesis.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/K2TX4D4C/raychev_et_al_2013_refactoring_with_synthesis.pdf:application/pdf}
}

@incollection{neron_theory_2015,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Theory} of {Name} {Resolution}},
	copyright = {©2015 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-662-46668-1 978-3-662-46669-8},
	url = {http://link.springer.com/chapter/10.1007/978-3-662-46669-8_9},
	abstract = {We describe a language-independent theory for name binding and resolution, suitable for programming languages with complex scoping rules including both lexical scoping and modules. We formulate name resolution as a two-stage problem. First a language-independent scope graph is constructed using language-specific rules from an abstract syntax tree. Then references in the scope graph are resolved to corresponding declarations using a language-independent resolution process. We introduce a resolution calculus as a concise, declarative, and languageindependent specification of name resolution. We develop a resolution algorithm that is sound and complete with respect to the calculus. Based on the resolution calculus we develop language-independent definitions of α-equivalence and rename refactoring. We illustrate the approach using a small example language with modules. In addition, we show how our approach provides a model for a range of name binding patterns in existing languages.},
	language = {en},
	number = {9032},
	urldate = {2015-04-27},
	booktitle = {Programming {Languages} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Neron, Pierre and Tolmach, Andrew and Visser, Eelco and Wachsmuth, Guido},
	editor = {Vitek, Jan},
	month = apr,
	year = {2015},
	keywords = {Algorithm Analysis and Problem Complexity, Artificial Intelligence (incl. Robotics), Database Management, Data Mining and Knowledge Discovery, Information Storage and Retrieval, Information Systems Applications (incl. Internet)},
	pages = {205--231},
	file = {neron_et_al_2015_a_theory_of_name_resolution.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/69KX5S9E/neron_et_al_2015_a_theory_of_name_resolution.pdf:application/pdf}
}

@incollection{sparud_tracing_1997,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Tracing lazy functional computations using redex trails},
	copyright = {©1997 Springer-Verlag},
	isbn = {978-3-540-63398-3 978-3-540-69537-0},
	url = {http://link.springer.com/chapter/10.1007/BFb0033851},
	abstract = {We describe the design and implementation of a system for tracing computations in a lazy functional language. The basis of our tracing method is a program transformation carried out by the compiler: transformed programs compute the same values as the original, but embedded in functional data structures that also include redex trails showing how the values were obtained. A special-purpose display program enables detailed but selective exploration of the redex trails, with cross-links to the source program.},
	language = {en},
	number = {1292},
	urldate = {2015-10-29},
	booktitle = {Programming {Languages}: {Implementations}, {Logics}, and {Programs}},
	publisher = {Springer Berlin Heidelberg},
	author = {Sparud, Jan and Runciman, Colin},
	editor = {Glaser, Hugh and Hartel, Pieter and Kuchen, Herbert},
	month = sep,
	year = {1997},
	note = {DOI: 10.1007/BFb0033851},
	keywords = {Artificial Intelligence (incl. Robotics), debugging, graph reduction, haskell, Logics and Meanings of Programs, Mathematical Logic and Formal Languages, Programming Languages, Compilers, Interpreters, Programming Techniques, program transformation},
	pages = {291--308},
	file = {Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/X5PFGIQ9/10.html:text/html;sparud_runciman_1997_tracing_lazy_functional_computations_using_redex_trails.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/PR2PPASK/sparud_runciman_1997_tracing_lazy_functional_computations_using_redex_trails.pdf:application/pdf}
}

@inproceedings{tobin-hochstadt_higher-order_2012,
	address = {New York, NY, USA},
	series = {{OOPSLA} '12},
	title = {Higher-order {Symbolic} {Execution} via {Contracts}},
	isbn = {978-1-4503-1561-6},
	url = {http://doi.acm.org/10.1145/2384616.2384655},
	doi = {10.1145/2384616.2384655},
	abstract = {We present a new approach to automated reasoning about higher-order programs by extending symbolic execution to use behavioral contracts as symbolic values, thus enabling symbolic approximation of higher-order behavior. Our approach is based on the idea of an abstract reduction semantics that gives an operational semantics to programs with both concrete and symbolic components. Symbolic components are approximated by their contract and our semantics gives an operational interpretation of contracts-as-values. The result is an executable semantics that soundly predicts program behavior, including contract failures, for all possible instantiations of symbolic components. We show that our approach scales to an expressive language of contracts including arbitrary programs embedded as predicates, dependent function contracts, and recursive contracts. Supporting this rich language of specifications leads to powerful symbolic reasoning using existing program constructs. We then apply our approach to produce a verifier for contract correctness of components, including a sound and computable approximation to our semantics that facilitates fully automated contract verification. Our implementation is capable of verifying contracts expressed in existing programs, and of justifying contract-elimination optimizations.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the {ACM} {International} {Conference} on {Object} {Oriented} {Programming} {Systems} {Languages} and {Applications}},
	publisher = {ACM},
	author = {Tobin-Hochstadt, Sam and Van Horn, David},
	year = {2012},
	keywords = {higher-order contracts, Reduction semantics, symbolic execution},
	pages = {537--554},
	file = {tobin-hochstadt_van_horn_2012_higher-order_symbolic_execution_via_contracts.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/2TWCQFKF/tobin-hochstadt_van_horn_2012_higher-order_symbolic_execution_via_contracts.pdf:application/pdf}
}

@inproceedings{grabmayer_maximal_2014,
	address = {New York, NY, USA},
	series = {{ICFP} '14},
	title = {Maximal {Sharing} in the {Lambda} {Calculus} with {Letrec}},
	isbn = {978-1-4503-2873-9},
	url = {http://doi.acm.org/10.1145/2628136.2628148},
	doi = {10.1145/2628136.2628148},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 19th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Grabmayer, Clemens and Rochel, Jan},
	year = {2014},
	keywords = {higher-order term graphs, lambda calculus with letrec, maximal sharing, subterm sharing, unfolding semantics},
	pages = {67--80},
	file = {grabmayer_rochel_2014_maximal_sharing_in_the_lambda_calculus_with_letrec.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/9KHTM4VD/grabmayer_rochel_2014_maximal_sharing_in_the_lambda_calculus_with_letrec.pdf:application/pdf}
}

@inproceedings{nguyen_relatively_2015,
	address = {New York, NY, USA},
	series = {{PLDI} 2015},
	title = {Relatively {Complete} {Counterexamples} for {Higher}-order {Programs}},
	isbn = {978-1-4503-3468-6},
	url = {http://doi.acm.org/10.1145/2737924.2737971},
	doi = {10.1145/2737924.2737971},
	abstract = {In this paper, we study the problem of generating inputs to a higher-order program causing it to error. We first approach the problem in the setting of PCF, a typed, core functional language and contribute the first relatively complete method for constructing counterexamples for PCF programs. The method is relatively complete with respect to a first-order solver over the base types of PCF. In practice, this means an SMT solver can be used for the effective, automated generation of higher-order counterexamples for a large class of programs. We achieve this result by employing a novel form of symbolic execution for higher-order programs. The remarkable aspect of this symbolic execution is that even though symbolic higher-order inputs and values are considered, the path condition remains a first-order formula. Our handling of symbolic function application enables the reconstruction of higher-order counterexamples from this first-order formula. After establishing our main theoretical results, we sketch how to apply the approach to untyped, higher-order, stateful languages with first-class contracts and show how counterexample generation can be used to detect contract violations in this setting. To validate our approach, we implement a tool generating counterexamples for erroneous modules written in Racket.},
	urldate = {2015-06-12},
	booktitle = {Proceedings of the 36th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Nguyễn, Phúc C. and Van Horn, David},
	year = {2015},
	keywords = {Contracts, Higher-order programs, symbolic execution},
	pages = {446--456},
	file = {nguyễn_van_horn_2015_relatively_complete_counterexamples_for_higher-order_programs.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/VQIPZHQK/nguyễn_van_horn_2015_relatively_complete_counterexamples_for_higher-order_programs.pdf:application/pdf}
}

@article{jackson_elements_1996,
	title = {Elements of style: analyzing a software design feature with a counterexample detector},
	volume = {22},
	issn = {0098-5589},
	shorttitle = {Elements of style},
	doi = {10.1109/32.538605},
	abstract = {Demonstrates how Nitpick, a specification checker, can be applied to the design of a style mechanism for a word processor. The design is cast, along with some expected properties, in a subset of Z. Nitpick checks a property by enumerating all possible cases within some finite bounds, displaying as a counterexample the first case for which the property fails to hold. Unlike animation or execution tools, Nitpick does not require state transitions to be expressed constructively, and unlike theorem provers, Nitpick operates completely automatically without user intervention. Using a variety of reduction mechanisms, it can cover an enormous number of cases in a reasonable time, so that subtle flaws can be rapidly detected},
	number = {7},
	journal = {IEEE Transactions on Software Engineering},
	author = {Jackson, D. and Damon, C.A.},
	month = jul,
	year = {1996},
	keywords = {abstract modeling, Animation, automatic operation, case enumeration, Computer architecture, counterexample detector, Detectors, exhaustive testing, finite bounds, Formal languages, formal specification, formal specification checker, Formal specifications, Hardware, model checking, Nitpick, Process design, program testing, program verification, Protocols, reduction mechanisms, Software design, software design feature analysis, software testing, state transitions, subtle flaw detection, word processing, word processor style mechanism, Z notation, Z specification language subset},
	pages = {484--495},
	file = {jackson_damon_1996_elements_of_style.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/FC76X8MB/jackson_damon_1996_elements_of_style.pdf:application/pdf}
}

@inproceedings{brady_programming_2013,
	address = {New York, NY, USA},
	series = {{ICFP} '13},
	title = {Programming and {Reasoning} with {Algebraic} {Effects} and {Dependent} {Types}},
	isbn = {978-1-4503-2326-0},
	url = {http://doi.acm.org/10.1145/2500365.2500581},
	doi = {10.1145/2500365.2500581},
	abstract = {One often cited benefit of pure functional programming is that pure code is easier to test and reason about, both formally and informally. However, real programs have side-effects including state management, exceptions and interactions with the outside world. Haskell solves this problem using monads to capture details of possibly side-effecting computations --- it provides monads for capturing state, I/O, exceptions, non-determinism, libraries for practical purposes such as CGI and parsing, and many others, as well as monad transformers for combining multiple effects. Unfortunately, useful as monads are, they do not compose very well. Monad transformers can quickly become unwieldy when there are lots of effects to manage, leading to a temptation in larger programs to combine everything into one coarse-grained state and exception monad. In this paper I describe an alternative approach based on handling algebraic effects, implemented in the IDRIS programming language. I show how to describe side effecting computations, how to write programs which compose multiple fine-grained effects, and how, using dependent types, we can use this approach to reason about states in effectful programs.},
	urldate = {2015-06-02},
	booktitle = {Proceedings of the 18th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Brady, Edwin},
	year = {2013},
	keywords = {algebraic effects, dependent types},
	pages = {133--144},
	file = {brady_2013_programming_and_reasoning_with_algebraic_effects_and_dependent_types.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/5E3NQX8Q/brady_2013_programming_and_reasoning_with_algebraic_effects_and_dependent_types.pdf:application/pdf}
}

@article{adrion_validation_1982,
	title = {Validation, {Verification}, and {Testing} of {Computer} {Software}},
	volume = {14},
	issn = {0360-0300},
	url = {http://doi.acm.org/10.1145/356876.356879},
	doi = {10.1145/356876.356879},
	number = {2},
	urldate = {2015-01-27},
	journal = {ACM Comput. Surv.},
	author = {Adrion, W. Richards and Branstad, Martha A. and Cherniavsky, John C.},
	month = jun,
	year = {1982},
	pages = {159--192},
	file = {adrion_et_al_1982_validation,_verification,_and_testing_of_computer_software.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/KHEPJTK5/adrion_et_al_1982_validation,_verification,_and_testing_of_computer_software.pdf:application/pdf}
}

@incollection{fetscher_making_2015,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Making {Random} {Judgments}: {Automatically} {Generating} {Well}-{Typed} {Terms} from the {Definition} of a {Type}-{System}},
	copyright = {©2015 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-662-46668-1 978-3-662-46669-8},
	shorttitle = {Making {Random} {Judgments}},
	url = {http://link.springer.com/chapter/10.1007/978-3-662-46669-8_16},
	abstract = {This paper presents a generic method for randomly generating well-typed expressions. It starts from a specification of a typing judgment in PLT Redex and uses a specialized solver that employs randomness to find many different valid derivations of the judgment form. Our motivation for building these random terms is to more effectively falsify conjectures as part of the tool-support for semantics models specified in Redex. Accordingly, we evaluate the generator against the other available methods for Redex, as well as the best available custom well-typed term generator. Our results show that our new generator is much more effective than generation techniques that do not explicitly take types into account and is competitive with generation techniques that do, even though they are specialized to particular type-systems and ours is not.},
	language = {en},
	number = {9032},
	urldate = {2015-04-27},
	booktitle = {Programming {Languages} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Fetscher, Burke and Claessen, Koen and Pałka, Michał and Hughes, John and Findler, Robert Bruce},
	editor = {Vitek, Jan},
	month = apr,
	year = {2015},
	keywords = {Algorithm Analysis and Problem Complexity, Artificial Intelligence (incl. Robotics), Database Management, Data Mining and Knowledge Discovery, Information Storage and Retrieval, Information Systems Applications (incl. Internet)},
	pages = {383--405},
	file = {fetscher_et_al_2015_making_random_judgments.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/TF238QHF/fetscher_et_al_2015_making_random_judgments.pdf:application/pdf}
}

@inproceedings{cadar_symbolic_2011,
	address = {New York, NY, USA},
	series = {{ICSE} '11},
	title = {Symbolic {Execution} for {Software} {Testing} in {Practice}: {Preliminary} {Assessment}},
	isbn = {978-1-4503-0445-0},
	shorttitle = {Symbolic {Execution} for {Software} {Testing} in {Practice}},
	url = {http://doi.acm.org/10.1145/1985793.1985995},
	doi = {10.1145/1985793.1985995},
	abstract = {We present results for the "Impact Project Focus Area" on the topic of symbolic execution as used in software testing. Symbolic execution is a program analysis technique introduced in the 70s that has received renewed interest in recent years, due to algorithmic advances and increased availability of computational power and constraint solving technology. We review classical symbolic execution and some modern extensions such as generalized symbolic execution and dynamic test generation. We also give a preliminary assessment of the use in academia, research labs, and industry.},
	urldate = {2015-01-22},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Cadar, Cristian and Godefroid, Patrice and Khurshid, Sarfraz and Păsăreanu, Corina S. and Sen, Koushik and Tillmann, Nikolai and Visser, Willem},
	year = {2011},
	keywords = {dynamic test generation, generalized symbolic execution},
	pages = {1066--1071},
	file = {cadar_et_al_2011_symbolic_execution_for_software_testing_in_practice.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/CSVNJGH3/cadar_et_al_2011_symbolic_execution_for_software_testing_in_practice.pdf:application/pdf}
}

@inproceedings{avgerinos_aeg:_2011,
	title = {{AEG}: {Automatic} {Exploit} {Generation}.},
	volume = {11},
	shorttitle = {{AEG}},
	url = {http://security.ece.cmu.edu/aeg/aeg-current.pdf},
	urldate = {2015-01-25},
	booktitle = {{NDSS}},
	author = {Avgerinos, Thanassis and Cha, Sang Kil and Hao, Brent Lim Tze and Brumley, David},
	year = {2011},
	pages = {59--66},
	file = {avgerinos_et_al_2011_aeg.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/TX28QM3T/avgerinos_et_al_2011_aeg.pdf:application/pdf}
}

@inproceedings{gligoric_test_2010,
	address = {New York, NY, USA},
	series = {{ICSE} '10},
	title = {Test {Generation} {Through} {Programming} in {UDITA}},
	isbn = {978-1-60558-719-6},
	url = {http://doi.acm.org/10.1145/1806799.1806835},
	doi = {10.1145/1806799.1806835},
	abstract = {We present an approach for describing tests using non-deterministic test generation programs. To write such programs, we introduce UDITA, a Java-based language with non-deterministic choice operators and an interface for generating linked structures. We also describe new algorithms that generate concrete tests by efficiently exploring the space of all executions of non-deterministic UDITA programs. We implemented our approach and incorporated it into the official, publicly available repository of Java PathFinder (JPF), a popular tool for verifying Java programs. We evaluate our technique by generating tests for data structures, refactoring engines, and JPF itself. Our experiments show that test generation using UDITA is faster and leads to test descriptions that are easier to write than in previous frameworks. Moreover, the novel execution mechanism of UDITA is essential for making test generation feasible. Using UDITA, we have discovered a number of bugs in Eclipse, NetBeans, Sun javac, and JPF.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 32Nd {ACM}/{IEEE} {International} {Conference} on {Software} {Engineering} - {Volume} 1},
	publisher = {ACM},
	author = {Gligoric, Milos and Gvero, Tihomir and Jagannath, Vilas and Khurshid, Sarfraz and Kuncak, Viktor and Marinov, Darko},
	year = {2010},
	keywords = {automated testing, Java PathFinder, Pex, test filtering, test generation, test predicates, test programs, UDITA},
	pages = {225--234},
	file = {gligoric_et_al_2010_test_generation_through_programming_in_udita.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/VJ579IB2/gligoric_et_al_2010_test_generation_through_programming_in_udita.pdf:application/pdf}
}

@article{pierce_local_2000,
	title = {Local {Type} {Inference}},
	volume = {22},
	issn = {0164-0925},
	url = {http://doi.acm.org/10.1145/345099.345100},
	doi = {10.1145/345099.345100},
	abstract = {We study two partial type inference methods for a language combining subtyping and impredicative polymorphism. Both methods are local in the sense that missing annotations are recovered using only information from adjacent nodes in the syntax tree, without long-distance constraints such as unification variables. One method infers type arguments in polymorphic applications using a local constraint solver. The other infers annotations on bound variables in function abstractions by propagating type constraints downward from enclosing application nodes. We motivate our design choices by a statistical analysis of the uses of type inference in a sizable body of existing ML code.},
	number = {1},
	urldate = {2015-05-27},
	journal = {ACM Trans. Program. Lang. Syst.},
	author = {Pierce, Benjamin C. and Turner, David N.},
	month = jan,
	year = {2000},
	keywords = {polymorphism, subtyping, type inference},
	pages = {1--44},
	file = {pierce_turner_2000_local_type_inference.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/3PA6HMRX/pierce_turner_2000_local_type_inference.pdf:application/pdf}
}

@inproceedings{dolby_finding_2007,
	address = {New York, NY, USA},
	series = {{ESEC}-{FSE} '07},
	title = {Finding {Bugs} {Efficiently} with a {SAT} {Solver}},
	isbn = {978-1-59593-811-4},
	url = {http://doi.acm.org/10.1145/1287624.1287653},
	doi = {10.1145/1287624.1287653},
	abstract = {We present an approach for checking code against rich specifications, based on existing work that consists of encoding the program in a relational logic and using a constraint solver to find specification violations. We improve the efficiency of this approach with a new encoding of the program that effectively slices it at the logical level with respect to the specification. We also present new encodings for integer values and arrays, enabling the verification of realistic fragments of code that manipulate both. Our technique can handle integers of much larger ranges than previously possible, and permits large sparse arrays to be handled efficiently. We present a soundness proof for our slicing algorithm and a general condition under which relational formulae may be sliced. We implemented our technique and evaluated it by checking data structure invariants of several classes taken from the Java Collections Framework. We also checked for violations of Java's equality contract in a variety of open-source programs, and found several bugs.},
	urldate = {2015-02-12},
	booktitle = {Proceedings of the the 6th {Joint} {Meeting} of the {European} {Software} {Engineering} {Conference} and the {ACM} {SIGSOFT} {Symposium} on {The} {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Dolby, Julian and Vaziri, Mandana and Tip, Frank},
	year = {2007},
	keywords = {model checking, SAT solving, sat solving, slicing, specification},
	pages = {195--204},
	file = {dolby_et_al_2007_finding_bugs_efficiently_with_a_sat_solver.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/NA4HMFVV/dolby_et_al_2007_finding_bugs_efficiently_with_a_sat_solver.pdf:application/pdf}
}

@inproceedings{hsiao_using_2014,
	address = {New York, NY, USA},
	series = {{OOPSLA} '14},
	title = {Using {Web} {Corpus} {Statistics} for {Program} {Analysis}},
	isbn = {978-1-4503-2585-1},
	url = {http://doi.acm.org/10.1145/2660193.2660226},
	doi = {10.1145/2660193.2660226},
	abstract = {Several program analysis tools - such as plagiarism detection and bug finding - rely on knowing a piece of code's relative semantic importance. For example, a plagiarism detector should not bother reporting two programs that have an identical simple loop counter test, but should report programs that share more distinctive code. Traditional program analysis techniques (e.g., finding data and control dependencies) are useful, but do not say how surprising or common a line of code is. Natural language processing researchers have encountered a similar problem and addressed it using an n-gram model of text frequency, derived from statistics computed over text corpora. We propose and compute an n-gram model for programming languages, computed over a corpus of 2.8 million JavaScript programs we downloaded from the Web. In contrast to previous techniques, we describe a code n-gram as a subgraph of the program dependence graph that contains all nodes and edges reachable in n steps from the statement. We can count n-grams in a program and count the frequency of n-grams in the corpus, enabling us to compute tf-idf-style measures that capture the differing importance of different lines of code. We demonstrate the power of this approach by implementing a plagiarism detector with accuracy that beats previous techniques, and a bug-finding tool that discovered over a dozen previously unknown bugs in a collection of real deployed programs.},
	urldate = {2015-04-27},
	booktitle = {Proceedings of the 2014 {ACM} {International} {Conference} on {Object} {Oriented} {Programming} {Systems} {Languages} \&\#38; {Applications}},
	publisher = {ACM},
	author = {Hsiao, Chun-Hung and Cafarella, Michael and Narayanasamy, Satish},
	year = {2014},
	keywords = {copy-paste bug, corpus-driven, javascript, plagiarism detection, programmatic n-gram},
	pages = {49--65},
	file = {hsiao_et_al_2014_using_web_corpus_statistics_for_program_analysis.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/5WT9V87R/hsiao_et_al_2014_using_web_corpus_statistics_for_program_analysis.pdf:application/pdf}
}

@inproceedings{chlipala_optimizing_2015,
	address = {New York, NY, USA},
	series = {{ICFP} 2015},
	title = {An {Optimizing} {Compiler} for a {Purely} {Functional} {Web}-application {Language}},
	isbn = {978-1-4503-3669-7},
	url = {http://doi.acm.org/10.1145/2784731.2784741},
	doi = {10.1145/2784731.2784741},
	abstract = {High-level scripting languages have become tremendously popular for development of dynamic Web applications. Many programmers appreciate the productivity benefits of automatic storage management, freedom from verbose type annotations, and so on. While it is often possible to improve performance substantially by rewriting an application in C or a similar language, very few programmers bother to do so, because of the consequences for human development effort. This paper describes a compiler that makes it possible to have most of the best of both worlds, coding Web applications in a high-level language but compiling to native code with performance comparable to handwritten C code. The source language is Ur/Web, a domain-specific, purely functional, statically typed language for the Web. Through a coordinated suite of relatively straightforward program analyses and algebraic optimizations, we transform Ur/Web programs into almost-idiomatic C code, with no garbage collection, little unnecessary memory allocation for intermediate values, etc. Our compiler is in production use for commercial Web sites supporting thousands of users, and microbenchmarks demonstrate very competitive performance versus mainstream tools.},
	urldate = {2015-09-03},
	booktitle = {Proceedings of the 20th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Chlipala, Adam},
	year = {2015},
	keywords = {pure functional programming, Web programming languages, whole-program optimization},
	pages = {10--21},
	file = {chlipala_2015_an_optimizing_compiler_for_a_purely_functional_web-application_language.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/KRF9DVB3/chlipala_2015_an_optimizing_compiler_for_a_purely_functional_web-application_language.pdf:application/pdf}
}

@inproceedings{gill_haskell_2007,
	address = {New York, NY, USA},
	series = {Haskell '07},
	title = {Haskell {Program} {Coverage}},
	isbn = {978-1-59593-674-5},
	url = {http://doi.acm.org/10.1145/1291201.1291203},
	doi = {10.1145/1291201.1291203},
	abstract = {We describe the design, implementation and use of HPC, a tool-kit to record and display Haskell Program Coverage. HPC includes tools that instrument Haskell programs to record program coverage, run instrumented programs, and display information derived from coverage data in various ways.},
	urldate = {2015-05-29},
	booktitle = {Proceedings of the {ACM} {SIGPLAN} {Workshop} on {Haskell} {Workshop}},
	publisher = {ACM},
	author = {Gill, Andy and Runciman, Colin},
	year = {2007},
	keywords = {code coverage, haskell, Software Engineering},
	pages = {1--12},
	file = {gill_runciman_2007_haskell_program_coverage.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/TD2QC7NT/gill_runciman_2007_haskell_program_coverage.pdf:application/pdf}
}

@inproceedings{isradisaikul_finding_2015,
	address = {New York, NY, USA},
	series = {{PLDI} 2015},
	title = {Finding {Counterexamples} from {Parsing} {Conflicts}},
	isbn = {978-1-4503-3468-6},
	url = {http://doi.acm.org/10.1145/2737924.2737961},
	doi = {10.1145/2737924.2737961},
	abstract = {Writing a parser remains remarkably painful. Automatic parser generators offer a powerful and systematic way to parse complex grammars, but debugging conflicts in grammars can be time-consuming even for experienced language designers. Better tools for diagnosing parsing conflicts will alleviate this difficulty. This paper proposes a practical algorithm that generates compact, helpful counterexamples for LALR grammars. For each parsing conflict in a grammar, a counterexample demonstrating the conflict is constructed. When the grammar in question is ambiguous, the algorithm usually generates a compact counterexample illustrating the ambiguity. This algorithm has been implemented as an extension to the CUP parser generator. The results from applying this implementation to a diverse collection of faulty grammars show that the algorithm is practical, effective, and suitable for inclusion in other LALR parser generators.},
	urldate = {2015-06-15},
	booktitle = {Proceedings of the 36th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Isradisaikul, Chinawat and Myers, Andrew C.},
	year = {2015},
	keywords = {ambiguous grammar, Context-free grammar, error diagnosis, lookahead-sensitive path, product parser, shift-reduce parser},
	pages = {555--564},
	file = {isradisaikul_myers_2015_finding_counterexamples_from_parsing_conflicts.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/IUBMPMRF/isradisaikul_myers_2015_finding_counterexamples_from_parsing_conflicts.pdf:application/pdf}
}

@inproceedings{bahr_composing_2014,
	address = {New York, NY, USA},
	series = {{WGP} '14},
	title = {Composing and {Decomposing} {Data} {Types}: {A} {Closed} {Type} {Families} {Implementation} of {Data} {Types} à {La} {Carte}},
	isbn = {978-1-4503-3042-8},
	shorttitle = {Composing and {Decomposing} {Data} {Types}},
	url = {http://doi.acm.org/10.1145/2633628.2633635},
	doi = {10.1145/2633628.2633635},
	abstract = {Wouter Swierstra's data types à la carte is a technique to modularise data type definitions in Haskell. We give an alternative implementation of data types à la carte that offers more flexibility in composing and decomposing data types. To achieve this, we refine the subtyping constraint, which is at the centre of data types à la carte. On the one hand this refinement is more general, allowing subtypings that intuitively should hold but were not derivable beforehand. This aspect of our implementation removes previous restrictions on how data types can be combined. On the other hand our refinement is more restrictive, disallowing subtypings that lead to more than one possible injection and should therefore be considered programming errors. Furthermore, from this refined subtyping constraint we derive a new constraint to express type isomorphism. We show how this isomorphism constraint allows us to decompose data types and to define extensible functions on data types in an ad hoc manner. The implementation makes essential use of closed type families in Haskell. The use of closed type families instead of type classes comes with a set of trade-offs, which we review in detail. Finally, we show that our technique can be used for other similar problem domains.},
	urldate = {2015-06-19},
	booktitle = {Proceedings of the 10th {ACM} {SIGPLAN} {Workshop} on {Generic} {Programming}},
	publisher = {ACM},
	author = {Bahr, Patrick},
	year = {2014},
	keywords = {\_tablet, closed type families, expression problem, Modularity, two-level types},
	pages = {71--82},
	file = {bahr_2014_composing_and_decomposing_data_types.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/KGCI44FJ/bahr_2014_composing_and_decomposing_data_types.pdf:application/pdf}
}

@article{meyer_applying_1992,
	title = {Applying 'design by contract'},
	volume = {25},
	issn = {0018-9162},
	doi = {10.1109/2.161279},
	abstract = {Methodological guidelines for object-oriented software construction that improve the reliability of the resulting software systems are presented. It is shown that the object-oriented techniques rely on the theory of design by contract, which underlies the design of the Eiffel analysis, design, and programming language and of the supporting libraries, from which a number of examples are drawn. The theory of contract design and the role of assertions in that theory are discussed.{\textless}{\textgreater}},
	number = {10},
	journal = {Computer},
	author = {Meyer, B.},
	month = oct,
	year = {1992},
	keywords = {Books, Computer bugs, Contracts, design by contract, Eiffel, Guidelines, Object oriented programming, object-oriented programming, object-oriented software construction, object-oriented techniques, Pressing, programming language, Reliability theory, Robustness, Software Engineering, software libraries, software reliability, software reusability, Software systems},
	pages = {40--51},
	file = {IEEE Xplore Abstract Record:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/A634XE9C/abs_all.html:text/html}
}

@inproceedings{panchekha_automatically_2015,
	address = {New York, NY, USA},
	series = {{PLDI} 2015},
	title = {Automatically {Improving} {Accuracy} for {Floating} {Point} {Expressions}},
	isbn = {978-1-4503-3468-6},
	url = {http://doi.acm.org/10.1145/2737924.2737959},
	doi = {10.1145/2737924.2737959},
	abstract = {Scientific and engineering applications depend on floating point arithmetic to approximate real arithmetic. This approximation introduces rounding error, which can accumulate to produce unacceptable results. While the numerical methods literature provides techniques to mitigate rounding error, applying these techniques requires manually rearranging expressions and understanding the finer details of floating point arithmetic. We introduce Herbie, a tool which automatically discovers the rewrites experts perform to improve accuracy. Herbie's heuristic search estimates and localizes rounding error using sampled points (rather than static error analysis), applies a database of rules to generate improvements, takes series expansions, and combines improvements for different input regions. We evaluated Herbie on examples from a classic numerical methods textbook, and found that Herbie was able to improve accuracy on each example, some by up to 60 bits, while imposing a median performance overhead of 40\%. Colleagues in machine learning have used Herbie to significantly improve the results of a clustering algorithm, and a mathematical library has accepted two patches generated using Herbie.},
	urldate = {2015-06-15},
	booktitle = {Proceedings of the 36th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Panchekha, Pavel and Sanchez-Stern, Alex and Wilcox, James R. and Tatlock, Zachary},
	year = {2015},
	keywords = {Floating point, numerical accuracy, program rewriting},
	pages = {1--11},
	file = {panchekha_et_al_2015_automatically_improving_accuracy_for_floating_point_expressions.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/JNSVK6MQ/panchekha_et_al_2015_automatically_improving_accuracy_for_floating_point_expressions.pdf:application/pdf}
}

@incollection{ohearn_local_2001,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Local {Reasoning} about {Programs} that {Alter} {Data} {Structures}},
	copyright = {©2001 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-42554-0 978-3-540-44802-0},
	url = {http://link.springer.com/chapter/10.1007/3-540-44802-0_1},
	abstract = {We describe an extension of Hoare’s logic for reasoning about programs that alter data structures. We consider a low-level storage model based on a heap with associated lookup, update, allocation and deallocation operations, and unrestricted address arithmetic. The assertion language is based on a possible worlds model of the logic of bunched implications, and includes spatial conjunction and implication connectives alongside those of classical logic. Heap operations are axiomatized using what we call the “small axioms”, each of which mentions only those cells accessed by a particular command. Through these and a number of examples we show that the formalism supports local reasoning: A specification and proof can concentrate on only those cells in memory that a program accesses. This paper builds on earlier work by Burstall, Reynolds, Ishtiaq and O’Hearn on reasoning about data structures.},
	language = {en},
	number = {2142},
	urldate = {2015-06-11},
	booktitle = {Computer {Science} {Logic}},
	publisher = {Springer Berlin Heidelberg},
	author = {O’Hearn, Peter and Reynolds, John and Yang, Hongseok},
	editor = {Fribourg, Laurent},
	year = {2001},
	keywords = {Artificial Intelligence (incl. Robotics), Logics and Meanings of Programs, Mathematical Logic and Formal Languages, Mathematical Logic and Foundations},
	pages = {1--19},
	file = {o’hearn_et_al_2001_local_reasoning_about_programs_that_alter_data_structures.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/84D349BN/o’hearn_et_al_2001_local_reasoning_about_programs_that_alter_data_structures.pdf:application/pdf}
}

@article{swierstra_data_2008,
	title = {Data types à la carte},
	volume = {18},
	issn = {1469-7653},
	url = {http://journals.cambridge.org/article_S0956796808006758},
	doi = {10.1017/S0956796808006758},
	abstract = {This paper describes a technique for assembling both data types and functions from isolated individual components. We also explore how the same technology can be used to combine free monads and, as a result, structure Haskell's monolithic IO monad.},
	number = {04},
	urldate = {2015-06-02},
	journal = {Journal of Functional Programming},
	author = {Swierstra, Wouter},
	year = {2008},
	pages = {423--436},
	file = {swierstra_2008_data_types_à_la_carte.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/5WJ7KS7M/swierstra_2008_data_types_à_la_carte.pdf:application/pdf}
}

@inproceedings{plociniczak_improving_2014,
	title = {Improving {Human}-{Compiler} {Interaction} {Through} {Customizable} {Type} {Feedback}},
	booktitle = {{SPLASH}},
	author = {Plociniczak, Hubert and Miller, Heather and Odersky, Martin},
	year = {2014},
	keywords = {\_tablet},
	file = {plociniczak_et_al_2014_improving_human-compiler_interaction_through_customizable_type_feedback.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/QIVUNM27/plociniczak_et_al_2014_improving_human-compiler_interaction_through_customizable_type_feedback.pdf:application/pdf}
}

@incollection{tillmann_pexwhite_2008,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Pex -- {White} {Box} {Test} {Generation} for .{NET}},
	copyright = {©2008 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-79123-2 978-3-540-79124-9},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-79124-9_10},
	abstract = {Pex automatically produces a small test suite with high code coverage for a .NET program. To this end, Pex performs a systematic program analysis (using dynamic symbolic execution, similar to path-bounded model-checking) to determine test inputs for Parameterized Unit Tests. Pex learns the program behavior by monitoring execution traces. Pex uses a constraint solver to produce new test inputs which exercise different program behavior. The result is an automatically generated small test suite which often achieves high code coverage. In one case study, we applied Pex to a core component of the .NET runtime which had already been extensively tested over several years. Pex found errors, including a serious issue.},
	language = {en},
	number = {4966},
	urldate = {2015-06-26},
	booktitle = {Tests and {Proofs}},
	publisher = {Springer Berlin Heidelberg},
	author = {Tillmann, Nikolai and Halleux, Jonathan de},
	editor = {Beckert, Bernhard and Hähnle, Reiner},
	year = {2008},
	keywords = {Computer Communication Networks, Computers and Society, Logics and Meanings of Programs, Software Engineering, System Performance and Evaluation},
	pages = {134--153},
	file = {tillmann_halleux_2008_pex–white_box_test_generation_for.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/G553QAXB/tillmann_halleux_2008_pex–white_box_test_generation_for.pdf:application/pdf}
}

@inproceedings{avanzini_analysing_2015,
	address = {New York, NY, USA},
	series = {{ICFP} 2015},
	title = {Analysing the {Complexity} of {Functional} {Programs}: {Higher}-order {Meets} {First}-order},
	isbn = {978-1-4503-3669-7},
	shorttitle = {Analysing the {Complexity} of {Functional} {Programs}},
	url = {http://doi.acm.org/10.1145/2784731.2784753},
	doi = {10.1145/2784731.2784753},
	abstract = {We show how the complexity of higher-order functional programs can be analysed automatically by applying program transformations to a defunctionalised versions of them, and feeding the result to existing tools for the complexity analysis of first-order term rewrite systems. This is done while carefully analysing complexity preservation and reflection of the employed transformations such that the complexity of the obtained term rewrite system reflects on the complexity of the initial program. Further, we describe suitable strategies for the application of the studied transformations and provide ample experimental data for assessing the viability of our method.},
	urldate = {2015-09-03},
	booktitle = {Proceedings of the 20th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Avanzini, Martin and Dal Lago, Ugo and Moser, Georg},
	year = {2015},
	keywords = {Defunctionalisation, termination and resource analysis, term rewriting},
	pages = {152--164},
	file = {avanzini_et_al_2015_analysing_the_complexity_of_functional_programs.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/AHC5V27F/avanzini_et_al_2015_analysing_the_complexity_of_functional_programs.pdf:application/pdf}
}

@inproceedings{guo_online_2013,
	address = {New York, NY, USA},
	series = {{SIGCSE} '13},
	title = {Online {Python} {Tutor}: {Embeddable} {Web}-based {Program} {Visualization} for {Cs} {Education}},
	isbn = {978-1-4503-1868-6},
	shorttitle = {Online {Python} {Tutor}},
	url = {http://doi.acm.org/10.1145/2445196.2445368},
	doi = {10.1145/2445196.2445368},
	abstract = {This paper presents Online Python Tutor, a web-based program visualization tool for Python, which is becoming a popular language for teaching introductory CS courses. Using this tool, teachers and students can write Python programs directly in the web browser (without installing any plugins), step forwards and backwards through execution to view the run-time state of data structures, and share their program visualizations on the web. In the past three years, over 200,000 people have used Online Python Tutor to visualize their programs. In addition, instructors in a dozen universities such as UC Berkeley, MIT, the University of Washington, and the University of Waterloo have used it in their CS1 courses. Finally, Online Python Tutor visualizations have been embedded within three web-based digital Python textbook projects, which collectively attract around 16,000 viewers per month and are being used in at least 25 universities. Online Python Tutor is free and open source software, available at pythontutor.com.},
	urldate = {2015-10-29},
	booktitle = {Proceeding of the 44th {ACM} {Technical} {Symposium} on {Computer} {Science} {Education}},
	publisher = {ACM},
	author = {Guo, Philip J.},
	year = {2013},
	keywords = {CS1, program visualization, python},
	pages = {579--584},
	file = {guo_2013_online_python_tutor.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/3Q3CG6I9/guo_2013_online_python_tutor.pdf:application/pdf}
}

@inproceedings{buiras_hlio:_2015,
	address = {New York, NY, USA},
	series = {{ICFP} 2015},
	title = {{HLIO}: {Mixing} {Static} and {Dynamic} {Typing} for {Information}-flow {Control} in {Haskell}},
	isbn = {978-1-4503-3669-7},
	shorttitle = {{HLIO}},
	url = {http://doi.acm.org/10.1145/2784731.2784758},
	doi = {10.1145/2784731.2784758},
	abstract = {Information-Flow Control (IFC) is a well-established approach for allowing untrusted code to manipulate sensitive data without disclosing it. IFC is typically enforced via type systems and static analyses or via dynamic execution monitors. The LIO Haskell library, originating in operating systems research, implements a purely dynamic monitor of the sensitivity level of a computation, particularly suitable when data sensitivity levels are only known at runtime. In this paper, we show how to give programmers the flexibility of deferring IFC checks to runtime (as in LIO), while also providing static guarantees---and the absence of runtime checks---for parts of their programs that can be statically verified (unlike LIO). We present the design and implementation of our approach, HLIO (Hybrid LIO), as an embedding in Haskell that uses a novel technique for deferring IFC checks based on singleton types and constraint polymorphism. We formalize HLIO, prove non-interference, and show how interesting IFC examples can be programmed. Although our motivation is IFC, our technique for deferring constraints goes well beyond and offers a methodology for programmer-controlled hybrid type checking in Haskell.},
	urldate = {2015-09-03},
	booktitle = {Proceedings of the 20th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Buiras, Pablo and Vytiniotis, Dimitrios and Russo, Alejandro},
	year = {2015},
	keywords = {constraint kinds, data kinds, dynamic typing, gradual typing, hybrid typing, Information-flow control, singleton types},
	pages = {289--301},
	file = {buiras_et_al_2015_hlio.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/IAXH5PEV/buiras_et_al_2015_hlio.pdf:application/pdf}
}

@inproceedings{rosner_bounded_2014,
	address = {New York, NY, USA},
	series = {{OOPSLA} '14},
	title = {Bounded {Exhaustive} {Test} {Input} {Generation} from {Hybrid} {Invariants}},
	isbn = {978-1-4503-2585-1},
	url = {http://doi.acm.org/10.1145/2660193.2660232},
	doi = {10.1145/2660193.2660232},
	abstract = {We present a novel technique for producing bounded exhaustive test suites from hybrid invariants, i.e., invariants that are expressed imperatively, declaratively, or as a combination of declarative and imperative predicates. Hybrid specifications are processed using known mechanisms for the imperative and declarative parts, but combined in a way that enables us to exploit information from the declarative side, such as tight bounds computed from the declarative specification, to improve the search both on the imperative and declarative sides. Moreover, our technique automatically evaluates different possible ways of processing the imperative side, and the alternative settings (imperative or declarative) for parts of the invariant available both declaratively and imperatively, to decide the most convenient invariant configuration with respect to efficiency in test generation. This is achieved by transcoping, i.e., by assessing the efficiency of the different alternatives on small scopes (where generation times are negligible), and then extrapolating the results to larger scopes. We also show experiments involving collection classes that support the effectiveness of our technique, by demonstrating that (i) bounded exhaustive suites can be computed from hybrid invariants significantly more efficiently than doing so using state-of-the-art purely imperative and purely declarative approaches, and (ii) our technique is able to automatically determine efficient hybrid invariants, in the sense that they lead to an efficient computation of bounded exhaustive suites, using transcoping.},
	urldate = {2015-04-27},
	booktitle = {Proceedings of the 2014 {ACM} {International} {Conference} on {Object} {Oriented} {Programming} {Systems} {Languages} \&\#38; {Applications}},
	publisher = {ACM},
	author = {Rosner, Nicolás and Bengolea, Valeria and Ponzio, Pablo and Khalek, Shadi Abdul and Aguirre, Nazareno and Frias, Marcelo F. and Khurshid, Sarfraz},
	year = {2014},
	keywords = {alloy, automated test generation, bounded exhaustive testing, korat, sat solving, transcoping},
	pages = {655--674},
	file = {rosner_et_al_2014_bounded_exhaustive_test_input_generation_from_hybrid_invariants.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/HB23EH29/rosner_et_al_2014_bounded_exhaustive_test_input_generation_from_hybrid_invariants.pdf:application/pdf}
}

@incollection{rudiak-gould_haskell_2006,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Haskell {Is} {Not} {Not} {ML}},
	copyright = {©2006 Springer Berlin Heidelberg},
	isbn = {978-3-540-33095-0 978-3-540-33096-7},
	url = {http://link.springer.com/chapter/10.1007/11693024_4},
	abstract = {We present a typed calculus IL (“intermediate language”) which supports the embedding of ML-like (strict, eager) and Haskell-like (non-strict, lazy) languages, without favoring either. IL’s type system includes negation (continuations), but not implication (function arrow). Within IL we find that lifted sums and products can be represented as the double negation of their unlifted counterparts. We exhibit a compilation function from IL to AM—an abstract von Neumann machine—which maps values of ordinary and doubly negated types to heap structures resembling those found in practical implementations of languages in the ML and Haskell families. Finally, we show that a small variation in the design of AM allows us to treat any ML value as a Haskell value at runtime without cost, and project a Haskell value onto an ML type with only the cost of a Haskell deepSeq. This suggests that IL and AM may be useful as a compilation and execution model for a new language which combines the best features of strict and non-strict functional programming.},
	language = {en},
	number = {3924},
	urldate = {2015-08-21},
	booktitle = {Programming {Languages} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Rudiak-Gould, Ben and Mycroft, Alan and Jones, Simon Peyton},
	editor = {Sestoft, Peter},
	month = mar,
	year = {2006},
	keywords = {data structures, Logics and Meanings of Programs, Mathematical Logic and Formal Languages, Programming Languages, Compilers, Interpreters, Programming Techniques, Software Engineering},
	pages = {38--53},
	file = {rudiak-gould_et_al_2006_haskell_is_not_not_ml.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/TI23HWPJ/rudiak-gould_et_al_2006_haskell_is_not_not_ml.pdf:application/pdf;Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/5EU7BJNX/10.html:text/html}
}

@inproceedings{allen_drjava:_2002,
	address = {New York, NY, USA},
	series = {{SIGCSE} '02},
	title = {{DrJava}: {A} {Lightweight} {Pedagogic} {Environment} for {Java}},
	isbn = {978-1-58113-473-5},
	shorttitle = {{DrJava}},
	url = {http://doi.acm.org/10.1145/563340.563395},
	doi = {10.1145/563340.563395},
	abstract = {DrJava is a pedagogic programming environment for Java that enables students to focus on designing programs, rather than learning how to use the environment. The environment provides a simple interface based on a "read-eval-print loop" that enables a programmer to develop, test, and debug Java programs in an interactive, incremental fashion. This paper gives an overview of DrJava including its pedagogic rationale, functionality, and implementation.},
	urldate = {2015-10-29},
	booktitle = {Proceedings of the 33rd {SIGCSE} {Technical} {Symposium} on {Computer} {Science} {Education}},
	publisher = {ACM},
	author = {Allen, Eric and Cartwright, Robert and Stoler, Brian},
	year = {2002},
	pages = {137--141},
	file = {allen_et_al_2002_drjava.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/UXCFG4AQ/allen_et_al_2002_drjava.pdf:application/pdf}
}

@inproceedings{godefroid_grammar-based_2008,
	address = {New York, NY, USA},
	series = {{PLDI} '08},
	title = {Grammar-based {Whitebox} {Fuzzing}},
	isbn = {978-1-59593-860-2},
	url = {http://doi.acm.org/10.1145/1375581.1375607},
	doi = {10.1145/1375581.1375607},
	abstract = {Whitebox fuzzing is a form of automatic dynamic test generation, based on symbolic execution and constraint solving, designed for security testing of large applications. Unfortunately, the current effectiveness of whitebox fuzzing is limited when testing applications with highly-structured inputs, such as compilers and interpreters. These applications process their inputs in stages, such as lexing, parsing and evaluation. Due to the enormous number of control paths in early processing stages, whitebox fuzzing rarely reaches parts of the application beyond those first stages. In this paper, we study how to enhance whitebox fuzzing of complex structured-input applications with a grammar-based specification of their valid inputs. We present a novel dynamic test generation algorithm where symbolic execution directly generates grammar-based constraints whose satisfiability is checked using a custom grammar-based constraint solver. We have implemented this algorithm and evaluated it on a large security-critical application, the JavaScript interpreter of Internet Explorer 7 (IE7). Results of our experiments show that grammar-based whitebox fuzzing explores deeper program paths and avoids dead-ends due to non-parsable inputs. Compared to regular whitebox fuzzing, grammar-based whitebox fuzzing increased coverage of the code generation module of the IE7 JavaScript interpreter from 53\% to 81\% while using three times fewer tests.},
	urldate = {2015-03-15},
	booktitle = {Proceedings of the 2008 {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Godefroid, Patrice and Kiezun, Adam and Levin, Michael Y.},
	year = {2008},
	keywords = {automatic test generation, grammars, program verification, software testing},
	pages = {206--215},
	file = {godefroid_et_al_2008_grammar-based_whitebox_fuzzing.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/EI4IE4W3/godefroid_et_al_2008_grammar-based_whitebox_fuzzing.pdf:application/pdf}
}

@inproceedings{rajala_ville:_2007,
	address = {Darlinghurst, Australia, Australia},
	series = {Koli {Calling} '07},
	title = {{VILLE}: {A} {Language}-independent {Program} {Visualization} {Tool}},
	isbn = {978-1-920682-69-9},
	shorttitle = {{VILLE}},
	url = {http://dl.acm.org/citation.cfm?id=2449323.2449340},
	abstract = {Visualization tools have proven to be useful for enhancing novice programmers' learning. However, existing tools are typically tied to particular programming languages, and tend to focus on low-level aspects of programming such as the changing values of variables during program code execution. In this paper we present a new program visualization tool, which provides a language-independent view of learning programming. Moreover, program execution can be viewed in two languages simultaneously. Complete with role information of variables, the tool supports the learning process at a more abstract level, thus emphasizing the similarities of basic programming concepts and syntax in all imperative programming languages.},
	urldate = {2015-10-29},
	booktitle = {Proceedings of the {Seventh} {Baltic} {Sea} {Conference} on {Computing} {Education} {Research} - {Volume} 88},
	publisher = {Australian Computer Society, Inc.},
	author = {Rajala, Teemu and Laakso, Mikko-Jussi and Kaila, Erkki and Salakoski, Tapio},
	year = {2007},
	keywords = {language independency, novice programming, program visualization, teaching programming},
	pages = {151--159},
	file = {rajala_et_al_2007_ville.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/6PAGI5V5/rajala_et_al_2007_ville.pdf:application/pdf}
}

@inproceedings{boyapati_korat:_2002,
	address = {New York, NY, USA},
	series = {{ISSTA} '02},
	title = {Korat: {Automated} {Testing} {Based} on {Java} {Predicates}},
	isbn = {1-58113-562-9},
	shorttitle = {Korat},
	url = {http://doi.acm.org/10.1145/566172.566191},
	doi = {10.1145/566172.566191},
	abstract = {This paper presents Korat, a novel framework for automated testing of Java programs. Given a formal specification for a method, Korat uses the method precondition to automatically generate all (nonisomorphic) test cases up to a given small size. Korat then executes the method on each test case, and uses the method postcondition as a test oracle to check the correctness of each output.To generate test cases for a method, Korat constructs a Java predicate (i.e., a method that returns a boolean) from the method's pre-condition. The heart of Korat is a technique for automatic test case generation: given a predicate and a bound on the size of its inputs, Korat generates all (nonisomorphic) inputs for which the predicate returns true. Korat exhaustively explores the bounded input space of the predicate but does so efficiently by monitoring the predicate's executions and pruning large portions of the search space.This paper illustrates the use of Korat for testing several data structures, including some from the Java Collections Framework. The experimental results show that it is feasible to generate test cases from Java predicates, even when the search space for inputs is very large. This paper also compares Korat with a testing framework based on declarative specifications. Contrary to our initial expectation, the experiments show that Korat generates test cases much faster than the declarative framework.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 2002 {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Boyapati, Chandrasekhar and Khurshid, Sarfraz and Marinov, Darko},
	year = {2002},
	pages = {123--133},
	file = {boyapati_et_al_2002_korat.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/JR5RA9FS/boyapati_et_al_2002_korat.pdf:application/pdf}
}

@phdthesis{laycock_theory_1993,
	title = {The {Theory} and {Practice} of {Specification} {Based} {Software} {Testing}},
	abstract = {In this thesis my aim is to examine the common ground between formal methods and testing, and the benefits the two fields bring to one another. All too often they are regarded as mutually exclusive approaches in the development of software systems. The thesis begins with an examination of the motivation behind software testing, a summary of its development over the past few decades, and a survey of existing techniques. This involves a detailed discussion of some of those techniques, and leads on to an extensive case study. The case study shows how the use of a formal specification enables an existing "partition" based testing method to be used with far greater precision, but also highlights some of the limitations of the partition based techniques. The thesis continues with a comprehensive look at the development of theoretical models of testing since the mid 1970's, and the way they have used successively more complex software models in order to be able to adequately describe suitable...},
	school = {University of Sheffield},
	author = {Laycock, Gilbert Thomas},
	year = {1993},
	file = {laycock_1993_the_theory_and_practice_of_specification_based_software_testing.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/PMUMC6UV/laycock_1993_the_theory_and_practice_of_specification_based_software_testing.pdf:application/pdf}
}

@inproceedings{pike_experience_2012,
	address = {New York, NY, USA},
	series = {{ICFP} '12},
	title = {Experience {Report}: {A} {Do}-it-yourself {High}-assurance {Compiler}},
	isbn = {978-1-4503-1054-3},
	shorttitle = {Experience {Report}},
	url = {http://doi.acm.org/10.1145/2364527.2364553},
	doi = {10.1145/2364527.2364553},
	abstract = {Embedded domain-specific languages (EDSLs) are an approach for quickly building new languages while maintaining the advantages of a rich metalanguage. We argue in this experience report that the "EDSL approach" can surprisingly ease the task of building a high-assurance compiler. We do not strive to build a fully formally-verified tool-chain, but take a "do-it-yourself" approach to increase our confidence in compiler-correctness without too much effort. Copilot is an EDSL developed by Galois, Inc. and the National Institute of Aerospace under contract to NASA for the purpose of runtime monitoring of flight-critical avionics. We report our experience in using type-checking, QuickCheck, and model-checking "off-the-shelf" to quickly increase confidence in our EDSL tool-chain.},
	urldate = {2015-04-29},
	booktitle = {Proceedings of the 17th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Pike, Lee and Wegmann, Nis and Niller, Sebastian and Goodloe, Alwyn},
	year = {2012},
	keywords = {compiler, embedded domain-specific language, verification},
	pages = {335--340},
	file = {pike_et_al_2012_experience_report.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/J2G5WMQ7/pike_et_al_2012_experience_report.pdf:application/pdf}
}

@article{bird_automatic_1983,
	title = {Automatic generation of random self-checking test cases},
	volume = {22},
	issn = {0018-8670},
	doi = {10.1147/sj.223.0229},
	abstract = {A technique of automatically generating random software test cases is described. The nature of such test cases ensures that they will execute to completion, and their execution is predicted at the time of generation. Wherever possible the test cases are self-checking. At run-time their execution is compared with the predicted execution. Also described are implementations of the technique that have been used to test various IBM programs—/I language processors, sort/merge programs, and Graphical Data Display Manager alphanumeric and graphics support.},
	number = {3},
	journal = {IBM Systems Journal},
	author = {Bird, D. L. and Munoz, C.U.},
	year = {1983},
	keywords = {\_tablet, research-exam},
	pages = {229--245},
	file = {bird_munoz_1983_automatic_generation_of_random_self-checking_test_cases.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/NMNNRU2C/bird_munoz_1983_automatic_generation_of_random_self-checking_test_cases.pdf:application/pdf}
}

@inproceedings{de_moura_z3:_2008,
	address = {Berlin, Heidelberg},
	series = {{TACAS}'08/{ETAPS}'08},
	title = {Z3: {An} {Efficient} {SMT} {Solver}},
	isbn = {3-540-78799-2 978-3-540-78799-0},
	shorttitle = {Z3},
	url = {http://dl.acm.org/citation.cfm?id=1792734.1792766},
	urldate = {2015-01-22},
	booktitle = {Proceedings of the {Theory} and {Practice} of {Software}, 14th {International} {Conference} on {Tools} and {Algorithms} for the {Construction} and {Analysis} of {Systems}},
	publisher = {Springer-Verlag},
	author = {De Moura, Leonardo and Bjørner, Nikolaj},
	year = {2008},
	pages = {337--340},
	file = {de_moura_bjørner_2008_z3.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/I4G5W9CF/de_moura_bjørner_2008_z3.pdf:application/pdf}
}

@article{jackson_alloy:_2002,
	title = {Alloy: {A} {Lightweight} {Object} {Modelling} {Notation}},
	volume = {11},
	issn = {1049-331X},
	shorttitle = {Alloy},
	url = {http://doi.acm.org/10.1145/505145.505149},
	doi = {10.1145/505145.505149},
	abstract = {Alloy is a little language for describing structural properties. It offers a declaration syntax compatible with graphical object models, and a set-based formula syntax powerful enough to express complex constraints and yet amenable to a fully automatic semantic analysis. Its meaning is given by translation to an even smaller (formally defined) kernel. This paper presents the language in its entirety, and explains its motivation, contributions and deficiencies.},
	number = {2},
	urldate = {2015-06-26},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Jackson, Daniel},
	month = apr,
	year = {2002},
	keywords = {first-order logic, object models, Z specification language},
	pages = {256--290},
	file = {jackson_2002_alloy.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/34RWS833/jackson_2002_alloy.pdf:application/pdf}
}

@inproceedings{ploeg_reflection_2014,
	address = {New York, NY, USA},
	series = {Haskell '14},
	title = {Reflection {Without} {Remorse}: {Revealing} a {Hidden} {Sequence} to {Speed} {Up} {Monadic} {Reflection}},
	isbn = {978-1-4503-3041-1},
	shorttitle = {Reflection {Without} {Remorse}},
	url = {http://doi.acm.org/10.1145/2633357.2633360},
	doi = {10.1145/2633357.2633360},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 2014 {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Ploeg, Atze van der and Kiselyov, Oleg},
	year = {2014},
	keywords = {data structures, monads, performance, reflection},
	pages = {133--144},
	file = {ploeg_kiselyov_2014_reflection_without_remorse.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/GJQ8WVIZ/ploeg_kiselyov_2014_reflection_without_remorse.pdf:application/pdf}
}

@inproceedings{gundry_typechecker_2015,
	address = {New York, NY, USA},
	series = {Haskell 2015},
	title = {A {Typechecker} {Plugin} for {Units} of {Measure}: {Domain}-specific {Constraint} {Solving} in {GHC} {Haskell}},
	isbn = {978-1-4503-3808-0},
	shorttitle = {A {Typechecker} {Plugin} for {Units} of {Measure}},
	url = {http://doi.acm.org/10.1145/2804302.2804305},
	doi = {10.1145/2804302.2804305},
	abstract = {Typed functional programming and units of measure are a natural combination, as F\# ably demonstrates. However, encoding statically-checked units in Haskell’s type system leads to inevitable disappointment with the usability of the resulting system. Extending the language itself would produce a much better result, but it would be a lot of work! In this paper, I demonstrate how typechecker plugins in the Glasgow Haskell Compiler allow users to define domain-specific constraint solving behaviour, making it possible to implement units of measure as a type system extension without rebuilding the compiler. This paves the way for a more modular treatment of constraint solving in GHC.},
	urldate = {2015-09-03},
	booktitle = {Proceedings of the 8th {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Gundry, Adam},
	year = {2015},
	keywords = {Dimensions, modular typechecking, type inference},
	pages = {11--22},
	file = {gundry_2015_a_typechecker_plugin_for_units_of_measure.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/4VPPU3UW/gundry_2015_a_typechecker_plugin_for_units_of_measure.pdf:application/pdf}
}

@inproceedings{khoo_designing_2012,
	title = {Designing a virtual environment to evaluate multimodal sensors for assisting the visually impaired},
	volume = {2},
	url = {http://portal.acm.org/citation.cfm?id=2363956.2364049&coll=DL&dl=ACM&CFID=326685788&CFTOKEN=70440880},
	abstract = {We describe how to design a virtual environment using Microsoft Robotics Developer Studio in order to evaluate multimodal sensors for assisting visually impaired people in daily tasks such as navigation and orientation. The work focuses on the design},
	booktitle = {{ICCHP}},
	publisher = {Springer},
	author = {Khoo, Wai L and Seidel, Eric L and Zhu, Zhigang},
	year = {2012},
	file = {khoo_et_al_2012_designing_a_virtual_environment_to_evaluate_multimodal_sensors_for_assisting.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/X6ZZ9E5Z/khoo_et_al_2012_designing_a_virtual_environment_to_evaluate_multimodal_sensors_for_assisting.pdf:application/pdf}
}

@inproceedings{banados_schwerter_theory_2014,
	address = {New York, NY, USA},
	series = {{ICFP} '14},
	title = {A {Theory} of {Gradual} {Effect} {Systems}},
	isbn = {978-1-4503-2873-9},
	url = {http://doi.acm.org/10.1145/2628136.2628149},
	doi = {10.1145/2628136.2628149},
	abstract = {Effect systems have the potential to help software developers, but their practical adoption has been very limited. We conjecture that this limited adoption is due in part to the difficulty of transitioning from a system where effects are implicit and unrestricted to a system with a static effect discipline, which must settle for conservative checking in order to be decidable. To address this hindrance, we develop a theory of gradual effect checking, which makes it possible to incrementally annotate and statically check effects, while still rejecting statically inconsistent programs. We extend the generic type-and-effect framework of Marino and Millstein with a notion of unknown effects, which turns out to be significantly more subtle than unknown types in traditional gradual typing. We appeal to abstract interpretation to develop and validate the concepts of gradual effect checking. We also demonstrate how an effect system formulated in Marino and Millstein's framework can be automatically extended to support gradual checking.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 19th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Bañados Schwerter, Felipe and Garcia, Ronald and Tanter, Éric},
	year = {2014},
	keywords = {abstract interpretation, gradual typing, type-and-effect systems},
	pages = {283--295},
	file = {bañados_schwerter_et_al_2014_a_theory_of_gradual_effect_systems.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/6XP2ZXQA/bañados_schwerter_et_al_2014_a_theory_of_gradual_effect_systems.pdf:application/pdf}
}

@incollection{smaragdakis_combining_2007,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Combining {Static} and {Dynamic} {Reasoning} for {Bug} {Detection}},
	copyright = {©2007 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-73769-8 978-3-540-73770-4},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-73770-4_1},
	abstract = {Many static and dynamic analyses have been developed to improve program quality. Several of them are well known and widely used in practice. It is not entirely clear, however, how to put these analyses together to achieve their combined benefits. This paper reports on our experiences with building a sequence of increasingly more powerful combinations of static and dynamic analyses for bug finding in the tools JCrasher, Check ’n’ Crash, and DSD-Crasher. We contrast the power and accuracy of the tools using the same example program as input to all three. At the same time, the paper discusses the philosophy behind all three tools. Specifically, we argue that trying to detect program errors (rather than to certify programs for correctness) is well integrated in the development process and a promising approach for both static and dynamic analyses. The emphasis on finding program errors influences many aspects of analysis tools, including the criteria used to evaluate them and the vocabulary of discourse.},
	language = {en},
	number = {4454},
	urldate = {2015-01-24},
	booktitle = {Tests and {Proofs}},
	publisher = {Springer Berlin Heidelberg},
	author = {Smaragdakis, Yannis and Csallner, Christoph},
	editor = {Gurevich, Yuri and Meyer, Bertrand},
	year = {2007},
	keywords = {Computer Communication Networks, Computers and Society, Logics and Meanings of Programs, Software Engineering, System Performance and Evaluation},
	pages = {1--16},
	file = {smaragdakis_csallner_2007_combining_static_and_dynamic_reasoning_for_bug_detection.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/7IB5RWM8/smaragdakis_csallner_2007_combining_static_and_dynamic_reasoning_for_bug_detection.pdf:application/pdf}
}

@phdthesis{perera_interactive_2013,
	type = {d\_ph},
	title = {Interactive functional programming},
	url = {http://etheses.bham.ac.uk/4209/},
	abstract = {We propose a new kind of execution environment where applications can be debugged and re-programmed while they are being used. We call our overall concept interactive programming. We develop some of the key components of interactive programming in the setting of a pure, call-by-value functional language. We illustrate our ideas via a proof-of-concept implementation called lambdaCalc, but leave several important components of the overall vision, including efficient incremental update and scaling to large programs, for future work.

Our specific achievements are as follows. First, we show how to reify the execution of a program into a live document which can be interactively decomposed into both sequential steps and parallel slices. We give a novel characterisation of forward and backward dynamic slicing and show that for a fixed computation the two problems describe a Galois connection.

Second, we introduce a novel execution indexing scheme which derives execution differences from program differences. Our scheme supports the wholesale reorganisation of a computation via operations such as moves and splices. The programmer is able to see the consequences of edits on the intensional structure of the execution. Where possible, node identity is preserved, allowing an edit to be made whilst an execution is being explored and the changes to be reflected in the user's current view of the execution.},
	language = {English},
	urldate = {2015-09-25},
	school = {University of Birmingham},
	author = {Perera, Roland},
	month = jul,
	year = {2013},
	file = {perera_2013_interactive_functional_programming.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/VQS5G4DE/perera_2013_interactive_functional_programming.pdf:application/pdf;Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/J5E4A9VU/4209.html:text/html}
}

@inproceedings{sjoberg_programming_2015,
	address = {New York, NY, USA},
	series = {{POPL} '15},
	title = {Programming {Up} to {Congruence}},
	isbn = {978-1-4503-3300-9},
	url = {http://doi.acm.org/10.1145/2676726.2676974},
	doi = {10.1145/2676726.2676974},
	abstract = {This paper presents the design of Zombie, a dependently-typed programming language that uses an adaptation of a congruence closure algorithm for proof and type inference. This algorithm allows the type checker to automatically use equality assumptions from the context when reasoning about equality. Most dependently-typed languages automatically use equalities that follow from beta-reduction during type checking; however, such reasoning is incompatible with congruence closure. In contrast, Zombie does not use automatic beta-reduction because types may contain potentially diverging terms. Therefore Zombie provides a unique opportunity to explore an alternative definition of equivalence in dependently-typed language design. Our work includes the specification of the language via a bidirectional type system, which works "up-to-congruence,'' and an algorithm for elaborating expressions in this language to an explicitly typed core language. We prove that our elaboration algorithm is complete with respect to the source type system, and always produces well typed terms in the core language. This algorithm has been implemented in the Zombie language, which includes general recursion, irrelevant arguments, heterogeneous equality and datatypes.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 42Nd {Annual} {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Sjöberg, Vilhelm and Weirich, Stephanie},
	year = {2015},
	keywords = {congruence closure, dependent types},
	pages = {369--382},
	file = {sjöberg_weirich_2015_programming_up_to_congruence.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/6SHSFSAV/sjöberg_weirich_2015_programming_up_to_congruence.pdf:application/pdf}
}

@inproceedings{runciman_smallcheck_2008,
	address = {New York, NY, USA},
	series = {Haskell '08},
	title = {Smallcheck and {Lazy} {Smallcheck}: {Automatic} {Exhaustive} {Testing} for {Small} {Values}},
	isbn = {978-1-60558-064-7},
	shorttitle = {Smallcheck and {Lazy} {Smallcheck}},
	url = {http://doi.acm.org/10.1145/1411286.1411292},
	doi = {10.1145/1411286.1411292},
	abstract = {This paper describes two Haskell libraries for property-based testing. Following the lead of QuickCheck, these testing libraries SmallCheck and Lazy SmallCheck also use type-based generators to obtain test-sets of finite values for which properties are checked, and report any counter-examples found. But instead of using a sample of randomly generated values they test properties for all values up to some limiting depth, progressively increasing this limit. The paper explains the design and implementation of both libraries and evaluates them in comparison with each other and with QuickCheck.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the {First} {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Runciman, Colin and Naylor, Matthew and Lindblad, Fredrik},
	year = {2008},
	keywords = {embedded language, exhaustive search, lazy evaluation, property-based testing, type classes},
	pages = {37--48},
	file = {runciman_et_al_2008_smallcheck_and_lazy_smallcheck.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/W58WX5BF/runciman_et_al_2008_smallcheck_and_lazy_smallcheck.pdf:application/pdf}
}

@inproceedings{beyer_generating_2004,
	address = {Washington, DC, USA},
	series = {{ICSE} '04},
	title = {Generating {Tests} from {Counterexamples}},
	isbn = {0-7695-2163-0},
	url = {http://dl.acm.org/citation.cfm?id=998675.999437},
	abstract = {We have extended the software model checker BLAST toautomatically generate test suites that guarantee full coveragewith respect to a given predicate. More precisely, givena C program and a target predicate p, BLAST determinesthe set L of program locations which program execution canreach with p true, and automatically generates a set of testvectors that exhibit the truth of p at all locations in L. Wehave used BLAST to generate test suites and to detect deadcode in C programs with up to 30 K lines of code. The analysisand test-vector generation is fully automatic (no userintervention) and exact (no false positives).},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 26th {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Beyer, Dirk and Chlipala, Adam J. and Henzinger, Thomas A. and Jhala, Ranjit and Majumdar, Rupak},
	year = {2004},
	pages = {326--335},
	file = {beyer_et_al_2004_generating_tests_from_counterexamples.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/XRH769EI/beyer_et_al_2004_generating_tests_from_counterexamples.pdf:application/pdf}
}

@book{wright_practical_1994,
	title = {Practical {Soft} {Typing}},
	abstract = {Soft typing is an approach to type checking for dynamically typed languages. Like a static type checker, a soft type checker infers syntactic types for identifiers and expressions. But rather than reject programs containing untypable fragments, a soft type checker inserts explicit run-time checks to ensure safe execution. Soft typing was first introduced in an idealized form by Cartwright and Fagan. This thesis investigates the issues involved in designing a practical soft type system. A soft type system for a purely functional, call-by-value language is developed by extending the Hindley-Milner polymorphic type system with recursive types and limited forms of union types. The extension adapts Remy's encoding of record types with subtyping to union types. The encoding yields more compact types and permits more efficient type inference than Cartwright and Fagan's early technique. Correctness proofs are developed by employing a new syntactic app...},
	author = {Wright, Andrew K.},
	year = {1994},
	file = {wright_1994_practical_soft_typing.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/I4UQGXAQ/wright_1994_practical_soft_typing.pdf:application/pdf}
}

@inproceedings{roundy_darcs:_2005,
	address = {New York, NY, USA},
	series = {Haskell '05},
	title = {Darcs: {Distributed} {Version} {Management} in {Haskell}},
	isbn = {1-59593-071-X},
	shorttitle = {Darcs},
	url = {http://doi.acm.org/10.1145/1088348.1088349},
	doi = {10.1145/1088348.1088349},
	abstract = {A common reaction from people who hear about darcs, the source control system I created, is that it sounds like a great tool, but it is a shame that it is written in Haskell. People think that because darcs is written in Haskell it will be a slow memory hog with very few contributors to the project. I will give a somewhat historical overview of my experiences with the Haskell language, libraries and tools.I will begin with a brief overview of the darcs advanced revision control system, how it works and how it differs from other version control systems. Then I will go through various problems and successes I have had in using the Haskell language and libraries in darcs, roughly in the order I encountered them. In the process I will give a bit of a tour through the darcs source code. In each case, I will tell about the problem I wanted to solve, what I tried, how it worked, and how it might have worked better (if that is possible).},
	urldate = {2015-02-05},
	booktitle = {Proceedings of the 2005 {ACM} {SIGPLAN} {Workshop} on {Haskell}},
	publisher = {ACM},
	author = {Roundy, David},
	year = {2005},
	pages = {1--4},
	file = {roundy_2005_darcs.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/VUDB64WF/roundy_2005_darcs.pdf:application/pdf}
}

@inproceedings{orchard_embedding_2014,
	address = {New York, NY, USA},
	series = {Haskell '14},
	title = {Embedding {Effect} {Systems} in {Haskell}},
	isbn = {978-1-4503-3041-1},
	url = {http://doi.acm.org/10.1145/2633357.2633368},
	doi = {10.1145/2633357.2633368},
	abstract = {Monads are now an everyday tool in functional programming for abstracting and delimiting effects. The link between monads and effect systems is well-known, but in their typical use, monads provide a much more coarse-grained view of effects. Effect systems capture fine-grained information about the effects, but monads provide only a binary view: effectful or pure. Recent theoretical work has unified fine-grained effect systems with monads using a monad-like structure indexed by a monoid of effect annotations (called parametric effect monads). This aligns the power of monads with the power of effect systems. This paper leverages recent advances in Haskell's type system (as provided by GHC) to embed this approach in Haskell, providing user-programmable effect systems. We explore a number of practical examples that make Haskell even better and safer for effectful programming. Along the way, we relate the examples to other concepts, such as Haskell's implicit parameters and coeffects.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 2014 {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Orchard, Dominic and Petricek, Tomas},
	year = {2014},
	keywords = {effect systems, parametric effect monads, type systems},
	pages = {13--24},
	file = {orchard_petricek_2014_embedding_effect_systems_in_haskell.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/XBQNPTHQ/orchard_petricek_2014_embedding_effect_systems_in_haskell.pdf:application/pdf}
}

@article{clarke_bounded_2001,
	title = {Bounded {Model} {Checking} {Using} {Satisfiability} {Solving}},
	volume = {19},
	issn = {0925-9856},
	url = {http://dx.doi.org/10.1023/A:1011276507260},
	doi = {10.1023/A:1011276507260},
	abstract = {The phrase model checking refers to algorithms for exploring the state space of a transition system to determine if it obeys a specification of its intended behavior. These algorithms can perform exhaustive verification in a highly automatic manner, and, thus, have attracted much interest in industry. Model checking programs are now being commercially marketed. However, model checking has been held back by the state explosion problem, which is the problem that the number of states in a system grows exponentially in the number of system components. Much research has been devoted to ameliorating this problem.In this tutorial, we first give a brief overview of the history of model checking to date, and then focus on recent techniques that combine model checking with satisfiability solving. These techniques, known as bounded model checking, do a very fast exploration of the state space, and for some types of problems seem to offer large performance improvements over previous approaches. We review experiments with bounded model checking on both public domain and industrial designs, and propose a methodology for applying the technique in industry for invariance checking. We then summarize the pros and cons of this new technology and discuss future research efforts to extend its capabilities.},
	number = {1},
	urldate = {2014-09-19},
	journal = {Form. Methods Syst. Des.},
	author = {Clarke, Edmund and Biere, Armin and Raimi, Richard and Zhu, Yunshan},
	month = jul,
	year = {2001},
	keywords = {bounded model checking, cone of influence reduction, model checking, processor verification, satisfiability},
	pages = {7--34},
	file = {clarke_et_al_2001_bounded_model_checking_using_satisfiability_solving.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/8GBWA62Q/clarke_et_al_2001_bounded_model_checking_using_satisfiability_solving.pdf:application/pdf}
}

@inproceedings{danner_denotational_2015,
	address = {New York, NY, USA},
	series = {{ICFP} 2015},
	title = {Denotational {Cost} {Semantics} for {Functional} {Languages} with {Inductive} {Types}},
	isbn = {978-1-4503-3669-7},
	url = {http://doi.acm.org/10.1145/2784731.2784749},
	doi = {10.1145/2784731.2784749},
	abstract = {A central method for analyzing the asymptotic complexity of a functional program is to extract and then solve a recurrence that expresses evaluation cost in terms of input size. The relevant notion of input size is often specific to a datatype, with measures including the length of a list, the maximum element in a list, and the height of a tree. In this work, we give a formal account of the extraction of cost and size recurrences from higher-order functional programs over inductive datatypes. Our approach allows a wide range of programmer-specified notions of size, and ensures that the extracted recurrences correctly predict evaluation cost. To extract a recurrence from a program, we first make costs explicit by applying a monadic translation from the source language to a complexity language, and then abstract datatype values as sizes. Size abstraction can be done semantically, working in models of the complexity language, or syntactically, by adding rules to a preorder judgement. We give several different models of the complexity language, which support different notions of size. Additionally, we prove by a logical relations argument that recurrences extracted by this process are upper bounds for evaluation cost; the proof is entirely syntactic and therefore applies to all of the models we consider.},
	urldate = {2015-09-03},
	booktitle = {Proceedings of the 20th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Danner, Norman and Licata, Daniel R. and Ramyaa, Ramyaa},
	year = {2015},
	keywords = {analysis, complexity, Semi-automatic},
	pages = {140--151},
	file = {danner_et_al_2015_denotational_cost_semantics_for_functional_languages_with_inductive_types.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/484XEDJN/danner_et_al_2015_denotational_cost_semantics_for_functional_languages_with_inductive_types.pdf:application/pdf}
}

@article{sheard_languages_2004,
	title = {Languages of the {Future}},
	volume = {39},
	issn = {0362-1340},
	url = {http://doi.acm.org/10.1145/1052883.1052897},
	doi = {10.1145/1052883.1052897},
	abstract = {This paper explores a new point in the design space of formal reasoning systems - part programming language, part logical framework. The system is built on a programming language where the user expresses equality constraints between types and the type checker then enforces these constraints. This simple extension to the type system allows the programmer to describe properties of his program in the types of witness objects which can be thought of as concrete evidence that the program has the property desired. These techniques and two other rich typing mechanisms, rank-N polymorphism and extensible kinds, create a powerful new programming idiom for writing programs whose types enforce semantic properties.A language with these features is both a practical programming language and a logic. This marriage between two previously separate entities increases the probability that users will apply formal methods to their programming designs. This kind of synthesis creates the foundations for the languages of the future.},
	number = {12},
	urldate = {2015-11-13},
	journal = {SIGPLAN Not.},
	author = {Sheard, Tim},
	month = dec,
	year = {2004},
	pages = {119--132},
	file = {sheard_2004_languages_of_the_future.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/9SFNAXCX/sheard_2004_languages_of_the_future.pdf:application/pdf}
}

@incollection{caballero_theoretical_2001,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Theoretical {Foundations} for the {Declarative} {Debugging} of {Lazy} {Functional} {Logic} {Programs}},
	copyright = {©2001 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-41739-2 978-3-540-44716-0},
	url = {http://link.springer.com/chapter/10.1007/3-540-44716-4_11},
	abstract = {The aim of this paper is to provide theoretical foundations for the declarative debugging of wrong answers in lazy functional logic programming. We rely on a logical framework which formalizes both the intended meaning and the execution model of programs in a simple language which combines the expressivity of pure Prolog and a significant subset of Haskell. As novelties w.r.t. to previous related approaches, we deal with functional values both as arguments and as results of higher order functions, we obtain a completely formal specification of the debugging method, and we extend known soundness and completeness results for the debugging of wrong answers in logic programming to a substantially more difficult context. A prototype implementation of a working debugger is planned as future work.},
	language = {en},
	number = {2024},
	urldate = {2015-01-08},
	booktitle = {Functional and {Logic} {Programming}},
	publisher = {Springer Berlin Heidelberg},
	author = {Caballero, Rafael and López-Fraguas, Francisco J. and Rodrìguez-Artalejo, Mario},
	editor = {Kuchen, Herbert and Ueda, Kazunori},
	month = jan,
	year = {2001},
	keywords = {Artificial Intelligence (incl. Robotics), Logics and Meanings of Programs, Programming Languages, Compilers, Interpreters, Programming Techniques},
	pages = {170--184},
	file = {caballero_et_al_2001_theoretical_foundations_for_the_declarative_debugging_of_lazy_functional_logic.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/WMF8H9U8/caballero_et_al_2001_theoretical_foundations_for_the_declarative_debugging_of_lazy_functional_logic.pdf:application/pdf}
}

@inproceedings{christiansen_type-directed_2014,
	address = {New York, NY, USA},
	series = {{IFL} '14},
	title = {Type-{Directed} {Elaboration} of {Quasiquotations}: {A} {High}-{Level} {Syntax} for {Low}-{Level} {Reflection}},
	isbn = {978-1-4503-3284-2},
	shorttitle = {Type-{Directed} {Elaboration} of {Quasiquotations}},
	url = {http://doi.acm.org/10.1145/2746325.2746326},
	doi = {10.1145/2746325.2746326},
	abstract = {Idris's reflection features allow Idris metaprograms to manipulate a representation of Idris's core language as a datatype, but these reflected terms were designed for ease of type checking and are therefore exceedingly verbose and tedious to work with. A simpler notation would make these programs both easier to read and easier to write. We describe a variation of quasiquotation that uses the language's compiler to translate high-level programs with holes into their corresponding reflected representation, both in pattern-matching and expression contexts. This provides a notation for reflected language that matches the notation used to write programs, allowing readable metaprograms.},
	urldate = {2015-09-02},
	booktitle = {Proceedings of the 26Nd 2014 {International} {Symposium} on {Implementation} and {Application} of {Functional} {Languages}},
	publisher = {ACM},
	author = {Christiansen, David Raymond},
	year = {2014},
	keywords = {metaprogramming, proof automation, quasiquotation},
	pages = {1:1--1:9},
	file = {christiansen_2014_type-directed_elaboration_of_quasiquotations.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/EKEHFMJP/christiansen_2014_type-directed_elaboration_of_quasiquotations.pdf:application/pdf}
}

@inproceedings{jeffrey_bugfix:_2009,
	title = {{BugFix}: {A} learning-based tool to assist developers in fixing bugs},
	shorttitle = {{BugFix}},
	doi = {10.1109/ICPC.2009.5090029},
	abstract = {We present a tool called BugFix that can assist developers in fixing program bugs. Our tool automatically analyzes the debugging situation at a statement and reports a prioritized list of relevant bug-fix suggestions that are likely to guide the developer to an appropriate fix at that statement. BugFix incorporates ideas from machine learning to automatically learn from new debugging situations and bug fixes over time. This enables more effective prediction of the most relevant bug-fix suggestions for newly-encountered debugging situations. The tool takes into account the static structure of a statement, the dynamic values used at that statement by both passing and failing runs, and the interesting value mapping pairs associated with that statement. We present a case study illustrating the efficacy of BugFix in helping developers to fix bugs.},
	booktitle = {{IEEE} 17th {International} {Conference} on {Program} {Comprehension}, 2009. {ICPC} '09},
	author = {Jeffrey, D. and Feng, Min and Gupta, N. and Gupta, R.},
	month = may,
	year = {2009},
	keywords = {BugFix, Computer bugs, Error correction, Failure analysis, Fault diagnosis, learning (artificial intelligence), learning-based tool, machine learning, program bugs, program debugging, Programming, Robustness, Runtime, Software debugging, Testing},
	pages = {70--79},
	file = {jeffrey_et_al_2009_bugfix.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/RCBQM8W3/jeffrey_et_al_2009_bugfix.pdf:application/pdf}
}

@inproceedings{gabriel_structure_2012,
	address = {New York, NY, USA},
	series = {Onward! 2012},
	title = {The {Structure} of a {Programming} {Language} {Revolution}},
	isbn = {978-1-4503-1562-3},
	url = {http://doi.acm.org/10.1145/2384592.2384611},
	doi = {10.1145/2384592.2384611},
	abstract = {Engineering often precedes science. Incommensurability is real.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the {ACM} {International} {Symposium} on {New} {Ideas}, {New} {Paradigms}, and {Reflections} on {Programming} and {Software}},
	publisher = {ACM},
	author = {Gabriel, Richard P.},
	year = {2012},
	keywords = {engineering, incommensurability, paradigms, science},
	pages = {195--214},
	file = {gabriel_2012_the_structure_of_a_programming_language_revolution.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/BEDPAIWN/gabriel_2012_the_structure_of_a_programming_language_revolution.pdf:application/pdf}
}

@inproceedings{tillmann_parameterized_2005,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE}-13},
	title = {Parameterized {Unit} {Tests}},
	isbn = {1-59593-014-0},
	url = {http://doi.acm.org/10.1145/1081706.1081749},
	doi = {10.1145/1081706.1081749},
	abstract = {Parameterized unit tests extend the current industry practice of using closed unit tests defined as parameterless methods. Parameterized unit tests separate two concerns: 1) They specify the external behavior of the involved methods for all test arguments. 2) Test cases can be re-obtained as traditional closed unit tests by instantiating the parameterized unit tests. Symbolic execution and constraint solving can be used to automatically choose a minimal set of inputs that exercise a parameterized unit test with respect to possible code paths of the implementation. In addition, parameterized unit tests can be used as symbolic summaries which allows symbolic execution to scale for arbitrary abstraction levels. We have developed a prototype tool which computes test cases from parameterized unit tests. We report on its first use testing parts of the .NET base class library.},
	urldate = {2015-01-24},
	booktitle = {Proceedings of the 10th {European} {Software} {Engineering} {Conference} {Held} {Jointly} with 13th {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Tillmann, Nikolai and Schulte, Wolfram},
	year = {2005},
	keywords = {algebraic data types, automatic test input generation, constraint solving, symbolic execution, unit testing},
	pages = {253--262},
	file = {tillmann_schulte_2005_parameterized_unit_tests.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/PCD9MUXQ/tillmann_schulte_2005_parameterized_unit_tests.pdf:application/pdf}
}

@article{khurshid_testera:_2004,
	title = {{TestEra}: {Specification}-{Based} {Testing} of {Java} {Programs} {Using} {SAT}},
	volume = {11},
	issn = {0928-8910, 1573-7535},
	shorttitle = {{TestEra}},
	url = {http://link.springer.com/article/10.1023/B%3AAUSE.0000038938.10589.b9},
	doi = {10.1023/B:AUSE.0000038938.10589.b9},
	abstract = {TestEra is a framework for automated specification-based testing of Java programs. TestEra requires as input a Java method (in sourcecode or bytecode), a formal specification of the pre- and post-conditions of that method, and a bound that limits the size of the test cases to be generated. Using the method's pre-condition, TestEra automatically generates all nonisomorphic test inputs up to the given bound. It executes the method on each test input, and uses the method postcondition as an oracle to check the correctness of each output. Specifications are first-order logic formulae. As an enabling technology, TestEra uses the Alloy toolset, which provides an automatic SAT-based tool for analyzing first-order logic formulae. We have used TestEra to check several Java programs including an architecture for dynamic networks, the Alloy-alpha analyzer, a fault-tree analyzer, and methods from the Java Collection Framework.},
	language = {en},
	number = {4},
	urldate = {2015-06-26},
	journal = {Automated Software Engineering},
	author = {Khurshid, Sarfraz and Marinov, Darko},
	month = oct,
	year = {2004},
	keywords = {alloy, Artificial Intelligence (incl. Robotics), automated test generation, Java testing, SAT enumeration, Software Engineering/Programming and Operating Systems, software testing, specification-based testing, TestEra},
	pages = {403--434},
	file = {khurshid_marinov_2004_testera.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/S7FEJ3Q6/khurshid_marinov_2004_testera.pdf:application/pdf}
}

@inproceedings{zhang_diagnosing_2015,
	address = {New York, NY, USA},
	series = {{PLDI} 2015},
	title = {Diagnosing {Type} {Errors} with {Class}},
	isbn = {978-1-4503-3468-6},
	url = {http://doi.acm.org/10.1145/2737924.2738009},
	doi = {10.1145/2737924.2738009},
	abstract = {Type inference engines often give terrible error messages, and the more sophisticated the type system the worse the problem. We show that even with the highly expressive type system implemented by the Glasgow Haskell Compiler (GHC)--including type classes, GADTs, and type families--it is possible to identify the most likely source of the type error, rather than the first source that the inference engine trips over. To determine which are the likely error sources, we apply a simple Bayesian model to a graph representation of the typing constraints; the satisfiability or unsatisfiability of paths within the graph provides evidence for or against possible explanations. While we build on prior work on error diagnosis for simpler type systems, inference in the richer type system of Haskell requires extending the graph with new nodes. The augmentation of the graph creates challenges both for Bayesian reasoning and for ensuring termination. Using a large corpus of Haskell programs, we show that this error localization technique is practical and significantly improves accuracy over the state of the art.},
	urldate = {2015-06-15},
	booktitle = {Proceedings of the 36th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Zhang, Danfeng and Myers, Andrew C. and Vytiniotis, Dimitrios and Peyton-Jones, Simon},
	year = {2015},
	keywords = {error diagnosis, haskell, type inference},
	pages = {12--21},
	file = {zhang_et_al_2015_diagnosing_type_errors_with_class.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/SKZHE8K8/zhang_et_al_2015_diagnosing_type_errors_with_class.pdf:application/pdf}
}

@inproceedings{krustev_software_1999,
	title = {Software test generation using refinement types},
	doi = {10.1109/ASE.1999.802321},
	abstract = {A novel approach for automatic software test generation is presented, which combines ideas from structural and functional testing as well as formal verification methods. It involves as an intermediate step, the construction of graphs and refinement types, which can be regarded as an automatically constructed semi-specification and used for formal verification. The technique is illustrated using a simple functional language, with algorithms for assigning refinement types and for test generation. Some desirable theoretical properties of the approach are briefly considered. It is also compared informally to other well-known as well as new techniques for automatic test generation},
	booktitle = {Automated {Software} {Engineering}, 1999. 14th {IEEE} {International} {Conference} on.},
	author = {Krustev, D.N.},
	month = oct,
	year = {1999},
	keywords = {automatically constructed semi-specification, automatic programming, automatic software test generation, automatic testing, Formal specifications, Formal verification, formal verification methods, functional languages, functional testing, Informatics, intermediate step, Logic testing, program testing, program verification, Read only memory, refinement types, simple functional language, Software quality, software testing, Tree graphs, type theory},
	pages = {279--282},
	file = {krustev_1999_software_test_generation_using_refinement_types.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/GMQ63HER/krustev_1999_software_test_generation_using_refinement_types.pdf:application/pdf}
}

@incollection{kuncak_executing_2013,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Executing {Specifications} {Using} {Synthesis} and {Constraint} {Solving}},
	copyright = {©2013 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-40786-4 978-3-642-40787-1},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-40787-1_1},
	abstract = {Specifications are key to improving software reliability as well as documenting precisely the intended behavior of software. Writing specifications is still perceived as expensive. Of course, writing implementations is at least as expensive, but is hardly questioned because there is currently no real alternative. Our goal is to give specifications a more balanced role compared to implementations, enabling the developers to compile, execute, optimize, and verify against each other mixed code fragments containing both specifications and implementations. To make specification constructs executable we combine deductive synthesis with run-time constraint solving, in both cases leveraging modern SMT solvers. Our tool decomposes specifications into simpler fragments using a cost-driven deductive synthesis framework. It compiles as many fragments as possible into conventional functional code; it executes the remaining fragments by invoking our constraint solver that extends an SMT solver to handle recursive functions. Using this approach we were able to execute constraints that describe the desired properties of integers, sets, maps and algebraic data types.},
	language = {en},
	number = {8174},
	urldate = {2015-06-26},
	booktitle = {Runtime {Verification}},
	publisher = {Springer Berlin Heidelberg},
	author = {Kuncak, Viktor and Kneuss, Etienne and Suter, Philippe},
	editor = {Legay, Axel and Bensalem, Saddek},
	year = {2013},
	keywords = {Algorithm Analysis and Problem Complexity, Logics and Meanings of Programs, Mathematical Logic and Formal Languages, Programming Languages, Compilers, Interpreters, Programming Techniques, Software Engineering},
	pages = {1--20},
	file = {kuncak_et_al_2013_executing_specifications_using_synthesis_and_constraint_solving.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/DVIGZENX/kuncak_et_al_2013_executing_specifications_using_synthesis_and_constraint_solving.pdf:application/pdf}
}

@inproceedings{csallner_check_2005,
	address = {New York, NY, USA},
	series = {{ICSE} '05},
	title = {Check '{N}' {Crash}: {Combining} {Static} {Checking} and {Testing}},
	isbn = {1-58113-963-2},
	shorttitle = {Check '{N}' {Crash}},
	url = {http://doi.acm.org/10.1145/1062455.1062533},
	doi = {10.1145/1062455.1062533},
	abstract = {We present an automatic error-detection approach that combines static checking and concrete test-case generation. Our approach consists of taking the abstract error conditions inferred using theorem proving techniques by a static checker (ESC/Java), deriving specific error conditions using a constraint solver, and producing concrete test cases (with the JCrasher tool) that are executed to determine whether an error truly exists. The combined technique has advantages over both static checking and automatic testing individually. Compared to ESC/Java, we eliminate spurious warnings and improve the ease-of-comprehension of error reports through the production of Java counterexamples. Compared to JCrasher, we eliminate the blind search of the input space, thus reducing the testing time and increasing the test quality.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Csallner, Christoph and Smaragdakis, Yannis},
	year = {2005},
	keywords = {automatic testing, dynamic analysis, extended static checking, static analysis, test case generation, usability},
	pages = {422--431},
	file = {csallner_smaragdakis_2005_check_'n'_crash.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/MBFSUM6J/csallner_smaragdakis_2005_check_'n'_crash.pdf:application/pdf}
}

@inproceedings{pike_smartcheck:_2014,
	address = {New York, NY, USA},
	series = {Haskell '14},
	title = {{SmartCheck}: {Automatic} and {Efficient} {Counterexample} {Reduction} and {Generalization}},
	isbn = {978-1-4503-3041-1},
	shorttitle = {{SmartCheck}},
	url = {http://doi.acm.org/10.1145/2633357.2633365},
	doi = {10.1145/2633357.2633365},
	abstract = {QuickCheck is a powerful library for automatic test-case generation. Because QuickCheck performs random testing, some of the counterexamples discovered are very large. QuickCheck provides an interface for the user to write shrink functions to attempt to reduce the size of counter examples. Hand-written implementations of shrink can be complex, inefficient, and consist of significant boilerplate code. Furthermore, shrinking is only one aspect in debugging: counterexample generalization is the process of extrapolating from individual counterexamples to a class of counterexamples, often requiring a flash of insight from the programmer. To improve counterexample reduction and generalization, we introduce SmartCheck. SmartCheck is a debugging tool that reduces algebraic data using generic search heuristics to efficiently find smaller counterexamples. In addition to shrinking, SmartCheck also automatically generalizes counterexamples to formulas representing classes of counterexamples. SmartCheck has been implemented for Haskell and is freely available.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 2014 {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Pike, Lee},
	year = {2014},
	keywords = {delta-debugging, property-based testing, test-case generalization},
	pages = {53--64},
	file = {pike_2014_smartcheck.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/F942STS2/pike_2014_smartcheck.pdf:application/pdf}
}

@article{nilsson_probabilistic_1993,
	title = {Probabilistic logic revisited},
	volume = {59},
	url = {http://www.sciencedirect.com/science/article/pii/000437029390167A},
	number = {1},
	urldate = {2015-05-14},
	journal = {Artificial intelligence},
	author = {Nilsson, Nils J.},
	year = {1993},
	pages = {39--42}
}

@inproceedings{wu_effect_2014,
	address = {New York, NY, USA},
	series = {Haskell '14},
	title = {Effect {Handlers} in {Scope}},
	isbn = {978-1-4503-3041-1},
	url = {http://doi.acm.org/10.1145/2633357.2633358},
	doi = {10.1145/2633357.2633358},
	abstract = {Algebraic effect handlers are a powerful means for describing effectful computations. They provide a lightweight and orthogonal technique to define and compose the syntax and semantics of different effects. The semantics is captured by handlers, which are functions that transform syntax trees. Unfortunately, the approach does not support syntax for scoping constructs, which arise in a number of scenarios. While handlers can be used to provide a limited form of scope, we demonstrate that this approach constrains the possible interactions of effects and rules out some desired semantics. This paper presents two different ways to capture scoped constructs in syntax, and shows how to achieve different semantics by reordering handlers. The first approach expresses scopes using the existing algebraic handlers framework, but has some limitations. The problem is fully solved in the second approach where we introduce higher-order syntax.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 2014 {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Wu, Nicolas and Schrijvers, Tom and Hinze, Ralf},
	year = {2014},
	keywords = {effect handlers, haskell, Modularity, monads, semantics, syntax},
	pages = {1--12},
	file = {wu_et_al_2014_effect_handlers_in_scope.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/4EHZZF57/wu_et_al_2014_effect_handlers_in_scope.pdf:application/pdf}
}

@article{ramsey_unparsing_1998,
	title = {Unparsing expressions with prefix and postfix operators},
	volume = {28},
	copyright = {Copyright © 1998 John Wiley \& Sons, Ltd.},
	issn = {1097-024X},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/(SICI)1097-024X(1998100)28:12<1327::AID-SPE195>3.0.CO;2-C/abstract},
	doi = {10.1002/(SICI)1097-024X(1998100)28:12<1327::AID-SPE195>3.0.CO;2-C},
	abstract = {Unparsing is the problem of transforming an internal representation of a program into an external, concrete syntax. In conjunction with prettyprinting, it is useful for generating readable programs from internal representations. If the target language uses prefix and postfix operators, the problem is nontrivial. This paper shows how to unparse expressions using a simple, bottom-up tree walk, which keeps track of the least tightly binding operator not enclosed by parentheses. During the tree walk, this operator is compared with the operator of the parent expression, and parentheses are inserted based on the precedence, associativity, and fixity (infix, prefix, or postfix) of the two operators. The paper is a literate program. It includes code for the unparser and for its inverse parser, both of which can handle operators of dynamically chosen precedence and associativity. Supporting such operators is useful for languages like ML, in which programmers may assign precedence and associativity to their own functions. © 1998 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {12},
	urldate = {2015-06-26},
	journal = {Software: Practice and Experience},
	author = {Ramsey, Norman},
	month = oct,
	year = {1998},
	keywords = {literate programming, parsing, prettyprinting, standard ml, unparsing},
	pages = {1327--1356},
	file = {ramsey_1998_unparsing_expressions_with_prefix_and_postfix_operators.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/M6JRWSUC/ramsey_1998_unparsing_expressions_with_prefix_and_postfix_operators.pdf:application/pdf}
}

@inproceedings{vazou_refinement_2014,
	address = {New York, NY, USA},
	series = {{ICFP} '14},
	title = {Refinement {Types} for {Haskell}},
	isbn = {978-1-4503-2873-9},
	url = {http://doi.acm.org/10.1145/2628136.2628161},
	doi = {10.1145/2628136.2628161},
	abstract = {SMT-based checking of refinement types for call-by-value languages is a well-studied subject. Unfortunately, the classical translation of refinement types to verification conditions is unsound under lazy evaluation. When checking an expression, such systems implicitly assume that all the free variables in the expression are bound to values. This property is trivially guaranteed by eager, but does not hold under lazy, evaluation. Thus, to be sound and precise, a refinement type system for Haskell and the corresponding verification conditions must take into account which subset of binders actually reduces to values. We present a stratified type system that labels binders as potentially diverging or not, and that (circularly) uses refinement types to verify the labeling. We have implemented our system in LIQUIDHASKELL and present an experimental evaluation of our approach on more than 10,000 lines of widely used Haskell libraries. We show that LIQUIDHASKELL is able to prove 96\% of all recursive functions terminating, while requiring a modest 1.7 lines of termination-annotations per 100 lines of code.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 19th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Vazou, Niki and Seidel, Eric L. and Jhala, Ranjit and Vytiniotis, Dimitrios and Peyton-Jones, Simon},
	year = {2014},
	pages = {269--282},
	file = {vazou_et_al_2014_refinement_types_for_haskell.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/F3PH3FUD/vazou_et_al_2014_refinement_types_for_haskell.pdf:application/pdf}
}

@inproceedings{meng_systematic_2011,
	address = {New York, NY, USA},
	series = {{PLDI} '11},
	title = {Systematic {Editing}: {Generating} {Program} {Transformations} from an {Example}},
	isbn = {978-1-4503-0663-8},
	shorttitle = {Systematic {Editing}},
	url = {http://doi.acm.org/10.1145/1993498.1993537},
	doi = {10.1145/1993498.1993537},
	abstract = {Software modifications are often systematic ---they consist of similar, but not identical, program changes to multiple contexts. Existing tools for systematic program transformation are limited because they require programmers to manually prescribe edits or only suggest a location to edit with a related example. This paper presents the design and implementation of a program transformation tool called SYDIT. Given an example edit, SYDIT generates a context-aware, abstract edit script, and then applies the edit script to new program locations. To correctly encode a relative position of the edits in a new location, the derived edit script includes unchanged statements on which the edits are control and data dependent. Furthermore, to make the edit script applicable to a new context using different identifier names, the derived edit script abstracts variable, method, and type names. The evaluation uses 56 systematic edit pairs from five large software projects as an oracle. SYDIT has high coverage and accuracy. For 82\% of the edits (46/56), SYDIT matches the context and applies an edit, producing code that is 96\% similar to the oracle. Overall, SYDIT mimics human programmers correctly on 70\% (39/56) of the edits. Generation of edit scripts seeks to improve programmer productivity by relieving developers from tedious, error-prone, manual code updates. It also has the potential to guide automated program repair by creating program transformations applicable to similar contexts.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 32Nd {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Meng, Na and Kim, Miryung and McKinley, Kathryn S.},
	year = {2011},
	keywords = {empirical study, program differencing, program transformation, software evolution},
	pages = {329--342},
	file = {meng_et_al_2011_systematic_editing.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/72RCAT5B/meng_et_al_2011_systematic_editing.pdf:application/pdf}
}

@inproceedings{pavlinovic_finding_2014,
	address = {New York, NY, USA},
	series = {{OOPSLA} '14},
	title = {Finding {Minimum} {Type} {Error} {Sources}},
	isbn = {978-1-4503-2585-1},
	url = {http://doi.acm.org/10.1145/2660193.2660230},
	doi = {10.1145/2660193.2660230},
	abstract = {Automatic type inference is a popular feature of functional programming languages. If a program cannot be typed, the compiler typically reports a single program location in its error message. This location is the point where the type inference failed, but not necessarily the actual source of the error. Other potential error sources are not even considered. Hence, the compiler often misses the true error source, which increases debugging time for the programmer. In this paper, we present a general framework for automatic localization of type errors. Our algorithm finds all minimum error sources, where the exact definition of minimum is given in terms of a compiler-specific ranking criterion. Compilers can use minimum error sources to produce more meaningful error reports, and for automatic error correction. Our approach works by reducing the search for minimum error sources to an optimization problem that we formulate in terms of weighted maximum satisfiability modulo theories (MaxSMT). The reduction to weighted MaxSMT allows us to build on SMT solvers to support rich type systems and at the same time abstract from the concrete criterion that is used for ranking the error sources. We have implemented an instance of our framework targeted at Hindley-Milner type systems and evaluated it on existing OCaml benchmarks for type error localization. Our evaluation shows that our approach has the potential to significantly improve the quality of type error reports produced by state of the art compilers.},
	urldate = {2015-04-27},
	booktitle = {Proceedings of the 2014 {ACM} {International} {Conference} on {Object} {Oriented} {Programming} {Systems} {Languages} \&\#38; {Applications}},
	publisher = {ACM},
	author = {Pavlinovic, Zvonimir and King, Tim and Wies, Thomas},
	year = {2014},
	keywords = {diagnostics, satisfiability modulo theories, type errors},
	pages = {525--542},
	file = {pavlinovic_et_al_2014_finding_minimum_type_error_sources.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/M8ICSCQE/pavlinovic_et_al_2014_finding_minimum_type_error_sources.pdf:application/pdf}
}

@inproceedings{kiselyov_freer_2015,
	address = {New York, NY, USA},
	series = {Haskell 2015},
	title = {Freer {Monads}, {More} {Extensible} {Effects}},
	isbn = {978-1-4503-3808-0},
	url = {http://doi.acm.org/10.1145/2804302.2804319},
	doi = {10.1145/2804302.2804319},
	abstract = {We present a rational reconstruction of extensible effects, the recently proposed alternative to monad transformers, as the confluence of efforts to make effectful computations compose. Free monads and then extensible effects emerge from the straightforward term representation of an effectful computation, as more and more boilerplate is abstracted away. The generalization process further leads to freer monads, constructed without the Functor constraint. The continuation exposed in freer monads can then be represented as an efficient type-aligned data structure. The end result is the algorithmically efficient extensible effects library, which is not only more comprehensible but also faster than earlier implementations. As an illustration of the new library, we show three surprisingly simple applications: non-determinism with committed choice (LogicT), catching IO exceptions in the presence of other effects, and the semi-automatic management of file handles and other resources through monadic regions. We extensively use and promote the new sort of `laziness', which underlies the left Kan extension: instead of performing an operation, keep its operands and pretend it is done.},
	urldate = {2015-09-03},
	booktitle = {Proceedings of the 8th {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Kiselyov, Oleg and Ishii, Hiromi},
	year = {2015},
	keywords = {coroutine, effect handler, effect interaction, free monad, Kan extension, open union, type and effect system},
	pages = {94--105},
	file = {kiselyov_ishii_2015_freer_monads,_more_extensible_effects.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/BM786FCE/kiselyov_ishii_2015_freer_monads,_more_extensible_effects.pdf:application/pdf}
}

@article{spohrer_novice_1986,
	title = {Novice {Mistakes}: {Are} the {Folk} {Wisdoms} {Correct}?},
	volume = {29},
	issn = {0001-0782},
	shorttitle = {Novice {Mistakes}},
	url = {http://doi.acm.org/10.1145/6138.6145},
	doi = {10.1145/6138.6145},
	abstract = {An evaluation of two folk wisdoms serves to elucidate the underlying or "deep-structure" reasons for novice errors.},
	number = {7},
	urldate = {2015-09-24},
	journal = {Commun. ACM},
	author = {Spohrer, James C. and Soloway, Elliot},
	month = jul,
	year = {1986},
	pages = {624--632},
	file = {spohrer_soloway_1986_novice_mistakes.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/QPMHR48B/spohrer_soloway_1986_novice_mistakes.pdf:application/pdf}
}

@incollection{okeefe_type_1992,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Type inference for partial types is decidable},
	copyright = {©1992 Springer-Verlag},
	isbn = {978-3-540-55253-6 978-3-540-46803-5},
	url = {http://link.springer.com/chapter/10.1007/3-540-55253-7_24},
	abstract = {The type inference problem for partial types, introduced by Thatte [15], is the problem of deducing types under a subtype relation with a largest element Ω and closed under the usual antimonotonic rule for function types. We show that this problem is decidable by reducing it to a satisfiability problem for type expressions over this partial order and giving an algorithm for the satisfiability problem. The satisfiability problem is harder than the one conventionally given because comparable types may have radically different shapes.},
	language = {en},
	number = {582},
	urldate = {2015-05-27},
	booktitle = {{ESOP} '92},
	publisher = {Springer Berlin Heidelberg},
	author = {O'Keefe, Patrick M. and Wand, Mitchell},
	editor = {Krieg-Brückner, Bernd},
	year = {1992},
	keywords = {Logics and Meanings of Programs, Mathematical Logic and Formal Languages, Programming Languages, Compilers, Interpreters, Software Engineering},
	pages = {408--417},
	file = {o'keefe_wand_1992_type_inference_for_partial_types_is_decidable.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/6G7TESNE/o'keefe_wand_1992_type_inference_for_partial_types_is_decidable.pdf:application/pdf}
}

@inproceedings{sen_cute:_2005,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE}-13},
	title = {{CUTE}: {A} {Concolic} {Unit} {Testing} {Engine} for {C}},
	isbn = {1-59593-014-0},
	shorttitle = {{CUTE}},
	url = {http://doi.acm.org/10.1145/1081706.1081750},
	doi = {10.1145/1081706.1081750},
	abstract = {In unit testing, a program is decomposed into units which are collections of functions. A part of unit can be tested by generating inputs for a single entry function. The entry function may contain pointer arguments, in which case the inputs to the unit are memory graphs. The paper addresses the problem of automating unit testing with memory graphs as inputs. The approach used builds on previous work combining symbolic and concrete execution, and more specifically, using such a combination to generate test inputs to explore all feasible execution paths. The current work develops a method to represent and track constraints that capture the behavior of a symbolic execution of a unit with memory graphs as inputs. Moreover, an efficient constraint solver is proposed to facilitate incremental generation of such test inputs. Finally, CUTE, a tool implementing the method is described together with the results of applying CUTE to real-world examples of C code.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 10th {European} {Software} {Engineering} {Conference} {Held} {Jointly} with 13th {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Sen, Koushik and Marinov, Darko and Agha, Gul},
	year = {2005},
	keywords = {concolic testing, data structure testing, explicit path model-checking, random testing, testing C programs, unit testing},
	pages = {263--272},
	file = {sen_et_al_2005_cute.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/9F5NHKNV/sen_et_al_2005_cute.pdf:application/pdf}
}

@inproceedings{rossberg_1ml_2015,
	address = {New York, NY, USA},
	series = {{ICFP} 2015},
	title = {1ML – {Core} and {Modules} {United} ({F}-ing {First}-class {Modules})},
	isbn = {978-1-4503-3669-7},
	url = {http://doi.acm.org/10.1145/2784731.2784738},
	doi = {10.1145/2784731.2784738},
	abstract = {ML is two languages in one: there is the core, with types and expressions, and there are modules, with signatures, structures and functors. Modules form a separate, higher-order functional language on top of the core. There are both practical and technical reasons for this stratification; yet, it creates substantial duplication in syntax and semantics, and it reduces expressiveness. For example, selecting a module cannot be made a dynamic decision. Language extensions allowing modules to be packaged up as first-class values have been proposed and implemented in different variations. However, they remedy expressiveness only to some extent, are syntactically cumbersome, and do not alleviate redundancy. We propose a redesign of ML in which modules are truly first-class values, and core and module layer are unified into one language. In this "1ML", functions, functors, and even type constructors are one and the same construct; likewise, no distinction is made between structures, records, or tuples. Or viewed the other way round, everything is just ("a mode of use of") modules. Yet, 1ML does not require dependent types, and its type structure is expressible in terms of plain System Fω, in a minor variation of our F-ing modules approach. We introduce both an explicitly typed version of 1ML, and an extension with Damas/Milner-style implicit quantification. Type inference for this language is not complete, but, we argue, not substantially worse than for Standard ML. An alternative view is that 1ML is a user-friendly surface syntax for System Fω that allows combining term and type abstraction in a more compositional manner than the bare calculus.},
	urldate = {2015-09-03},
	booktitle = {Proceedings of the 20th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Rossberg, Andreas},
	year = {2015},
	keywords = {abstract data types, elaboration, existential types, first-class modules, ML modules, System F, type systems},
	pages = {35--47},
	file = {rossberg_2015_1ml_–_core_and_modules_united_(f-ing_first-class_modules).pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/UWAIM9B7/rossberg_2015_1ml_–_core_and_modules_united_(f-ing_first-class_modules).pdf:application/pdf}
}

@inproceedings{stuckey_improving_2004,
	address = {New York, NY, USA},
	series = {Haskell '04},
	title = {Improving {Type} {Error} {Diagnosis}},
	isbn = {1-58113-850-4},
	url = {http://doi.acm.org/10.1145/1017472.1017486},
	doi = {10.1145/1017472.1017486},
	abstract = {We present a number of methods for providing improved type error reports in the Haskell and Chameleon programming languages. We build upon our previous work [19] where we first introduced the idea of discovering type errors by translating the typing problem into a constraint problem and looking for minimal unsatisfiable subsets of constraints. This allowed us to find precise sets of program locations which are in conflict with each other. Here we extend this approach by extracting additional useful information from these minimal unsatisfiable sets. This allows us to report errors as conflicts amongst a number of possible, candidate types. The advantage of our approach is that it offers implementors the flexibility to employ heuristics to select where, amongst all the locations involved, an error should be reported. In addition, we present methods for providing improved subsumption and ambiguity error reporting.},
	urldate = {2015-05-27},
	booktitle = {Proceedings of the 2004 {ACM} {SIGPLAN} {Workshop} on {Haskell}},
	publisher = {ACM},
	author = {Stuckey, Peter J. and Sulzmann, Martin and Wazny, Jeremy},
	year = {2004},
	keywords = {constraints, Hindley/Milner, overloading, type classes, type debugging, type inference},
	pages = {80--91},
	file = {stuckey_et_al_2004_improving_type_error_diagnosis.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/3BBHE74E/stuckey_et_al_2004_improving_type_error_diagnosis.pdf:application/pdf}
}

@inproceedings{bahr_generalising_2015,
	address = {New York, NY, USA},
	series = {{PEPM} '15},
	title = {Generalising {Tree} {Traversals} to {DAGs}: {Exploiting} {Sharing} {Without} the {Pain}},
	isbn = {978-1-4503-3297-2},
	shorttitle = {Generalising {Tree} {Traversals} to {DAGs}},
	url = {http://doi.acm.org/10.1145/2678015.2682539},
	doi = {10.1145/2678015.2682539},
	abstract = {We present a recursion scheme based on attribute grammars that can be transparently applied to trees and acyclic graphs. Our recursion scheme allows the programmer to implement a tree traversal and then apply it to compact graph representations of trees instead. The resulting graph traversals avoid recomputation of intermediate results for shared nodes -- even if intermediate results are used in different contexts. Consequently, this approach leads to asymptotic speedup proportional to the compression provided by the graph representation. In general, however, this sharing of intermediate results is not sound. Therefore, we complement our implementation of the recursion scheme with a number of correspondence theorems that ensure soundness for various classes of traversals. We illustrate the practical applicability of the implementation as well as the complementing theory with a number of examples.},
	urldate = {2015-08-25},
	booktitle = {Proceedings of the 2015 {Workshop} on {Partial} {Evaluation} and {Program} {Manipulation}},
	publisher = {ACM},
	author = {Bahr, Patrick and Axelsson, Emil},
	year = {2015},
	keywords = {attribute grammars, graph traversal, haskell, sharing},
	pages = {27--38},
	file = {bahr_axelsson_2015_generalising_tree_traversals_to_dags.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/828T5JEN/bahr_axelsson_2015_generalising_tree_traversals_to_dags.pdf:application/pdf}
}

@inproceedings{godefroid_dart:_2005,
	address = {New York, NY, USA},
	series = {{PLDI} '05},
	title = {{DART}: {Directed} {Automated} {Random} {Testing}},
	isbn = {1-59593-056-6},
	shorttitle = {{DART}},
	url = {http://doi.acm.org/10.1145/1065010.1065036},
	doi = {10.1145/1065010.1065036},
	abstract = {We present a new tool, named DART, for automatically testing software that combines three main techniques: (1) automated extraction of the interface of a program with its external environment using static source-code parsing; (2) automatic generation of a test driver for this interface that performs random testing to simulate the most general environment the program can operate in; and (3) dynamic analysis of how the program behaves under random testing and automatic generation of new test inputs to direct systematically the execution along alternative program paths. Together, these three techniques constitute Directed Automated Random Testing, or DART for short. The main strength of DART is thus that testing can be performed completely automatically on any program that compiles -- there is no need to write any test driver or harness code. During testing, DART detects standard errors such as program crashes, assertion violations, and non-termination. Preliminary experiments to unit test several examples of C programs are very encouraging.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 2005 {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Godefroid, Patrice and Klarlund, Nils and Sen, Koushik},
	year = {2005},
	keywords = {automated test generation, interfaces, program verification, random testing, software testing},
	pages = {213--223},
	file = {godefroid_et_al_2005_dart.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/9ZTJ7DXR/godefroid_et_al_2005_dart.pdf:application/pdf}
}

@incollection{seidel_automatically_2010,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Automatically {Generating} {Counterexamples} to {Naive} {Free} {Theorems}},
	copyright = {©2010 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-12250-7 978-3-642-12251-4},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-12251-4_14},
	abstract = {Disproof can be as important as proof in studying programs and programming languages. In particular, side conditions in a statement about program behavior are sometimes best understood and explored by trying to exhibit a falsifying example in the absence of a condition in question. Automation is as desirable for such falsification as it is for verification. We develop formal and implemented tools for counterexample generation in the context of free theorems, i.e., statements derived from polymorphic types à la relational parametricity. The machinery we use is rooted in constraining the type system and in intuitionistic proof search.},
	language = {en},
	number = {6009},
	urldate = {2015-05-23},
	booktitle = {Functional and {Logic} {Programming}},
	publisher = {Springer Berlin Heidelberg},
	author = {Seidel, Daniel and Voigtländer, Janis},
	editor = {Blume, Matthias and Kobayashi, Naoki and Vidal, Germán},
	year = {2010},
	keywords = {Artificial Intelligence (incl. Robotics), Logics and Meanings of Programs, Mathematical Logic and Formal Languages, Programming Languages, Compilers, Interpreters, Programming Techniques, Software Engineering},
	pages = {175--190},
	file = {seidel_voigtländer_2010_automatically_generating_counterexamples_to_naive_free_theorems.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/8U3S92DA/seidel_voigtländer_2010_automatically_generating_counterexamples_to_naive_free_theorems.pdf:application/pdf}
}

@inproceedings{sampson_expressing_2014,
	address = {New York, NY, USA},
	series = {{PLDI} '14},
	title = {Expressing and {Verifying} {Probabilistic} {Assertions}},
	isbn = {978-1-4503-2784-8},
	url = {http://doi.acm.org/10.1145/2594291.2594294},
	doi = {10.1145/2594291.2594294},
	abstract = {Traditional assertions express correctness properties that must hold on every program execution. However, many applications have probabilistic outcomes and consequently their correctness properties are also probabilistic (e.g., they identify faces in images, consume sensor data, or run on unreliable hardware). Traditional assertions do not capture these correctness properties. This paper proposes that programmers express probabilistic correctness properties with probabilistic assertions and describes a new probabilistic evaluation approach to efficiently verify these assertions. Probabilistic assertions are Boolean expressions that express the probability that a property will be true in a given execution rather than asserting that the property must always be true. Given either specific inputs or distributions on the input space, probabilistic evaluation verifies probabilistic assertions by first performing distribution extraction to represent the program as a Bayesian network. Probabilistic evaluation then uses statistical properties to simplify this representation to efficiently compute assertion probabilities directly or with sampling. Our approach is a mix of both static and dynamic analysis: distribution extraction statically builds and optimizes the Bayesian network representation and sampling dynamically interprets this representation. We implement our approach in a tool called Mayhap for C and C++ programs. We evaluate expressiveness, correctness, and performance of Mayhap on programs that use sensors, perform approximate computation, and obfuscate data for privacy. Our case studies demonstrate that probabilistic assertions describe useful correctness properties and that Mayhap efficiently verifies them.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 35th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Sampson, Adrian and Panchekha, Pavel and Mytkowicz, Todd and McKinley, Kathryn S. and Grossman, Dan and Ceze, Luis},
	year = {2014},
	keywords = {approximate computing, data obfuscation, differential privacy, probabilistic programming, sensors, symbolic execution},
	pages = {112--122},
	file = {sampson_et_al_2014_expressing_and_verifying_probabilistic_assertions.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/F8N7WMHU/sampson_et_al_2014_expressing_and_verifying_probabilistic_assertions.pdf:application/pdf}
}

@misc{_hott_????,
	title = {The {HoTT} {Book}},
	url = {http://homotopytypetheory.org/book/},
	abstract = {Homotopy Type Theory: Univalent Foundations of Mathematics The Univalent Foundations Program Institute for Advanced Study Buy a hardcover copy for \$21.83. [609 pages, 6" × 9" size, hardcover] Buy a...},
	urldate = {2015-05-26},
	journal = {Homotopy Type Theory},
	file = {Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/K7MG424M/book.html:text/html}
}

@misc{chuchem_lamdu:_????,
	title = {Lamdu: {Towards} a next generation {IDE}},
	shorttitle = {Yair {Chuchem}},
	url = {https://vimeo.com/97713439},
	abstract = {Lamdu is a structural code editor which mimics the convenience of textual code editing while leveraging its understanding of the code to offer intelligent type-aware…},
	urldate = {2015-09-17},
	author = {Chuchem, Yair},
	file = {Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/TAJ6PX9V/97713439.html:text/html}
}

@inproceedings{adams_indentation-sensitive_2014,
	address = {New York, NY, USA},
	series = {Haskell '14},
	title = {Indentation-sensitive {Parsing} for {Parsec}},
	isbn = {978-1-4503-3041-1},
	url = {http://doi.acm.org/10.1145/2633357.2633369},
	doi = {10.1145/2633357.2633369},
	abstract = {Several popular languages including Haskell and Python use the indentation and layout of code as an essential part of their syntax. In the past, implementations of these languages used ad hoc techniques to implement layout. Recent work has shown that a simple extension to context-free grammars can replace these ad hoc techniques and provide both formal foundations and efficient parsing algorithms for indentation sensitivity. However, that previous work is limited to bottom-up, LR(\$k\$) parsing, and many combinator-based parsing frameworks including Parsec use top-down algorithms that are outside its scope. This paper remedies this by showing how to add indentation sensitivity to parsing frameworks like Parsec. It explores both the formal semantics of and efficient algorithms for indentation sensitivity. It derives a Parsec-based library for indentation-sensitive parsing and presents benchmarks on a real-world language that show its efficiency and practicality.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 2014 {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Adams, Michael D. and Ağacan, Ömer S.},
	year = {2014},
	keywords = {indentation sensitivity, layout, offside rule, parsec, parsing},
	pages = {121--132},
	file = {adams_ağacan_2014_indentation-sensitive_parsing_for_parsec.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/5VBQKU39/adams_ağacan_2014_indentation-sensitive_parsing_for_parsec.pdf:application/pdf}
}

@incollection{dybjer_combining_2003,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Combining {Testing} and {Proving} in {Dependent} {Type} {Theory}},
	copyright = {©2003 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-40664-8 978-3-540-45130-3},
	url = {http://link.springer.com/chapter/10.1007/10930755_12},
	abstract = {We extend the proof assistant Agda/Alfa for dependent type theory with a modified version of Claessen and Hughes’ tool QuickCheck for random testing of functional programs. In this way we combine testing and proving in one system. Testing is used for debugging programs and specifications before a proof is attempted. Furthermore, we demonstrate by example how testing can be used repeatedly during proof for testing suitable subgoals. Our tool uses testdata generators which are defined inside Agda/Alfa. We can therefore use the type system to prove properties about them, in particular surjectivity stating that all possible test cases can indeed be generated.},
	language = {en},
	number = {2758},
	urldate = {2015-04-29},
	booktitle = {Theorem {Proving} in {Higher} {Order} {Logics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Dybjer, Peter and Haiyan, Qiao and Takeyama, Makoto},
	editor = {Basin, David and Wolff, Burkhart},
	year = {2003},
	keywords = {Artificial Intelligence (incl. Robotics), Logic Design, Logics and Meanings of Programs, Mathematical Logic and Formal Languages, Software Engineering},
	pages = {188--203},
	file = {dybjer_et_al_2003_combining_testing_and_proving_in_dependent_type_theory.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/FX5JZVXV/dybjer_et_al_2003_combining_testing_and_proving_in_dependent_type_theory.pdf:application/pdf}
}

@book{pope_buddha_1998,
	title = {Buddha – {A} {Declarative} {Debugger} for {Haskell}},
	abstract = {Due to their reliance on the execution order of programs, traditional debugging techniques are not well suited to locating the source of logical errors in programs written in lazy functional languages. We describe the implementation of a declarative debugger for the programming language Haskell, which assists the location of logical errors based on the declarative semantics of program definitions. The implementation is based on the Hugs interpreter, and both solidifies previous work in the field and extends it to incorporate features typical of many modern lazy functional languages.},
	author = {Pope, Bernard},
	year = {1998},
	file = {pope_1998_buddha_–_a_declarative_debugger_for_haskell.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/2XJPXC56/pope_1998_buddha_–_a_declarative_debugger_for_haskell.pdf:application/pdf}
}

@inproceedings{wortmann_causality_2013,
	address = {New York, NY, USA},
	series = {Haskell '13},
	title = {Causality of {Optimized} {Haskell}: {What} is {Burning} {Our} {Cycles}?},
	isbn = {978-1-4503-2383-3},
	shorttitle = {Causality of {Optimized} {Haskell}},
	url = {http://doi.acm.org/10.1145/2503778.2503788},
	doi = {10.1145/2503778.2503788},
	abstract = {Profiling real-world Haskell programs is hard, as compiler optimizations make it tricky to establish causality between the source code and program behavior. In this paper we attack the root issue by performing a causality analysis of functional programs under optimization. We apply our findings to build a novel profiling infrastructure on top of the Glasgow Haskell Compiler, allowing for performance analysis even of aggressively optimized programs.},
	urldate = {2015-06-02},
	booktitle = {Proceedings of the 2013 {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Wortmann, Peter M. and Duke, David},
	year = {2013},
	keywords = {causality, haskell, optimization, profiling},
	pages = {141--152},
	file = {wortmann_duke_2013_causality_of_optimized_haskell.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/TEKE4EKS/wortmann_duke_2013_causality_of_optimized_haskell.pdf:application/pdf}
}

@article{hall_type_1996,
	title = {Type {Classes} in {Haskell}},
	volume = {18},
	issn = {0164-0925},
	url = {http://doi.acm.org/10.1145/227699.227700},
	doi = {10.1145/227699.227700},
	number = {2},
	urldate = {2015-06-26},
	journal = {ACM Trans. Program. Lang. Syst.},
	author = {Hall, Cordelia V. and Hammond, Kevin and Peyton Jones, Simon L. and Wadler, Philip L.},
	month = mar,
	year = {1996},
	keywords = {functional programming, haskell, type classes, types},
	pages = {109--138},
	file = {hall_et_al_1996_type_classes_in_haskell.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/XRBDZ56D/hall_et_al_1996_type_classes_in_haskell.pdf:application/pdf}
}

@inproceedings{pacheco_feedback-directed_2007,
	title = {Feedback-{Directed} {Random} {Test} {Generation}},
	doi = {10.1109/ICSE.2007.37},
	abstract = {We present a technique that improves random test generation by incorporating feedback obtained from executing test inputs as they are created. Our technique builds inputs incrementally by randomly selecting a method call to apply and finding arguments from among previously-constructed inputs. As soon as an input is built, it is executed and checked against a set of contracts and filters. The result of the execution determines whether the input is redundant, illegal, contract-violating, or useful for generating more inputs. The technique outputs a test suite consisting of unit tests for the classes under test. Passing tests can be used to ensure that code contracts are preserved across program changes; failing tests (that violate one or more contract) point to potential errors that should be corrected. Our experimental results indicate that feedback-directed random test generation can outperform systematic and undirected random test generation, in terms of coverage and error detection. On four small but nontrivial data structures (used previously in the literature), our technique achieves higher or equal block and predicate coverage than model checking (with and without abstraction) and undirected random generation. On 14 large, widely-used libraries (comprising 780KLOC), feedback-directed random test generation finds many previously-unknown errors, not found by either model checking or undirected random generation.},
	booktitle = {29th {International} {Conference} on {Software} {Engineering}, 2007. {ICSE} 2007},
	author = {Pacheco, C. and Lahiri, S.K. and Ernst, M.D. and Ball, T.},
	month = may,
	year = {2007},
	keywords = {Contracts, Error correction codes, error detection, failing tests, Feedback, feedback-directed random test generation, Filters, Law, Legal factors, Object oriented modeling, Open source software, passing tests, program testing, software testing, System testing},
	pages = {75--84},
	file = {pacheco_et_al_2007_feedback-directed_random_test_generation.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/5B9A7FW2/pacheco_et_al_2007_feedback-directed_random_test_generation.pdf:application/pdf}
}

@article{gonthier_how_2013,
	title = {How to make ad hoc proof automation less ad hoc},
	volume = {23},
	issn = {1469-7653},
	url = {http://journals.cambridge.org/article_S0956796813000051},
	doi = {10.1017/S0956796813000051},
	abstract = {Most interactive theorem provers provide support for some form of user-customizable proof automation. In a number of popular systems, such as Coq and Isabelle, this automation is achieved primarily through tactics, which are programmed in a separate language from that of the prover's base logic. While tactics are clearly useful in practice, they can be difficult to maintain and compose because, unlike lemmas, their behavior cannot be specified within the expressive type system of the prover itself. We propose a novel approach to proof automation in Coq that allows the user to specify the behavior of custom automated routines in terms of Coq's own type system. Our approach involves a sophisticated application of Coq's canonical structures, which generalize Haskell type classes and facilitate a flexible style of dependently-typed logic programming. Specifically, just as Haskell type classes are used to infer the canonical implementation of an overloaded term at a given type, canonical structures can be used to infer the canonical proof of an overloaded lemma for a given instantiation of its parameters. We present a series of design patterns for canonical structure programming that enable one to carefully and predictably coax Coq's type inference engine into triggering the execution of user-supplied algorithms during unification, and we illustrate these patterns through several realistic examples drawn from Hoare Type Theory. We assume no prior knowledge of Coq and describe the relevant aspects of Coq type inference from first principles.},
	number = {Special Issue 04},
	urldate = {2015-06-26},
	journal = {Journal of Functional Programming},
	author = {Gonthier, Georges and Ziliani, Beta and Nanevski, Aleksandar and Dreyer, Derek},
	year = {2013},
	pages = {357--401},
	file = {gonthier_et_al_2013_how_to_make_ad_hoc_proof_automation_less_ad_hoc.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/ZFK3CDHX/gonthier_et_al_2013_how_to_make_ad_hoc_proof_automation_less_ad_hoc.pdf:application/pdf}
}

@techreport{andoni_evaluating_2003,
	title = {Evaluating the “{Small} {Scope} {Hypothesis}”},
	abstract = {The “small scope hypothesis” argues that a high proportion of bugs can be found by testing the program for all test inputs within some small scope. In object-oriented programs, a test input is constructed from objects of different classes; a test input is within a scope of ¢ if at most ¢ objects of any given class appear in it. If the hypothesis holds, it follows that it is more effective to do systematic testing within a small scope than to generate fewer test inputs of a larger scope. This paper evaluates the hypothesis for several implementations of data structures, including some from the Java Collections Framework. We measure how statement coverage, branch coverage, and rate of mutant killing vary with scope. For systematic input generation and correctness checking of Java programs, we use the Korat framework. This paper also presents the Ferastrau framework that we have developed for mutation testing of Java programs. The experimental results show that exhaustive testing within small scopes can achieve complete coverage and kill most of the mutants, even for intricate methods that manipulate complex data structures. The results also show that Korat can be used effectively to generate inputs and check correctness for these scopes. 1.},
	author = {Andoni, Alexandr and Daniliuc, Dumitru and Khurshid, Sarfraz},
	year = {2003},
	file = {andoni_et_al_2003_evaluating_the_“small_scope_hypothesis”.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/RRXNS3AU/andoni_et_al_2003_evaluating_the_“small_scope_hypothesis”.pdf:application/pdf}
}

@incollection{chugh_isolate:_2015,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{IsoLATE}: {A} {Type} {System} for {Self}-recursion},
	copyright = {©2015 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-662-46668-1 978-3-662-46669-8},
	shorttitle = {{IsoLATE}},
	url = {http://link.springer.com/chapter/10.1007/978-3-662-46669-8_11},
	abstract = {A fundamental aspect of object-oriented languages is how recursive functions are defined. One semantic approach is to use simple record types and explicit recursion (i.e. fix) to define mutually recursive units of functionality. Another approach is to use records and recursive types to describe recursion through a “self” parameter. Many systems rely on both semantic approaches as well as combinations of universally quantified types, existentially quantified types, and mixin operators to encode patterns of method reuse, data encapsulation, and “open recursion” through self. These more complex mechanisms are needed to support many important use cases, but they often lack desirable theoretical properties, such as decidability, and can be difficult to implement, because of the equirecursive interpretation that identifies mu-types with their unfoldings. Furthermore, these systems do not apply to languages without explicit recursion (such as JavaScript, Python, and Ruby). In this paper, we present a statically typed calculus of functional objects called ISOLATE that can reason about a pattern of mixin composition without relying on an explicit fixpoint operation. To accomplish this, ISOLATE extends a standard isorecursive type system with a mechanism for checking the “mutual consistency” of a collection of functions, that is, that all of the assumptions about self are implied by the collection itself. We prove the soundness of ISOLATE via a type-preserving translation to a calculus with F-bounded polymorphism. Therefore, ISOLATE can be regarded as a stylized subset of the more expressive calculus that admits an interesting class of programs yet is easy to implement. In the future, we plan to investigate how other, more complicated forms of mixin composition (again, without explicit recursion) may be supported by lightweight type systems.},
	language = {en},
	number = {9032},
	urldate = {2015-04-27},
	booktitle = {Programming {Languages} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Chugh, Ravi},
	editor = {Vitek, Jan},
	month = apr,
	year = {2015},
	keywords = {Algorithm Analysis and Problem Complexity, Artificial Intelligence (incl. Robotics), Database Management, Data Mining and Knowledge Discovery, Information Storage and Retrieval, Information Systems Applications (incl. Internet)},
	pages = {257--282},
	file = {chugh_2015_isolate.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/9GSWVE4R/chugh_2015_isolate.pdf:application/pdf}
}

@article{hoyrup_computability_2009,
	title = {Computability of probability measures and {Martin}-{Löf} randomness over metric spaces},
	volume = {207},
	issn = {0890-5401},
	url = {http://www.sciencedirect.com/science/article/pii/S0890540109000170},
	doi = {10.1016/j.ic.2008.12.009},
	abstract = {In this paper, we investigate algorithmic randomness on more general spaces than the Cantor space, namely computable metric spaces. To do this, we first develop a unified framework allowing computations with probability measures. We show that any computable metric space with a computable probability measure is isomorphic to the Cantor space in a computable and measure-theoretic sense. We show that any computable metric space admits a universal uniform randomness test (without further assumption).},
	number = {7},
	urldate = {2015-06-26},
	journal = {Information and Computation},
	author = {Hoyrup, Mathieu and Rojas, Cristóbal},
	month = jul,
	year = {2009},
	keywords = {Algorithmic randomness, Almost computable function, Computability, Computable metric space, Computable probability measure, Enumerative lattice, Kolmogorov–Chaitin complexity, Universal test},
	pages = {830--847},
	file = {hoyrup_rojas_2009_computability_of_probability_measures_and_martin-löf_randomness_over_metric.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/7JWDVVSA/hoyrup_rojas_2009_computability_of_probability_measures_and_martin-löf_randomness_over_metric.pdf:application/pdf}
}

@inproceedings{diehl_generic_2014,
	address = {New York, NY, USA},
	series = {{WGP} '14},
	title = {Generic {Constructors} and {Eliminators} from {Descriptions}: {Type} {Theory} {As} a {Dependently} {Typed} {Internal} {DSL}},
	isbn = {978-1-4503-3042-8},
	shorttitle = {Generic {Constructors} and {Eliminators} from {Descriptions}},
	url = {http://doi.acm.org/10.1145/2633628.2633630},
	doi = {10.1145/2633628.2633630},
	abstract = {Dependently typed languages with an "open" type theory introduce new datatypes using an axiomatic approach. Each new datatype introduces axioms for constructing values of the datatype, and an elimination axiom (which we call the standard eliminator) for consuming such values. In a "closed" type theory a single introduction rule primitive and a single elimination rule primitive can be used for all datatypes, without adding axioms to the theory. We review a closed type theory, specified as an Agda program, that uses descriptions for datatype construction. Descriptions make datatype definitions first class values, but writing programs using such datatypes requires low-level understanding of how the datatypes are encoded in terms of descriptions. In this work we derive constructors and standard eliminators, by defining generic functions parameterized by a description. Our generic type theory constructions are defined as generic wrappers around the closed type theory primitives, which are themselves generic functions in the Agda model. Thus, we allow users to write programs in the model without understanding the details of the description-based encoding of datatypes, by using open type theory constructions as an internal domain-specific language (IDSL).},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 10th {ACM} {SIGPLAN} {Workshop} on {Generic} {Programming}},
	publisher = {ACM},
	author = {Diehl, Larry and Sheard, Tim},
	year = {2014},
	keywords = {dependent types, descriptions, eliminators, generic programming},
	pages = {3--14},
	file = {diehl_sheard_2014_generic_constructors_and_eliminators_from_descriptions.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/2U8JCBM7/diehl_sheard_2014_generic_constructors_and_eliminators_from_descriptions.pdf:application/pdf}
}

@inproceedings{chen_type-based_2014,
	address = {New York, NY, USA},
	series = {{ICFP} '14},
	title = {Type-based {Parametric} {Analysis} of {Program} {Families}},
	isbn = {978-1-4503-2873-9},
	url = {http://doi.acm.org/10.1145/2628136.2628155},
	doi = {10.1145/2628136.2628155},
	abstract = {Previous research on static analysis for program families has focused on lifting analyses for single, plain programs to program families by employing idiosyncratic representations. The lifting effort typically involves a significant amount of work for proving the correctness of the lifted algorithm and demonstrating its scalability. In this paper, we propose a parameterized static analysis framework for program families that can automatically lift a class of type-based static analyses for plain programs to program families. The framework consists of a parametric logical specification and a parametric variational constraint solver. We prove that a lifted algorithm is correct provided that the underlying analysis algorithm is correct. An evaluation of our framework has revealed an error in a previous manually lifted analysis. Moreover, performance tests indicate that the overhead incurred by the general framework is bounded by a factor of 2.},
	urldate = {2015-10-27},
	booktitle = {Proceedings of the 19th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Chen, Sheng and Erwig, Martin},
	year = {2014},
	keywords = {choice calculus, constraint-based type system, program families, static-analysis lifting, variational types},
	pages = {39--51},
	file = {chen_erwig_2014_type-based_parametric_analysis_of_program_families.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/GNEVVRA5/chen_erwig_2014_type-based_parametric_analysis_of_program_families.pdf:application/pdf}
}

@incollection{seidel_type_2015,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Type {Targeted} {Testing}},
	copyright = {©2015 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-662-46668-1 978-3-662-46669-8},
	url = {http://link.springer.com/chapter/10.1007/978-3-662-46669-8_33},
	abstract = {We present a new technique called type targeted testing, which translates precise refinement types into comprehensive test-suites. The key insight behind our approach is that through the lens of SMT solvers, refinement types can also be viewed as a high-level, declarative, test generation technique, wherein types are converted to SMT queries whose models can be decoded into concrete program inputs. Our approach enables the systematic and exhaustive testing of implementations from high-level declarative specifications, and furthermore, provides a gradual path from testing to full verification. We have implemented our approach as a Haskell testing tool called TARGET, and present an evaluation that shows how TARGET can be used to test a wide variety of properties and how it compares against state-of-the-art testing approaches.},
	language = {en},
	number = {9032},
	urldate = {2015-04-27},
	booktitle = {Programming {Languages} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Seidel, Eric L. and Vazou, Niki and Jhala, Ranjit},
	editor = {Vitek, Jan},
	month = apr,
	year = {2015},
	keywords = {Algorithm Analysis and Problem Complexity, Artificial Intelligence (incl. Robotics), Database Management, Data Mining and Knowledge Discovery, Information Storage and Retrieval, Information Systems Applications (incl. Internet)},
	pages = {812--836},
	file = {seidel_et_al_2015_type_targeted_testing.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/A4728DCV/seidel_et_al_2015_type_targeted_testing.pdf:application/pdf}
}

@article{chamarthi_integrating_2011,
	title = {Integrating {Testing} and {Interactive} {Theorem} {Proving}},
	volume = {70},
	issn = {2075-2180},
	url = {http://arxiv.org/abs/1105.4394},
	doi = {10.4204/EPTCS.70.1},
	abstract = {Using an interactive theorem prover to reason about programs involves a sequence of interactions where the user challenges the theorem prover with conjectures. Invariably, many of the conjectures posed are in fact false, and users often spend considerable effort examining the theorem prover's output before realizing this. We present a synergistic integration of testing with theorem proving, implemented in the ACL2 Sedan (ACL2s), for automatically generating concrete counterexamples. Our method uses the full power of the theorem prover and associated libraries to simplify conjectures; this simplification can transform conjectures for which finding counterexamples is hard into conjectures where finding counterexamples is trivial. In fact, our approach even leads to better theorem proving, e.g. if testing shows that a generalization step leads to a false conjecture, we force the theorem prover to backtrack, allowing it to pursue more fruitful options that may yield a proof. The focus of the paper is on the engineering of a synergistic integration of testing with interactive theorem proving; this includes extending ACL2 with new functionality that we expect to be of general interest. We also discuss our experience in using ACL2s to teach freshman students how to reason about their programs.},
	urldate = {2015-04-29},
	journal = {Electronic Proceedings in Theoretical Computer Science},
	author = {Chamarthi, Harsh Raju and Dillinger, Peter C. and Kaufmann, Matt and Manolios, Panagiotis},
	month = oct,
	year = {2011},
	note = {arXiv: 1105.4394},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Computer Science - Software Engineering},
	pages = {4--19},
	file = {chamarthi_et_al_2011_integrating_testing_and_interactive_theorem_proving.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/H6DS5ZZB/chamarthi_et_al_2011_integrating_testing_and_interactive_theorem_proving.pdf:application/pdf}
}

@book{beizer_black-box_1995,
	address = {New York, NY, USA},
	title = {Black-box {Testing}: {Techniques} for {Functional} {Testing} of {Software} and {Systems}},
	isbn = {0-471-12094-4},
	shorttitle = {Black-box {Testing}},
	publisher = {John Wiley \&amp; Sons, Inc.},
	author = {Beizer, Boris},
	year = {1995}
}

@inproceedings{neubauer_discriminative_2003,
	address = {New York, NY, USA},
	series = {{ICFP} '03},
	title = {Discriminative {Sum} {Types} {Locate} the {Source} of {Type} {Errors}},
	isbn = {1-58113-756-7},
	url = {http://doi.acm.org/10.1145/944705.944708},
	doi = {10.1145/944705.944708},
	abstract = {We propose a type system for locating the source of type errors in an applied lambda calculus with ML-style polymorphism. The system is based on discriminative sum types---known from work on soft typing---with annotation subtyping and recursive types. This way, type clashes can be registered in the type for later reporting. The annotations track the potential producers and consumers for each value so that clashes can be traced to their cause.Every term is typeable in our system and type inference is decidable. A type derivation in our system describes all type errors present in the program, so that a principal derivation yields a principal description of all type errors present. Error messages are derived from completed type derivations. Thus, error messages are independent of the particular algorithm used for type inference, provided it constructs such a derivation.},
	urldate = {2015-05-27},
	booktitle = {Proceedings of the {Eighth} {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Neubauer, Matthias and Thiemann, Peter},
	year = {2003},
	keywords = {polymorphism, type errors, type inference},
	pages = {15--26},
	file = {neubauer_thiemann_2003_discriminative_sum_types_locate_the_source_of_type_errors.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/DF55DR6U/neubauer_thiemann_2003_discriminative_sum_types_locate_the_source_of_type_errors.pdf:application/pdf}
}

@inproceedings{lindahl_practical_2006,
	address = {New York, NY, USA},
	series = {{PPDP} '06},
	title = {Practical {Type} {Inference} {Based} on {Success} {Typings}},
	isbn = {1-59593-388-3},
	url = {http://doi.acm.org/10.1145/1140335.1140356},
	doi = {10.1145/1140335.1140356},
	abstract = {In languages where the compiler performs no static type checks, many programs never go wrong, but the intended use of functions and component interfaces is often undocumented or appears only in the form of comments which cannot always be trusted. This often makes program maintenance problematic. We show that it is possible to reconstruct a significant portion of the type information which is implicit in a program, automatically annotate function interfaces, and detect definite type clashes without fundamental changes to the philosophy of the language or imposing a type system which unnecessarily rejects perfectly reasonable programs. To do so, we introduce the notion of success typings of functions. Unlike most static type systems, success typings incorporate subtyping and never disallow a use of a function that will not result in a type clash during runtime. Unlike most soft typing systems that have previously been proposed, success typings allow for compositional, bottom-up type inference which appears to scale well in practice. Moreover, by taking control-flow into account and exploiting properties of the language such as its module system, success typings can be refined and become accurate and precise We demonstrate the power and practicality of the approach by applying it to Erlang. We report on our experiences from employing the type inference algorithm, without any guidance, on programs of significant size},
	urldate = {2015-05-27},
	booktitle = {Proceedings of the 8th {ACM} {SIGPLAN} {International} {Conference} on {Principles} and {Practice} of {Declarative} {Programming}},
	publisher = {ACM},
	author = {Lindahl, Tobias and Sagonas, Konstantinos},
	year = {2006},
	keywords = {constraint-based type inference, Erlang, subtyping, success typings},
	pages = {167--178},
	file = {lindahl_sagonas_2006_practical_type_inference_based_on_success_typings.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/KAMINNVP/lindahl_sagonas_2006_practical_type_inference_based_on_success_typings.pdf:application/pdf}
}

@inproceedings{farmer_reasoning_2015,
	address = {New York, NY, USA},
	series = {Haskell 2015},
	title = {Reasoning with the {HERMIT}: {Tool} {Support} for {Equational} {Reasoning} on {GHC} {Core} {Programs}},
	isbn = {978-1-4503-3808-0},
	shorttitle = {Reasoning with the {HERMIT}},
	url = {http://doi.acm.org/10.1145/2804302.2804303},
	doi = {10.1145/2804302.2804303},
	abstract = {A benefit of pure functional programming is that it encourages equational reasoning. However, the Haskell language has lacked direct tool support for such reasoning. Consequently, reasoning about Haskell programs is either performed manually, or in another language that does provide tool support (e.g. Agda or Coq). HERMIT is a Haskell-specific toolkit designed to support equational reasoning and user-guided program transformation, and to do so as part of the GHC compilation pipeline. This paper describes HERMIT's recently developed support for equational reasoning, and presents two case studies of HERMIT usage: checking that type-class laws hold for specific instance declarations, and mechanising textbook equational reasoning.},
	urldate = {2015-09-03},
	booktitle = {Proceedings of the 8th {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Farmer, Andrew and Sculthorpe, Neil and Gill, Andy},
	year = {2015},
	keywords = {equational reasoning, HERMIT, Type Class Laws},
	pages = {23--34},
	file = {farmer_et_al_2015_reasoning_with_the_hermit.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/QRW9EW9Z/farmer_et_al_2015_reasoning_with_the_hermit.pdf:application/pdf}
}

@inproceedings{pombrio_hygienic_2015,
	address = {New York, NY, USA},
	series = {{ICFP} 2015},
	title = {Hygienic {Resugaring} of {Compositional} {Desugaring}},
	isbn = {978-1-4503-3669-7},
	url = {http://doi.acm.org/10.1145/2784731.2784755},
	doi = {10.1145/2784731.2784755},
	abstract = {Syntactic sugar is widely used in language implementation. Its benefits are, however, offset by the comprehension problems it presents to programmers once their program has been transformed. In particular, after a transformed program has begun to evaluate (or otherwise be altered by a black-box process), it can become unrecognizable. We present a new approach to \_resugaring\_ programs, which is the act of reflecting evaluation steps in the core language in terms of the syntactic sugar that the programmer used. Relative to prior work, our approach has two important advances: it handles hygiene, and it allows almost arbitrary rewriting rules (as opposed to restricted patterns). We do this in the context of a DAG representation of programs, rather than more traditional trees.},
	urldate = {2015-09-03},
	booktitle = {Proceedings of the 20th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Pombrio, Justin and Krishnamurthi, Shriram},
	year = {2015},
	keywords = {abstract syntax DAG, hygiene, resugaring, syntactic sugar},
	pages = {75--87},
	file = {pombrio_krishnamurthi_2015_hygienic_resugaring_of_compositional_desugaring.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/467UIZ8S/pombrio_krishnamurthi_2015_hygienic_resugaring_of_compositional_desugaring.pdf:application/pdf}
}

@inproceedings{kawaguchi_type-based_2009,
	address = {New York, NY, USA},
	series = {{PLDI} '09},
	title = {Type-based {Data} {Structure} {Verification}},
	isbn = {978-1-60558-392-1},
	url = {http://doi.acm.org/10.1145/1542476.1542510},
	doi = {10.1145/1542476.1542510},
	abstract = {We present a refinement type-based approach for the static verification of complex data structure invariants. Our approach is based on the observation that complex data structures are typically fashioned from two elements: recursion (e.g., lists and trees), and maps (e.g., arrays and hash tables). We introduce two novel type-based mechanisms targeted towards these elements: recursive refinements and polymorphic refinements. These mechanisms automate the challenging work of generalizing and instantiating rich universal invariants by piggybacking simple refinement predicates on top of types, and carefully dividing the labor of analysis between the type system and an SMT solver. Further, the mechanisms permit the use of the abstract interpretation framework of liquid type inference to automatically synthesize complex invariants from simple logical qualifiers, thereby almost completely automating the verification. We have implemented our approach in dsolve, which uses liquid types to verify ocaml programs. We present experiments that show that our type-based approach reduces the manual annotation required to verify complex properties like sortedness, balancedness, binary-search-ordering, and acyclicity by more than an order of magnitude.},
	urldate = {2015-01-30},
	booktitle = {Proceedings of the 2009 {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Kawaguchi, Ming and Rondon, Patrick and Jhala, Ranjit},
	year = {2009},
	keywords = {dependent types, hindley-milner, predicate abstraction, type inference},
	pages = {304--315},
	file = {kawaguchi_et_al_2009_type-based_data_structure_verification.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/JS4AX98K/kawaguchi_et_al_2009_type-based_data_structure_verification.pdf:application/pdf}
}

@incollection{reich_advances_2013,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Advances in {Lazy} {SmallCheck}},
	copyright = {©2013 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-41581-4 978-3-642-41582-1},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-41582-1_4},
	abstract = {A property-based testing library enables users to perform lightweight verification of software. This paper presents improvements to the Lazy SmallCheck property-based testing library. Users can now test properties that quantify over first-order functional values and nest universal and existential quantifiers in properties. When a property fails, Lazy SmallCheck now accurately expresses the partiality of the counterexample. These improvements are demonstrated through several practical examples.},
	language = {en},
	number = {8241},
	urldate = {2015-06-26},
	booktitle = {Implementation and {Application} of {Functional} {Languages}},
	publisher = {Springer Berlin Heidelberg},
	author = {Reich, Jason S. and Naylor, Matthew and Runciman, Colin},
	editor = {Hinze, Ralf},
	year = {2013},
	keywords = {automated testing, Existential quantification, Functional values, Information Systems Applications (incl. Internet), Lazy SmallCheck, Logics and Meanings of Programs, Mathematical Logic and Formal Languages, Programming Languages, Compilers, Interpreters, Programming Techniques, Search-based software engineering, Software Engineering},
	pages = {53--70},
	file = {reich_et_al_2013_advances_in_lazy_smallcheck.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/RNWX34XF/reich_et_al_2013_advances_in_lazy_smallcheck.pdf:application/pdf}
}

@article{csallner_dsd-crasher:_2008,
	title = {{DSD}-{Crasher}: {A} {Hybrid} {Analysis} {Tool} for {Bug} {Finding}},
	volume = {17},
	issn = {1049-331X},
	shorttitle = {{DSD}-{Crasher}},
	url = {http://doi.acm.org/10.1145/1348250.1348254},
	doi = {10.1145/1348250.1348254},
	abstract = {DSD-Crasher is a bug finding tool that follows a three-step approach to program analysis: D. Capture the program's intended execution behavior with dynamic invariant detection. The derived invariants exclude many unwanted values from the program's input domain. S. Statically analyze the program within the restricted input domain to explore many paths. D. Automatically generate test cases that focus on reproducing the predictions of the static analysis. Thereby confirmed results are feasible. This three-step approach yields benefits compared to past two-step combinations in the literature. In our evaluation with third-party applications, we demonstrate higher precision over tools that lack a dynamic step and higher efficiency over tools that lack a static step.},
	number = {2},
	urldate = {2015-01-24},
	journal = {ACM Trans. Softw. Eng. Methodol.},
	author = {Csallner, Christoph and Smaragdakis, Yannis and Xie, Tao},
	month = may,
	year = {2008},
	keywords = {automatic testing, bug finding, dynamic analysis, dynamic invariant detection, extended static checking, false positives, static analysis, test case generation, usability},
	pages = {8:1--8:37},
	file = {csallner_et_al_2008_dsd-crasher.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/VM436B8S/csallner_et_al_2008_dsd-crasher.pdf:application/pdf}
}

@article{jones_practical_2007,
	title = {Practical type inference for arbitrary-rank types},
	volume = {17},
	issn = {1469-7653},
	url = {http://journals.cambridge.org/article_S0956796806006034},
	doi = {10.1017/S0956796806006034},
	abstract = {Haskell's popularity has driven the need for ever more expressive type system features, most of which threaten the decidability and practicality of Damas-Milner type inference. One such feature is the ability to write functions with higher-rank types – that is, functions that take polymorphic functions as their arguments. Complete type inference is known to be undecidable for higher-rank (impredicative) type systems, but in practice programmers are more than willing to add type annotations to guide the type inference engine, and to document their code. However, the choice of just what annotations are required, and what changes are required in the type system and its inference algorithm, has been an ongoing topic of research. We take as our starting point a λ-calculus proposed by Odersky and Läufer. Their system supports arbitrary-rank polymorphism through the exploitation of type annotations on λ-bound arguments and arbitrary sub-terms. Though elegant, and more convenient than some other proposals, Odersky and Läufer's system requires many annotations. We show how to use local type inference (invented by Pierce and Turner) to greatly reduce the annotation burden, to the point where higher-rank types become eminently usable. Higher-rank types have a very modest impact on type inference. We substantiate this claim in a very concrete way, by presenting a complete type-inference engine, written in Haskell, for a traditional Damas-Milner type system, and then showing how to extend it for higher-rank types. We write the type-inference engine using a monadic framework: it turns out to be a particularly compelling example of monads in action. The paper is long, but is strongly tutorial in style. Although we use Haskell as our example source language, and our implementation language, much of our work is directly applicable to any ML-like functional language.},
	number = {01},
	urldate = {2015-08-04},
	journal = {Journal of Functional Programming},
	author = {Jones, Simon Peyton and Vytiniotis, Dimitrios and Weirich, Stephanie and Shields, Mark},
	year = {2007},
	pages = {1--82},
	file = {Cambridge Journals Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/P5KXDIFZ/displayAbstract.html:text/html;jones_et_al_2007_practical_type_inference_for_arbitrary-rank_types.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/R4R5F4H3/jones_et_al_2007_practical_type_inference_for_arbitrary-rank_types.pdf:application/pdf}
}

@inproceedings{fisher_next_2006,
	address = {New York, NY, USA},
	series = {{POPL} '06},
	title = {The {Next} 700 {Data} {Description} {Languages}},
	isbn = {1-59593-027-2},
	url = {http://doi.acm.org/10.1145/1111037.1111039},
	doi = {10.1145/1111037.1111039},
	abstract = {In the spirit of Landin, we present a calculus of dependent types to serve as the semantic foundation for a family of languages called data description languages. Such languages, which include pads, datascript, and packettypes, are designed to facilitate programming with ad hoc data, ie, data not in well-behaved relational or xml formats. In the calculus, each type describes the physical layout and semantic properties of a data source. In the semantics, we interpret types simultaneously as the in-memory representation of the data described and as parsers for the data source. The parsing functions are robust, automatically detecting and recording errors in the data stream without halting parsing. We show the parsers are type-correct, returning data whose type matches the simple-type interpretation of the specification. We also prove the parsers are "error-correct," accurately reporting the number of physical and semantic errors that occur in the returned data. We use the calculus to describe the features of various data description languages, and we discuss how we have used the calculus to improve PADS.},
	urldate = {2015-04-29},
	booktitle = {Conference {Record} of the 33rd {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Fisher, Kathleen and Mandelbaum, Yitzhak and Walker, David},
	year = {2006},
	keywords = {data description language, dependent types, domain-specific languages},
	pages = {2--15},
	file = {fisher_et_al_2006_the_next_700_data_description_languages.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/IPTDG93G/fisher_et_al_2006_the_next_700_data_description_languages.pdf:application/pdf}
}

@inproceedings{bowman_profile-guided_2015,
	address = {New York, NY, USA},
	series = {{PLDI} 2015},
	title = {Profile-guided {Meta}-programming},
	isbn = {978-1-4503-3468-6},
	url = {http://doi.acm.org/10.1145/2737924.2737990},
	doi = {10.1145/2737924.2737990},
	abstract = {Contemporary compiler systems such as GCC, .NET, and LLVM incorporate profile-guided optimizations (PGOs) on low-level intermediate code and basic blocks, with impressive results over purely static heuristics. Recent work shows that profile information is also useful for performing source-to-source optimizations via meta-programming. For example, using profiling information to inform decisions about data structures and algorithms can potentially lead to asymptotic improvements in performance. We present a design for profile-guided meta-programming in a general-purpose meta-programming system. Our design is parametric over the particular profiler and meta-programming system. We implement this design in two different meta-programming systems---the syntactic extensions systems of Chez Scheme and Racket---and provide several profile-guided meta-programs as usability case studies.},
	urldate = {2015-06-15},
	booktitle = {Proceedings of the 36th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Bowman, William J. and Miller, Swaha and St-Amour, Vincent and Dybvig, R. Kent},
	year = {2015},
	keywords = {meta-programming, optimization, PGO, profile-guided optimization, profiling},
	pages = {403--412},
	file = {bowman_et_al_2015_profile-guided_meta-programming.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/5WD2W5R2/bowman_et_al_2015_profile-guided_meta-programming.pdf:application/pdf}
}

@incollection{christiansen_sloth_2011,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Sloth – {A} {Tool} for {Checking} {Minimal}-{Strictness}},
	copyright = {©2011 Springer Berlin Heidelberg},
	isbn = {978-3-642-18377-5 978-3-642-18378-2},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-18378-2_14},
	abstract = {We present a light-weight tool called Sloth which assists programmers in identifying unnecessarily strict first order functions. Sloth reports counterexamples in form of a partial value, the corresponding result of the tested function and a recommended result. We present examples where the hints reported by Sloth can be used to improve a function with respect to memory behaviour, non-termination, and performance in the context of functional-logic programming. Furthermore we give an example-driven introduction into the basics of the implementation of Sloth. To improve the results in comparison to an existing approach we use additional constraints to assure that Sloth’s suggestions are implementable without employing parallelism.},
	language = {en},
	number = {6539},
	urldate = {2015-08-06},
	booktitle = {Practical {Aspects} of {Declarative} {Languages}},
	publisher = {Springer Berlin Heidelberg},
	author = {Christiansen, Jan},
	editor = {Rocha, Ricardo and Launchbury, John},
	year = {2011},
	keywords = {Artificial Intelligence (incl. Robotics), Curry, haskell, Logics and Meanings of Programs, Mathematical Logic and Formal Languages, minimal-strictness, non-strictness, Programming Languages, Compilers, Interpreters, Programming Techniques, sequentiality, Software Engineering, Testing},
	pages = {160--174},
	file = {christiansen_2011_sloth_–_a_tool_for_checking_minimal-strictness.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/JEQQ6247/christiansen_2011_sloth_–_a_tool_for_checking_minimal-strictness.pdf:application/pdf;Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/T885ATU7/10.html:text/html}
}

@incollection{xie_symstra:_2005,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Symstra: {A} {Framework} for {Generating} {Object}-{Oriented} {Unit} {Tests} {Using} {Symbolic} {Execution}},
	copyright = {©2005 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-25333-4 978-3-540-31980-1},
	shorttitle = {Symstra},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-31980-1_24},
	abstract = {Object-oriented unit tests consist of sequences of method invocations. Behavior of an invocation depends on the method’s arguments and the state of the receiver at the beginning of the invocation. Correspondingly, generating unit tests involves two tasks: generating method sequences that build relevant receiver-object states and generating relevant method arguments. This paper proposes Symstra, a framework that achieves both test generation tasks using symbolic execution of method sequences with symbolic arguments. The paper defines symbolic states of object-oriented programs and novel comparisons of states. Given a set of methods from the class under test and a bound on the length of sequences, Symstra systematically explores the object-state space of the class and prunes this exploration based on the state comparisons. Experimental results show that Symstra generates unit tests that achieve higher branch coverage faster than the existing test-generation techniques based on concrete method arguments.},
	language = {en},
	number = {3440},
	urldate = {2015-06-26},
	booktitle = {Tools and {Algorithms} for the {Construction} and {Analysis} of {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Xie, Tao and Marinov, Darko and Schulte, Wolfram and Notkin, David},
	editor = {Halbwachs, Nicolas and Zuck, Lenore D.},
	year = {2005},
	keywords = {Algorithm Analysis and Problem Complexity, Computer Communication Networks, Logics and Meanings of Programs, Software Engineering},
	pages = {365--381},
	file = {xie_et_al_2005_symstra.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/4JBZXBQN/xie_et_al_2005_symstra.pdf:application/pdf}
}

@inproceedings{dinges_targeted_2014,
	address = {New York, NY, USA},
	series = {{ASE} '14},
	title = {Targeted {Test} {Input} {Generation} {Using} {Symbolic}-concrete {Backward} {Execution}},
	isbn = {978-1-4503-3013-8},
	url = {http://doi.acm.org/10.1145/2642937.2642951},
	doi = {10.1145/2642937.2642951},
	abstract = {Knowing inputs that cover a specific branch or statement in a program is useful for debugging and regression testing. Symbolic backward execution (SBE) is a natural approach to find such targeted inputs. However, SBE struggles with complicated arithmetic, external method calls, and data-dependent loops that occur in many real-world programs. We propose symcretic execution, a novel combination of SBE and concrete forward execution that can efficiently find targeted inputs despite these challenges. An evaluation of our approach on a range of test cases shows that symcretic execution finds inputs in more cases than concolic testing tools while exploring fewer path segments. Integration of our approach will allow test generation tools to fill coverage gaps and static bug detectors to verify candidate bugs with concrete test cases.},
	urldate = {2015-01-22},
	booktitle = {Proceedings of the 29th {ACM}/{IEEE} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Dinges, Peter and Agha, Gul},
	year = {2014},
	keywords = {backward execution, concolic, goal-directed, symcretic},
	pages = {31--36},
	file = {dinges_agha_2014_targeted_test_input_generation_using_symbolic-concrete_backward_execution.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/SZ7CUCPX/dinges_agha_2014_targeted_test_input_generation_using_symbolic-concrete_backward_execution.pdf:application/pdf}
}

@inproceedings{gill_type-safe_2009,
	address = {New York, NY, USA},
	series = {Haskell '09},
	title = {Type-safe {Observable} {Sharing} in {Haskell}},
	isbn = {978-1-60558-508-6},
	url = {http://doi.acm.org/10.1145/1596638.1596653},
	doi = {10.1145/1596638.1596653},
	abstract = {Haskell is a great language for writing and supporting embedded Domain Specific Languages (DSLs). Some form of observable sharing is often a critical capability for allowing so-called deep DSLs to be compiled and processed. In this paper, we describe and explore uses of an IO function for reification which allows direct observation of sharing.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 2Nd {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Gill, Andy},
	year = {2009},
	keywords = {DSL compilation, observable sharing},
	pages = {117--128},
	file = {gill_2009_type-safe_observable_sharing_in_haskell.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/3BDW99A2/gill_2009_type-safe_observable_sharing_in_haskell.pdf:application/pdf}
}

@incollection{cadar_execution_2005,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Execution {Generated} {Test} {Cases}: {How} to {Make} {Systems} {Code} {Crash} {Itself}},
	copyright = {©2005 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-28195-5 978-3-540-31899-6},
	shorttitle = {Execution {Generated} {Test} {Cases}},
	url = {http://link.springer.com/chapter/10.1007/11537328_2},
	abstract = {This paper presents a technique that uses code to automatically generate its own test cases at run time by using a combination of symbolic and concrete (i.e regular) execution The input values to a program (or software component) provide the standard interface of any testing framework with the program it is testing and generating input values that will explore all the “interesting” behavior in the tested program remains an important open problem in software testing research. Our approach works by turning the problem on its head: we lazily generate from within the program itself the input values to the program (and values derived from input values) as needed. We applied the technique to real code and found numerous corner case errors ranging from simple memory overflows and infinite loops to subtle issues in the interpretation of language standards.},
	language = {en},
	number = {3639},
	urldate = {2015-01-23},
	booktitle = {Model {Checking} {Software}},
	publisher = {Springer Berlin Heidelberg},
	author = {Cadar, Cristian and Engler, Dawson},
	editor = {Godefroid, Patrice},
	year = {2005},
	keywords = {Logics and Meanings of Programs, Programming Languages, Compilers, Interpreters, Software Engineering},
	pages = {2--23},
	file = {cadar_engler_2005_execution_generated_test_cases.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/5SNNI9U6/cadar_engler_2005_execution_generated_test_cases.pdf:application/pdf}
}

@misc{_moving_????,
	title = {Moving {Fast} with {Software} {Verification}},
	url = {https://research.facebook.com/publications/422671501231772/moving-fast-with-software-verification/},
	abstract = {For organisations like Facebook, high quality software is important. However, the pace of change and increasing complexity of modern code makes it difficult to produce error free software. Available tools are often lacking in helping programmers develop more reliable and secure applications.},
	urldate = {2015-06-11},
	journal = {Research at Facebook},
	file = {Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/PTFFNRHA/moving-fast-with-software-verification.html:text/html}
}

@inproceedings{claessen_testing_2002,
	address = {New York, NY, USA},
	series = {Haskell '02},
	title = {Testing {Monadic} {Code} with {QuickCheck}},
	isbn = {1-58113-605-6},
	url = {http://doi.acm.org/10.1145/581690.581696},
	doi = {10.1145/581690.581696},
	abstract = {QuickCheck is a previously published random testing tool for Haskell programs. In this paper we show how to use it for testing monadic code, and in particular imperative code written using the ST monad. QuickCheck tests a program against a specification: we show that QuickCheck's specification language is sufficiently powerful to represent common forms of specifications: algebraic, model-based (both functional and relational), and pre-/post-conditional. Moreover, all these forms of specification can be used directly for testing. We define a new language of monadic properties, and make a link between program testing and the notion of observational equivalence.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 2002 {ACM} {SIGPLAN} {Workshop} on {Haskell}},
	publisher = {ACM},
	author = {Claessen, Koen and Hughes, John},
	year = {2002},
	pages = {65--77},
	file = {claessen_hughes_2002_testing_monadic_code_with_quickcheck.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/X8H78432/claessen_hughes_2002_testing_monadic_code_with_quickcheck.pdf:application/pdf}
}

@misc{_lambdacalc_????,
	title = {{LambdaCalc}},
	url = {http://dynamicaspects.org/blog/lambdacalc/},
	abstract = {In this introductory video, I describe an experimental interactive programming tool, LambdaCalc. We see how to program factorial in this system: we start with an executing copy of the identity func...},
	urldate = {2015-09-25},
	journal = {dynamic aspects},
	file = {Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/PW6899IF/lambdacalc.html:text/html}
}

@inproceedings{eisenberg_promoting_2014,
	address = {New York, NY, USA},
	series = {Haskell '14},
	title = {Promoting {Functions} to {Type} {Families} in {Haskell}},
	isbn = {978-1-4503-3041-1},
	url = {http://doi.acm.org/10.1145/2633357.2633361},
	doi = {10.1145/2633357.2633361},
	abstract = {Haskell, as implemented in the Glasgow Haskell Compiler (GHC), is enriched with many extensions that support type-level programming, such as promoted datatypes, kind polymorphism, and type families. Yet, the expressiveness of the type-level language remains limited. It is missing many features present at the term level, including case expressions, anonymous functions, partially-applied functions, and let expressions. In this paper, we present an algorithm - with a proof of correctness - to encode these term-level constructs at the type level. Our approach is automated and capable of promoting a wide array of functions to type families. We also highlight and discuss those term-level features that are not promotable. In so doing, we offer a critique on GHC's existing type system, showing what it is already capable of and where it may want improvement. We believe that delineating the mismatch between GHC's term level and its type level is a key step toward supporting dependently typed programming.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 2014 {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Eisenberg, Richard A. and Stolarek, Jan},
	year = {2014},
	keywords = {defunctionalization, haskell, type-level programming},
	pages = {95--106},
	file = {eisenberg_stolarek_2014_promoting_functions_to_type_families_in_haskell.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/IIG92NGN/eisenberg_stolarek_2014_promoting_functions_to_type_families_in_haskell.pdf:application/pdf}
}

@inproceedings{hickey_building_2014,
	address = {New York, NY, USA},
	series = {{ICFP} '14},
	title = {Building {Embedded} {Systems} with {Embedded} {DSLs}},
	isbn = {978-1-4503-2873-9},
	url = {http://doi.acm.org/10.1145/2628136.2628146},
	doi = {10.1145/2628136.2628146},
	abstract = {We report on our experiences in synthesizing a fully-featured autopilot from embedded domain-specific languages (EDSLs) hosted in Haskell. The autopilot is approximately 50k lines of C code generated from 10k lines of EDSL code and includes control laws, mode logic, encrypted communications system, and device drivers. The autopilot was built in less than two engineer years. This is the story of how EDSLs provided the productivity and safety gains to do large-scale low-level embedded programming and lessons we learned in doing so.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 19th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Hickey, Patrick C. and Pike, Lee and Elliott, Trevor and Bielman, James and Launchbury, John},
	year = {2014},
	keywords = {embedded domain specific languages, embedded systems},
	pages = {3--9},
	file = {hickey_et_al_2014_building_embedded_systems_with_embedded_dsls.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/XVQC9HAN/hickey_et_al_2014_building_embedded_systems_with_embedded_dsls.pdf:application/pdf}
}

@inproceedings{yakushev_generic_2009,
	address = {New York, NY, USA},
	series = {{ICFP} '09},
	title = {Generic {Programming} with {Fixed} {Points} for {Mutually} {Recursive} {Datatypes}},
	isbn = {978-1-60558-332-7},
	url = {http://doi.acm.org/10.1145/1596550.1596585},
	doi = {10.1145/1596550.1596585},
	abstract = {Many datatype-generic functions need access to the recursive positions in the structure of the datatype, and therefore adopt a fixed point view on datatypes. Examples include variants of fold that traverse the data following the recursive structure, or the Zipper data structure that enables navigation along the recursive positions. However, Hindley-Milner-inspired type systems with algebraic datatypes make it difficult to express fixed points for anything but regular datatypes. Many real-life examples such as abstract syntax trees are in fact systems of mutually recursive datatypes and therefore excluded. Using Haskell's GADTs and type families, we describe a technique that allows a fixed-point view for systems of mutually recursive datatypes. We demonstrate that our approach is widely applicable by giving several examples of generic functions for this view, most prominently the Zipper.},
	urldate = {2015-06-02},
	booktitle = {Proceedings of the 14th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Yakushev, Alexey Rodriguez and Holdermans, Stefan and Löh, Andres and Jeuring, Johan},
	year = {2009},
	keywords = {datatype-generic programming, fixed points, haskell, mutually recursive datatypes},
	pages = {233--244},
	file = {yakushev_et_al_2009_generic_programming_with_fixed_points_for_mutually_recursive_datatypes.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/RUQK5354/yakushev_et_al_2009_generic_programming_with_fixed_points_for_mutually_recursive_datatypes.pdf:application/pdf}
}

@inproceedings{samak_synthesizing_2015,
	address = {New York, NY, USA},
	series = {{PLDI} 2015},
	title = {Synthesizing {Racy} {Tests}},
	isbn = {978-1-4503-3468-6},
	url = {http://doi.acm.org/10.1145/2737924.2737998},
	doi = {10.1145/2737924.2737998},
	abstract = {Subtle concurrency errors in multithreaded libraries that arise because of incorrect or inadequate synchronization are often difficult to pinpoint precisely using only static techniques. On the other hand, the effectiveness of dynamic race detectors is critically dependent on multithreaded test suites whose execution can be used to identify and trigger races. Usually, such multithreaded tests need to invoke a specific combination of methods with objects involved in the invocations being shared appropriately to expose a race. Without a priori knowledge of the race, construction of such tests can be challenging. In this paper, we present a lightweight and scalable technique for synthesizing precisely these kinds of tests. Given a multithreaded library and a sequential test suite, we describe a fully automated analysis that examines sequential execution traces, and produces as its output a concurrent client program that drives shared objects via library method calls to states conducive for triggering a race. Experimental results on a variety of well-tested Java libraries yield 101 synthesized multithreaded tests in less than four minutes. Analyzing the execution of these tests using an off-the-shelf race detector reveals 187 harmful races, including several previously unreported ones. Our implementation, named NARADA, and the results of our experiments are available at http://www.csa.iisc.ernet.in/{\textasciitilde}sss/tools/narada.},
	urldate = {2015-06-15},
	booktitle = {Proceedings of the 36th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Samak, Malavika and Ramanathan, Murali Krishna and Jagannathan, Suresh},
	year = {2015},
	keywords = {concurrency, dynamic analysis, race detection},
	pages = {175--185},
	file = {samak_et_al_2015_synthesizing_racy_tests.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/RUC3K2J8/samak_et_al_2015_synthesizing_racy_tests.pdf:application/pdf}
}

@inproceedings{breitner_safe_2014,
	address = {New York, NY, USA},
	series = {{ICFP} '14},
	title = {Safe {Zero}-cost {Coercions} for {Haskell}},
	isbn = {978-1-4503-2873-9},
	url = {http://doi.acm.org/10.1145/2628136.2628141},
	doi = {10.1145/2628136.2628141},
	abstract = {Generative type abstractions -- present in Haskell, OCaml, and other languages -- are useful concepts to help prevent programmer errors. They serve to create new types that are distinct at compile time but share a run-time representation with some base type. We present a new mechanism that allows for zero-cost conversions between generative type abstractions and their representations, even when such types are deeply nested. We prove type safety in the presence of these conversions and have implemented our work in GHC.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 19th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Breitner, Joachim and Eisenberg, Richard A. and Peyton Jones, Simon and Weirich, Stephanie},
	year = {2014},
	keywords = {coercion, haskell, newtype deriving, type class},
	pages = {189--202},
	file = {breitner_et_al_2014_safe_zero-cost_coercions_for_haskell.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/K864MVDN/breitner_et_al_2014_safe_zero-cost_coercions_for_haskell.pdf:application/pdf}
}

@inproceedings{barthe_probabilistic_2014,
	address = {New York, NY, USA},
	series = {{POPL} '14},
	title = {Probabilistic {Relational} {Verification} for {Cryptographic} {Implementations}},
	isbn = {978-1-4503-2544-8},
	url = {http://doi.acm.org/10.1145/2535838.2535847},
	doi = {10.1145/2535838.2535847},
	abstract = {Relational program logics have been used for mechanizing formal proofs of various cryptographic constructions. With an eye towards scaling these successes towards end-to-end security proofs for implementations of distributed systems, we present RF*, a relational extension of F*, a general-purpose higher-order stateful programming language with a verification system based on refinement types. The distinguishing feature of F* is a relational Hoare logic for a higher-order, stateful, probabilistic language. Through careful language design, we adapt the F* typechecker to generate both classic and relational verification conditions, and to automatically discharge their proofs using an SMT solver. Thus, we are able to benefit from the existing features of F*, including its abstraction facilities for modular reasoning about program fragments. We evaluate RF* experimentally by programming a series of cryptographic constructions and protocols, and by verifying their security properties, ranging from information flow to unlinkability, integrity, and privacy. Moreover, we validate the design of RF* by formalizing in Coq a core probabilistic λ calculus and a relational refinement type system and proving the soundness of the latter against a denotational semantics of the probabilistic lambda λ calculus.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 41st {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Barthe, Gilles and Fournet, Cédric and Grégoire, Benjamin and Strub, Pierre-Yves and Swamy, Nikhil and Zanella-Béguelin, Santiago},
	year = {2014},
	keywords = {probabilistic programming, program logics},
	pages = {193--205},
	file = {barthe_et_al_2014_probabilistic_relational_verification_for_cryptographic_implementations.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/TXJAMIJW/barthe_et_al_2014_probabilistic_relational_verification_for_cryptographic_implementations.pdf:application/pdf}
}

@incollection{chang_stack_2012,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {From {Stack} {Traces} to {Lazy} {Rewriting} {Sequences}},
	copyright = {©2012 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-34406-0 978-3-642-34407-7},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-34407-7_7},
	abstract = {Reasoning about misbehaving lazy functional programs can be confusing, particularly for novice programmers. Unfortunately, the complicated nature of laziness also renders most debugging tools ineffective at clarifying this confusion. In this paper, we introduce a new lazy debugging tool for novice programmers, an algebraic stepper that presents computation as a sequence of parallel rewriting steps. Parallel program rewriting represents sharing accurately and enables debugging at the level of source syntax, minimizing the presentation of low-level details or the effects of distorting transformations that are typical for other lazy debuggers. Semantically, our rewriting system represents a compromise between Launchbury’s store-based semantics and an axiomatic description of lazy computation as sharing-via-parameters. Finally, we prove the correctness of our tool by showing that the stepper’s run-time machinery reconstructs the expected lazy rewriting sequence.},
	language = {en},
	number = {7257},
	urldate = {2015-05-29},
	booktitle = {Implementation and {Application} of {Functional} {Languages}},
	publisher = {Springer Berlin Heidelberg},
	author = {Chang, Stephen and Barzilay, Eli and Clements, John and Felleisen, Matthias},
	editor = {Gill, Andy and Hage, Jurriaan},
	year = {2012},
	keywords = {debugging, lazy lambda calculus, lazy programming, Logics and Meanings of Programs, Mathematical Logic and Formal Languages, Programming Languages, Compilers, Interpreters, Programming Techniques, Software Engineering},
	pages = {100--115},
	file = {chang_et_al_2012_from_stack_traces_to_lazy_rewriting_sequences.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/XEB7RDDG/chang_et_al_2012_from_stack_traces_to_lazy_rewriting_sequences.pdf:application/pdf}
}

@inproceedings{wright_practical_1994-1,
	address = {New York, NY, USA},
	series = {{LFP} '94},
	title = {A {Practical} {Soft} {Type} {System} for {Scheme}},
	isbn = {0-89791-643-3},
	url = {http://doi.acm.org/10.1145/182409.182485},
	doi = {10.1145/182409.182485},
	abstract = {Soft typing is a generalization of static type checking that accommodates both dynamic typing and static typing in one framework. A soft type checker infers types for identifiers and inserts explicit run-time checks to transform untypable programs into typable form. Soft Scheme is a practical soft type system for R4RS Scheme. The type checker uses a representation for types that is expressive, easy to interpret, and supports efficient type inference. Soft Scheme supports all of R4RS Scheme, including uncurried procedures of fixed and variable arity, assignment, and continuations.},
	urldate = {2015-05-27},
	booktitle = {Proceedings of the 1994 {ACM} {Conference} on {LISP} and {Functional} {Programming}},
	publisher = {ACM},
	author = {Wright, Andrew K. and Cartwright, Robert},
	year = {1994},
	pages = {250--262},
	file = {wright_cartwright_1994_a_practical_soft_type_system_for_scheme.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/MD6GEKG3/wright_cartwright_1994_a_practical_soft_type_system_for_scheme.pdf:application/pdf}
}

@article{parnas_criteria_1972,
	title = {On the {Criteria} to {Be} {Used} in {Decomposing} {Systems} into {Modules}},
	volume = {15},
	issn = {0001-0782},
	url = {http://doi.acm.org/10.1145/361598.361623},
	doi = {10.1145/361598.361623},
	abstract = {This paper discusses modularization as a mechanism for improving the flexibility and comprehensibility of a system while allowing the shortening of its development time. The effectiveness of a “modularization” is dependent upon the criteria used in dividing the system into modules. A system design problem is presented and both a conventional and unconventional decomposition are described. It is shown that the unconventional decompositions have distinct advantages for the goals outlined. The criteria used in arriving at the decompositions are discussed. The unconventional decomposition, if implemented with the conventional assumption that a module consists of one or more subroutines, will be less efficient in most cases. An alternative approach to implementation which does not have this effect is sketched.},
	number = {12},
	urldate = {2015-06-26},
	journal = {Commun. ACM},
	author = {Parnas, D. L.},
	month = dec,
	year = {1972},
	keywords = {KWIC index, Modularity, modules, software, Software design, Software Engineering},
	pages = {1053--1058},
	file = {parnas_1972_on_the_criteria_to_be_used_in_decomposing_systems_into_modules.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/D7QC72PC/parnas_1972_on_the_criteria_to_be_used_in_decomposing_systems_into_modules.pdf:application/pdf}
}

@inproceedings{chen_counter-factual_2014,
	address = {New York, NY, USA},
	series = {{POPL} '14},
	title = {Counter-factual {Typing} for {Debugging} {Type} {Errors}},
	isbn = {978-1-4503-2544-8},
	url = {http://doi.acm.org/10.1145/2535838.2535863},
	doi = {10.1145/2535838.2535863},
	abstract = {Changing a program in response to a type error plays an important part in modern software development. However, the generation of good type error messages remains a problem for highly expressive type systems. Existing approaches often suffer from a lack of precision in locating errors and proposing remedies. Specifically, they either fail to locate the source of the type error consistently, or they report too many potential error locations. Moreover, the change suggestions offered are often incorrect. This makes the debugging process tedious and ineffective. We present an approach to the problem of type debugging that is based on generating and filtering a comprehensive set of type-change suggestions. Specifically, we generate all (program-structure-preserving) type changes that can possibly fix the type error. These suggestions will be ranked and presented to the programmer in an iterative fashion. In some cases we also produce suggestions to change the program. In most situations, this strategy delivers the correct change suggestions quickly, and at the same time never misses any rare suggestions. The computation of the potentially huge set of type-change suggestions is efficient since it is based on a variational type inference algorithm that type checks a program with variations only once, efficiently reusing type information for shared parts. We have evaluated our method and compared it with previous approaches. Based on a large set of examples drawn from the literature, we have found that our method outperforms other approaches and provides a viable alternative.},
	urldate = {2015-05-26},
	booktitle = {Proceedings of the 41st {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Chen, Sheng and Erwig, Martin},
	year = {2014},
	keywords = {change suggestions, choice types, error localization, type-error debugging, type error messages, type inference},
	pages = {583--594},
	file = {chen_erwig_2014_counter-factual_typing_for_debugging_type_errors.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/UGFAND8T/chen_erwig_2014_counter-factual_typing_for_debugging_type_errors.pdf:application/pdf}
}

@inproceedings{flanagan_extended_2002,
	address = {New York, NY, USA},
	series = {{PLDI} '02},
	title = {Extended {Static} {Checking} for {Java}},
	isbn = {1-58113-463-0},
	url = {http://doi.acm.org/10.1145/512529.512558},
	doi = {10.1145/512529.512558},
	abstract = {Software development and maintenance are costly endeavors. The cost can be reduced if more software defects are detected earlier in the development cycle. This paper introduces the Extended Static Checker for Java (ESC/Java), an experimental compile-time program checker that finds common programming errors. The checker is powered by verification-condition generation and automatic theorem-proving techniques. It provides programmers with a simple annotation language with which programmer design decisions can be expressed formally. ESC/Java examines the annotated software and warns of inconsistencies between the design decisions recorded in the annotations and the actual code, and also warns of potential runtime errors in the code. This paper gives an overview of the checker architecture and annotation language and describes our experience applying the checker to tens of thousands of lines of Java programs.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the {ACM} {SIGPLAN} 2002 {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Flanagan, Cormac and Leino, K. Rustan M. and Lillibridge, Mark and Nelson, Greg and Saxe, James B. and Stata, Raymie},
	year = {2002},
	keywords = {checking, compile-time, program},
	pages = {234--245},
	file = {flanagan_et_al_2002_extended_static_checking_for_java.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/PPRQBKJE/flanagan_et_al_2002_extended_static_checking_for_java.pdf:application/pdf}
}

@inproceedings{gomard_partial_1990,
	address = {New York, NY, USA},
	series = {{LFP} '90},
	title = {Partial {Type} {Inference} for {Untyped} {Functional} {Programs}},
	isbn = {0-89791-368-X},
	url = {http://doi.acm.org/10.1145/91556.91672},
	doi = {10.1145/91556.91672},
	abstract = {This extended abstract describes a way of inferring as much type information as possible about programs written in an untyped programming language. We present an algorithm that underlines the untypable parts of a program and assigns types to the rest. The algorithm is derived in a very simple manner from the well-known algorithm W of Damas \& Milner [Damas and Milner 1982].
Our algorithm provides us with an easy solution to the problem of doing binding time analysis of the untyped higher order lambda calculus, and thereby of the wide range of programming languages based upon the lambda calculus. The techniques can also be used to eliminate superfluous runtime type checking in untyped functional languages, to produce better error messages from type analyzers for strongly typed languages, and to analyze feasibility of arity raising.},
	urldate = {2015-05-27},
	booktitle = {Proceedings of the 1990 {ACM} {Conference} on {LISP} and {Functional} {Programming}},
	publisher = {ACM},
	author = {Gomard, Carsten K.},
	year = {1990},
	pages = {282--287},
	file = {gomard_1990_partial_type_inference_for_untyped_functional_programs.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/W3EIII3E/gomard_1990_partial_type_inference_for_untyped_functional_programs.pdf:application/pdf}
}

@article{calcagno_compositional_2011,
	title = {Compositional {Shape} {Analysis} by {Means} of {Bi}-{Abduction}},
	volume = {58},
	issn = {0004-5411},
	url = {http://doi.acm.org/10.1145/2049697.2049700},
	doi = {10.1145/2049697.2049700},
	abstract = {The accurate and efficient treatment of mutable data structures is one of the outstanding problem areas in automatic program verification and analysis. Shape analysis is a form of program analysis that attempts to infer descriptions of the data structures in a program, and to prove that these structures are not misused or corrupted. It is one of the more challenging and expensive forms of program analysis, due to the complexity of aliasing and the need to look arbitrarily deeply into the program heap. This article describes a method of boosting shape analyses by defining a compositional method, where each procedure is analyzed independently of its callers. The analysis algorithm uses a restricted fragment of separation logic, and assigns a collection of Hoare triples to each procedure; the triples provide an over-approximation of data structure usage. Our method brings the usual benefits of compositionality---increased potential to scale, ability to deal with incomplete programs, graceful way to deal with imprecision---to shape analysis, for the first time. The analysis rests on a generalized form of abduction (inference of explanatory hypotheses), which we call bi-abduction. Bi-abduction displays abduction as a kind of inverse to the frame problem: it jointly infers anti-frames (missing portions of state) and frames (portions of state not touched by an operation), and is the basis of a new analysis algorithm. We have implemented our analysis and we report case studies on smaller programs to evaluate the quality of discovered specifications, and larger code bases (e.g., sendmail, an imap server, a Linux distribution) to illustrate the level of automation and scalability that we obtain from our compositional method. This article makes number of specific technical contributions on proof procedures and analysis algorithms, but in a sense its more important contribution is holistic: the explanation and demonstration of how a massive increase in automation is possible using abductive inference.},
	number = {6},
	urldate = {2015-06-11},
	journal = {J. ACM},
	author = {Calcagno, Cristiano and Distefano, Dino and O'Hearn, Peter W. and Yang, Hongseok},
	month = dec,
	year = {2011},
	keywords = {abstract interpretation, compositionality, program proving, separation logic, static analysis},
	pages = {26:1--26:66},
	file = {calcagno_et_al_2011_compositional_shape_analysis_by_means_of_bi-abduction.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/DV9J29SG/calcagno_et_al_2011_compositional_shape_analysis_by_means_of_bi-abduction.pdf:application/pdf}
}

@article{korel_dynamic_1988,
	title = {Dynamic program slicing},
	volume = {29},
	issn = {0020-0190},
	url = {http://www.sciencedirect.com/science/article/pii/0020019088900543},
	doi = {10.1016/0020-0190(88)90054-3},
	abstract = {A dynamic program slice is an executable subset of the original program that produces the same computations on a subset of selected variables and inputs. It differs from the static slice (Weiser, 1982, 1984) in that it is entirely defined on the basis of a computation. The two main advantages are the following: Arrays and dynamic data structures can be handled more precisely and the size of slice can be significantly reduced, leading to a finer localization of the fault. The approach is being investigated as a possible extension of the debugging capabilities of STAD, a recently developed System for Testing and Debugging (Korel and Laski, 1987; Laski, 1987).},
	number = {3},
	urldate = {2015-07-24},
	journal = {Information Processing Letters},
	author = {Korel, Bogdan and Laski, Janusz},
	month = oct,
	year = {1988},
	keywords = {control dependence, data dependence, debugging, dynamic slice, slicing, trajectory},
	pages = {155--163},
	file = {korel_laski_1988_dynamic_program_slicing.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/MXE37AJZ/korel_laski_1988_dynamic_program_slicing.pdf:application/pdf;ScienceDirect Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/IDFD5763/0020019088900543.html:text/html}
}

@incollection{chen_guided_2014,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Guided {Type} {Debugging}},
	copyright = {©2014 Springer International Publishing Switzerland},
	isbn = {978-3-319-07150-3 978-3-319-07151-0},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-07151-0_3},
	abstract = {We present guided type debugging as a new approach to quickly and reliably remove type errors from functional programs. The method works by generating type-change suggestions that satisfy type specifications that are elicited from programmers during the debugging process. A key innovation is the incorporation of target types into the type error debugging process. Whereas previous approaches have aimed exclusively at the removal of type errors and disregarded the resulting types, guided type debugging exploits user feedback about result types to achieve better type-change suggestions. Our method can also identify and remove errors in type annotations, which has been a problem for previous approaches. To efficiently implement our approach, we systematically generate all potential type changes and arrange them in a lattice structure that can be efficiently traversed when guided by target types that are provided by programmers.},
	language = {en},
	number = {8475},
	urldate = {2015-10-27},
	booktitle = {Functional and {Logic} {Programming}},
	publisher = {Springer International Publishing},
	author = {Chen, Sheng and Erwig, Martin},
	editor = {Codish, Michael and Sumii, Eijiro},
	month = jun,
	year = {2014},
	note = {DOI: 10.1007/978-3-319-07151-0\_3},
	keywords = {Artificial Intelligence (incl. Robotics), change suggestions, choice types, error localization, Logics and Meanings of Programs, Mathematical Logic and Formal Languages, Programming Languages, Compilers, Interpreters, Programming Techniques, Software Engineering, type debugging, type error messages, type inference},
	pages = {35--51},
	file = {chen_erwig_2014_guided_type_debugging.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/T3TH9JQC/chen_erwig_2014_guided_type_debugging.pdf:application/pdf;Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/6VM4MKSE/10.html:text/html}
}

@article{bahr_parametric_2012,
	title = {Parametric {Compositional} {Data} {Types}},
	volume = {76},
	issn = {2075-2180},
	url = {http://arxiv.org/abs/1202.2917},
	doi = {10.4204/EPTCS.76.3},
	abstract = {In previous work we have illustrated the benefits that compositional data types (CDTs) offer for implementing languages and in general for dealing with abstract syntax trees (ASTs). Based on Swierstra's data types {\textbackslash}'a la carte, CDTs are implemented as a Haskell library that enables the definition of recursive data types and functions on them in a modular and extendable fashion. Although CDTs provide a powerful tool for analysing and manipulating ASTs, they lack a convenient representation of variable binders. In this paper we remedy this deficiency by combining the framework of CDTs with Chlipala's parametric higher-order abstract syntax (PHOAS). We show how a generalisation from functors to difunctors enables us to capture PHOAS while still maintaining the features of the original implementation of CDTs, in particular its modularity. Unlike previous approaches, we avoid so-called exotic terms without resorting to abstract types: this is crucial when we want to perform transformations on CDTs that inspect the recursively computed CDTs, e.g. constant folding.},
	urldate = {2015-08-25},
	journal = {Electronic Proceedings in Theoretical Computer Science},
	author = {Bahr, Patrick and Hvitved, Tom},
	month = feb,
	year = {2012},
	note = {arXiv: 1202.2917},
	keywords = {Computer Science - Programming Languages},
	pages = {3--24},
	file = {arXiv.org Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/XP53XHIT/1202.html:text/html;bahr_hvitved_2012_parametric_compositional_data_types.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/6KA8MWEE/bahr_hvitved_2012_parametric_compositional_data_types.pdf:application/pdf}
}

@incollection{jeuring_ask-elle:_2012,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Ask-{Elle}: {A} {Haskell} {Tutor}},
	copyright = {©2012 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-33262-3 978-3-642-33263-0},
	shorttitle = {Ask-{Elle}},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-33263-0_42},
	abstract = {In this demonstration we will introduce Ask-Elle, a Haskell tutor. Ask-Elle supports the incremental development of Haskell programs. It can give hints on how to proceed with solving a programming exercise, and feedback on incomplete student programs. We will show Ask-Elle in action, and discuss how a teacher can configure its behaviour.},
	language = {en},
	number = {7563},
	urldate = {2015-08-13},
	booktitle = {21st {Century} {Learning} for 21st {Century} {Skills}},
	publisher = {Springer Berlin Heidelberg},
	author = {Jeuring, Johan and Gerdes, Alex and Heeren, Bastiaan},
	editor = {Ravenscroft, Andrew and Lindstaedt, Stefanie and Kloos, Carlos Delgado and Hernández-Leo, Davinia},
	year = {2012},
	keywords = {Artificial Intelligence (incl. Robotics), Computer Communication Networks, Data Mining and Knowledge Discovery, Information Storage and Retrieval, Information Systems Applications (incl. Internet), User Interfaces and Human Computer Interaction},
	pages = {453--458},
	file = {jeuring_et_al_2012_ask-elle.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/CWNK66C8/jeuring_et_al_2012_ask-elle.pdf:application/pdf;Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/82XK9SB4/10.html:text/html}
}

@inproceedings{perelman_test-driven_2014,
	address = {New York, NY, USA},
	series = {{PLDI} '14},
	title = {Test-driven {Synthesis}},
	isbn = {978-1-4503-2784-8},
	url = {http://doi.acm.org/10.1145/2594291.2594297},
	doi = {10.1145/2594291.2594297},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 35th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Perelman, Daniel and Gulwani, Sumit and Grossman, Dan and Provost, Peter},
	year = {2014},
	keywords = {end-user programming, Program Synthesis, test driven development},
	pages = {408--418},
	file = {perelman_et_al_2014_test-driven_synthesis.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/HXGKKKZN/perelman_et_al_2014_test-driven_synthesis.pdf:application/pdf}
}

@inproceedings{petricek_coeffects:_2014,
	address = {New York, NY, USA},
	series = {{ICFP} '14},
	title = {Coeffects: {A} {Calculus} of {Context}-dependent {Computation}},
	isbn = {978-1-4503-2873-9},
	shorttitle = {Coeffects},
	url = {http://doi.acm.org/10.1145/2628136.2628160},
	doi = {10.1145/2628136.2628160},
	abstract = {The notion of context in functional languages no longer refers just to variables in scope. Context can capture additional properties of variables (usage patterns in linear logics; caching requirements in dataflow languages) as well as additional resources or properties of the execution environment (rebindable resources; platform version in a cross-platform application). The recently introduced notion of coeffects captures the latter, whole-context properties, but it failed to capture fine-grained per-variable properties. We remedy this by developing a generalized coeffect system with annotations indexed by a coeffect shape. By instantiating a concrete shape, our system captures previously studied flat (whole-context) coeffects, but also structural (per-variable) coeffects, making coeffect analyses more useful. We show that the structural system enjoys desirable syntactic properties and we give a categorical semantics using extended notions of indexed comonad. The examples presented in this paper are based on analysis of established language features (liveness, linear logics, dataflow, dynamic scoping) and we argue that such context-aware properties will also be useful for future development of languages for increasingly heterogeneous and distributed platforms.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 19th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Petricek, Tomas and Orchard, Dominic and Mycroft, Alan},
	year = {2014},
	keywords = {coeffects, context, indexed comonads, types},
	pages = {123--135},
	file = {petricek_et_al_2014_coeffects.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/Q6ENRE37/petricek_et_al_2014_coeffects.pdf:application/pdf}
}

@inproceedings{crestani_experience_2010,
	address = {New York, NY, USA},
	series = {{ICFP} '10},
	title = {Experience {Report}: {Growing} {Programming} {Languages} for {Beginning} {Students}},
	isbn = {978-1-60558-794-3},
	shorttitle = {Experience {Report}},
	url = {http://doi.acm.org/10.1145/1863543.1863576},
	doi = {10.1145/1863543.1863576},
	abstract = {A student learning how to program learns best when the programming language and programming environment cater to her specific needs. These needs are different from the requirements of a professional programmer. Consequently, the design of teaching languages poses challenges different from the design of professional languages. Using a functional language by itself gives advantages over more popular, professional languages, but fully exploiting these advantages requires careful adaptation to the needs of the students' as-is, these languages do not support the students nearly as well as they could. This paper describes our experience adopting the didactic approach of How to Design Programs, focussing on the design process for our own set of teaching languages. We have observed students as they try to program as part of our introductory course, and used these observations to significantly improve the design of these languages. This paper describes the changes we have made, and the journey we took to get there.},
	urldate = {2015-09-24},
	booktitle = {Proceedings of the 15th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Crestani, Marcus and Sperber, Michael},
	year = {2010},
	keywords = {introductory, Programming},
	pages = {229--234},
	file = {crestani_sperber_2010_experience_report.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/U8B7JSDB/crestani_sperber_2010_experience_report.pdf:application/pdf}
}

@inproceedings{diatchki_improving_2015,
	address = {New York, NY, USA},
	series = {Haskell 2015},
	title = {Improving {Haskell} {Types} with {SMT}},
	isbn = {978-1-4503-3808-0},
	url = {http://doi.acm.org/10.1145/2804302.2804307},
	doi = {10.1145/2804302.2804307},
	abstract = {We present a technique for integrating GHC's type-checker with an SMT solver. The technique was developed to add support for reasoning about type-level functions on natural numbers, and so our implementation uses the theory of linear arithmetic. However, the approach is not limited to this theory, and makes it possible to experiment with other external decision procedures, such as reasoning about type-level booleans, bit-vectors, or any other theory supported by SMT solvers.},
	urldate = {2015-09-03},
	booktitle = {Proceedings of the 8th {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Diatchki, Iavor S.},
	year = {2015},
	keywords = {constraint solving, SMT, type systems},
	pages = {1--10},
	file = {diatchki_2015_improving_haskell_types_with_smt.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/A36FKUUI/diatchki_2015_improving_haskell_types_with_smt.pdf:application/pdf}
}

@inproceedings{nienaltowski_compiler_2008,
	address = {New York, NY, USA},
	series = {{SIGCSE} '08},
	title = {Compiler {Error} {Messages}: {What} {Can} {Help} {Novices}?},
	isbn = {978-1-59593-799-5},
	shorttitle = {Compiler {Error} {Messages}},
	url = {http://doi.acm.org/10.1145/1352135.1352192},
	doi = {10.1145/1352135.1352192},
	abstract = {Novices find it difficult to understand and use compiler error messages. It is useful to refine this observation and study the effect of different message styles on how well and quickly students identify errors in programs. For example, does an increased level of detail simplify the understanding of errors and their correction? We analyzed messages produced by a number of compilers for five programming languages, and grouped them into three style categories from their level of detail and presentation format, and correlated the level of experience and error type with performance and speed of response. The study involved two groups of students taking an introductory programming course at two different institutions; they used messages in these three styles to debug erroneous code. The results indicate that more detailed messages do not necessarily simplify the understanding of errors but that it matters more where information is placed and how it is structured.},
	urldate = {2015-09-24},
	booktitle = {Proceedings of the 39th {SIGCSE} {Technical} {Symposium} on {Computer} {Science} {Education}},
	publisher = {ACM},
	author = {Nienaltowski, Marie-Hélène and Pedroni, Michela and Meyer, Bertrand},
	year = {2008},
	keywords = {compiler error messages, novice programmers},
	pages = {168--172},
	file = {nienaltowski_et_al_2008_compiler_error_messages.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/KTFF3NS8/nienaltowski_et_al_2008_compiler_error_messages.pdf:application/pdf}
}

@incollection{claessen_testing_2003,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Testing and {Tracing} {Lazy} {Functional} {Programs} {Using} {QuickCheck} and {Hat}},
	copyright = {©2003 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-40132-2 978-3-540-44833-4},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-44833-4_3},
	language = {en},
	number = {2638},
	urldate = {2015-09-25},
	booktitle = {Advanced {Functional} {Programming}},
	publisher = {Springer Berlin Heidelberg},
	author = {Claessen, Koen and Runciman, Colin and Chitil, Olaf and Hughes, John and Wallace, Malcolm},
	editor = {Jeuring, Johan and Jones, Simon L. Peyton},
	year = {2003},
	keywords = {Logics and Meanings of Programs, Programming Languages, Compilers, Interpreters, Programming Techniques, Software Engineering},
	pages = {59--99},
	file = {claessen_et_al_2003_testing_and_tracing_lazy_functional_programs_using_quickcheck_and_hat.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/6ZPW5RXB/claessen_et_al_2003_testing_and_tracing_lazy_functional_programs_using_quickcheck_and_hat.pdf:application/pdf;claessen_et_al_2003_testing_and_tracing_lazy_functional_programs_using_quickcheck_and_hat.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/QBUTKGPF/claessen_et_al_2003_testing_and_tracing_lazy_functional_programs_using_quickcheck_and_hat.pdf:application/pdf;Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/JPAKV5ZU/10.html:text/html}
}

@inproceedings{karachalias_gadts_2015,
	address = {New York, NY, USA},
	series = {{ICFP} 2015},
	title = {{GADTs} {Meet} {Their} {Match}: {Pattern}-matching {Warnings} {That} {Account} for {GADTs}, {Guards}, and {Laziness}},
	isbn = {978-1-4503-3669-7},
	shorttitle = {{GADTs} {Meet} {Their} {Match}},
	url = {http://doi.acm.org/10.1145/2784731.2784748},
	doi = {10.1145/2784731.2784748},
	abstract = {For ML and Haskell, accurate warnings when a function definition has redundant or missing patterns are mission critical. But today's compilers generate bogus warnings when the programmer uses guards (even simple ones), GADTs, pattern guards, or view patterns. We give the first algorithm that handles all these cases in a single, uniform framework, together with an implementation in GHC, and evidence of its utility in practice.},
	urldate = {2015-09-03},
	booktitle = {Proceedings of the 20th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Karachalias, Georgios and Schrijvers, Tom and Vytiniotis, Dimitrios and Jones, Simon Peyton},
	year = {2015},
	keywords = {generalized algebraic data types, haskell, OutsideIn(X), pattern matching},
	pages = {424--436},
	file = {karachalias_et_al_2015_gadts_meet_their_match.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/AMSATNC5/karachalias_et_al_2015_gadts_meet_their_match.pdf:application/pdf}
}

@incollection{claessen_generating_2014,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Generating {Constrained} {Random} {Data} with {Uniform} {Distribution}},
	copyright = {©2014 Springer International Publishing Switzerland},
	isbn = {978-3-319-07150-3 978-3-319-07151-0},
	url = {http://link.springer.com/chapter/10.1007/978-3-319-07151-0_2},
	abstract = {We present a technique for automatically deriving test data generators from a predicate expressed as a Boolean function. The distribution of these generators is uniform over values of a given size. To make the generation efficient we rely on laziness of the predicate, allowing us to prune the space of values quickly. In contrast, implementing test data generators by hand is labour intensive and error prone. Moreover, handwritten generators often have an unpredictable distribution of values, risking that some values are arbitrarily underrepresented. We also present a variation of the technique where the distribution is skewed in a limited and predictable way, potentially increasing the performance. Experimental evaluation of the techniques shows that the uniform derived generators are much easier to define than hand-written ones, and their performance, while lower, is adequate for some realistic applications.},
	language = {en},
	number = {8475},
	urldate = {2015-06-26},
	booktitle = {Functional and {Logic} {Programming}},
	publisher = {Springer International Publishing},
	author = {Claessen, Koen and Duregård, Jonas and Pałka, Michał H.},
	editor = {Codish, Michael and Sumii, Eijiro},
	month = jun,
	year = {2014},
	keywords = {Artificial Intelligence (incl. Robotics), Logics and Meanings of Programs, Mathematical Logic and Formal Languages, Programming Languages, Compilers, Interpreters, Programming Techniques, Software Engineering},
	pages = {18--34},
	file = {claessen_et_al_2014_generating_constrained_random_data_with_uniform_distribution.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/AKKI939H/claessen_et_al_2014_generating_constrained_random_data_with_uniform_distribution.pdf:application/pdf}
}

@inproceedings{bahr_compositional_2011,
	address = {New York, NY, USA},
	series = {{WGP} '11},
	title = {Compositional {Data} {Types}},
	isbn = {978-1-4503-0861-8},
	url = {http://doi.acm.org/10.1145/2036918.2036930},
	doi = {10.1145/2036918.2036930},
	abstract = {Building on Wouter Swierstra's Data types à la carte, we present a comprehensive Haskell library of compositional data types suitable for practical applications. In this framework, data types and functions on them can be defined in a modular fashion. We extend the existing work by implementing a wide array of recursion schemes including monadic computations. Above all, we generalise recursive data types to contexts, which allow us to characterise a special yet frequent kind of catamorphisms. The thus established notion of term homomorphisms allows for flexible reuse and enables short-cut fusion style deforestation which yields considerable speedups. We demonstrate our framework in the setting of compiler construction, and moreover, we compare compositional data types with generic programming techniques and show that both are comparable in run-time performance and expressivity while our approach allows for stricter types. We substantiate this conclusion by lifting compositional data types to recursive data types and generalised algebraic data types. Lastly, we compare the run-time performance of our techniques with traditional implementations over algebraic data types. The results are surprisingly good.},
	urldate = {2015-06-02},
	booktitle = {Proceedings of the {Seventh} {ACM} {SIGPLAN} {Workshop} on {Generic} {Programming}},
	publisher = {ACM},
	author = {Bahr, Patrick and Hvitved, Tom},
	year = {2011},
	keywords = {algebraic programming, deforestation, mutual recursion, reusability},
	pages = {83--94},
	file = {bahr_hvitved_2011_compositional_data_types.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/BJVX5K78/bahr_hvitved_2011_compositional_data_types.pdf:application/pdf;bahr_hvitved_2011_compositional_data_types.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/NSRKEIAB/bahr_hvitved_2011_compositional_data_types.pdf:application/pdf}
}

@inproceedings{magalhaes_generic_2010,
	address = {New York, NY, USA},
	series = {Haskell '10},
	title = {A {Generic} {Deriving} {Mechanism} for {Haskell}},
	isbn = {978-1-4503-0252-4},
	url = {http://doi.acm.org/10.1145/1863523.1863529},
	doi = {10.1145/1863523.1863529},
	abstract = {Haskell's deriving mechanism supports the automatic generation of instances for a number of functions. The Haskell 98 Report only specifies how to generate instances for the Eq, Ord, Enum, Bounded, Show, and Read classes. The description of how to generate instances is largely informal. The generation of instances imposes restrictions on the shape of datatypes, depending on the particular class to derive. As a consequence, the portability of instances across different compilers is not guaranteed. We propose a new approach to Haskell's deriving mechanism, which allows users to specify how to derive arbitrary class instances using standard datatype-generic programming techniques. Generic functions, including the methods from six standard Haskell 98 derivable classes, can be specified entirely within Haskell 98 plus multi-parameter type classes, making them lightweight and portable. We can also express Functor, Typeable, and many other derivable classes with our technique. We implemented our deriving mechanism together with many new derivable classes in the Utrecht Haskell Compiler.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the {Third} {ACM} {Haskell} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Magalhães, José Pedro and Dijkstra, Atze and Jeuring, Johan and Löh, Andres},
	year = {2010},
	keywords = {datatype-generic programming, haskell, type classes},
	pages = {37--48},
	file = {magalhães_et_al_2010_a_generic_deriving_mechanism_for_haskell.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/XPKN7HH7/magalhães_et_al_2010_a_generic_deriving_mechanism_for_haskell.pdf:application/pdf}
}

@article{ruiz_tilc:_2009,
	title = {{TILC}: {The} {Interactive} {Lambda}-{Calculus} {Tracer}},
	volume = {248},
	issn = {15710661},
	shorttitle = {{TILC}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1571066109002904},
	doi = {10.1016/j.entcs.2009.07.067},
	language = {en},
	urldate = {2015-08-05},
	journal = {Electronic Notes in Theoretical Computer Science},
	author = {Ruiz, David and Villaret, Mateu},
	month = aug,
	year = {2009},
	pages = {173--183},
	file = {ruiz_villaret_2009_tilc.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/IM73QURT/ruiz_villaret_2009_tilc.pdf:application/pdf}
}

@inproceedings{wadler_how_1989,
	address = {New York, NY, USA},
	series = {{POPL} '89},
	title = {How to {Make} {Ad}-hoc {Polymorphism} {Less} {Ad} {Hoc}},
	isbn = {0-89791-294-2},
	url = {http://doi.acm.org/10.1145/75277.75283},
	doi = {10.1145/75277.75283},
	abstract = {This paper presents type classes, a new approach to ad-hoc polymorphism. Type classes permit overloading of arithmetic operators such as multiplication, and generalise the “eqtype variables” of Standard ML. Type classes extend the Hindley/Milner polymorphic type system, and provide a new approach to issues that arise in object-oriented programming, bounded type quantification, and abstract data types. This paper provides an informal introduction to type classes, and defines them formally by means of type inference rules.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 16th {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Wadler, P. and Blott, S.},
	year = {1989},
	pages = {60--76},
	file = {wadler_blott_1989_how_to_make_ad-hoc_polymorphism_less_ad_hoc.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/KD24IJPA/wadler_blott_1989_how_to_make_ad-hoc_polymorphism_less_ad_hoc.pdf:application/pdf}
}

@incollection{cooper_probabilistic_2014,
	title = {A {Probabilistic} {Rich} {Type} {Theory} for {Semantic} {Interpretation}},
	volume = {s. 72},
	isbn = {978-1-937284-74-9},
	url = {http://www.aclweb.org/anthology/W/W14/W14-1409.pdf},
	abstract = {We propose a probabilistic type theory in which a situation s is judged to be of a type T with probability p. In addition to basic and functional types it includes, inter alia, record types and a notion of typing based on them. The type system is intensional in that types of situations are not reduced to sets of situations. We specify the fragment of a compositional semantics in which truth conditions are replaced by probability conditions. The type system is the interface between classifying situations in perception and computing the semantic interpretations of phrases in natural language.},
	urldate = {2015-05-13},
	booktitle = {Proceedings of the {Workshop} on {Type} {Theory} and {Natural} {Language} {Semantics} ({TTNLS}),14th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}, {Gothenburg}, {Sweden} 2014. ed. by {Cooper}, {Robin}, {Simon} {Dobnik}, {Shalom} {Lappin} and {Staffan} {Larsson}},
	publisher = {Proceedings of the Workshop on Type Theory and Natural Language Semantics (TTNLS),14th Conference of the European Chapter of the Association for Computational Linguistics, Gothenburg, Sweden 2014. ed. by Cooper, Robin, Simon Dobnik, Shalom Lappin and Staffan Larsson},
	author = {Cooper, Robin and Dobnik, Simon and Lappin, Shalom and Larsson, Staffan},
	year = {2014},
	file = {cooper_et_al_2014_a_probabilistic_rich_type_theory_for_semantic_interpretation.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/JCNWKSFK/cooper_et_al_2014_a_probabilistic_rich_type_theory_for_semantic_interpretation.pdf:application/pdf}
}

@article{joosten_teaching_1993,
	title = {Teaching functional programming to first-year students},
	volume = {3},
	issn = {1469-7653},
	url = {http://journals.cambridge.org/article_S0956796800000599},
	doi = {10.1017/S0956796800000599},
	abstract = {In the period 1986–1991, experiments have been carried out with an introductory course in computer programming, based on functional programming. Due to thorough educational design and evaluation, a successful course has been developed. This has led to a revision of the computer programming education in the first year of the computer science curriculum at the University of Twente.This article describes the approach, the aim of the computer programming course, the outline and subject matter of the course, and the evaluation. Educational research has been done to assess the quality of the course.},
	number = {01},
	urldate = {2015-03-04},
	journal = {Journal of Functional Programming},
	author = {Joosten, Stef and Van Den Berg, Klaas and Van Der Hoeven, Gerrit},
	month = jan,
	year = {1993},
	keywords = {education, functional programming},
	pages = {49--65},
	file = {joosten_et_al_1993_teaching_functional_programming_to_first-year_students.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/KESEGIV9/joosten_et_al_1993_teaching_functional_programming_to_first-year_students.pdf:application/pdf}
}

@inproceedings{kuncak_relational_2005,
	address = {New York, NY, USA},
	series = {{ESEC}/{FSE}-13},
	title = {Relational {Analysis} of {Algebraic} {Datatypes}},
	isbn = {1-59593-014-0},
	url = {http://doi.acm.org/10.1145/1081706.1081740},
	doi = {10.1145/1081706.1081740},
	abstract = {We present a technique that enables the use of finite model finding to check the satisfiability of certain formulas whose intended models are infinite. Such formulas arise when using the language of sets and relations to reason about structured values such as algebraic datatypes. The key idea of our technique is to identify a natural syntactic class of formulas in relational logic for which reasoning about infinite structures can be reduced to reasoning about finite structures. As a result, when a formula belongs to this class, we can use existing finite model finding tools to check whether the formula holds in the desired infinite model.},
	urldate = {2014-09-09},
	booktitle = {Proceedings of the 10th {European} {Software} {Engineering} {Conference} {Held} {Jointly} with 13th {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Kuncak, Viktor and Jackson, Daniel},
	year = {2005},
	keywords = {algebraic datatypes, constraint solving, model checking, model finding, transitive closure logic},
	pages = {207--216},
	file = {kuncak_jackson_2005_relational_analysis_of_algebraic_datatypes.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/7QN33F6J/kuncak_jackson_2005_relational_analysis_of_algebraic_datatypes.pdf:application/pdf}
}

@inproceedings{ball_symptom_2003,
	address = {New York, NY, USA},
	series = {{POPL} '03},
	title = {From {Symptom} to {Cause}: {Localizing} {Errors} in {Counterexample} {Traces}},
	isbn = {1-58113-628-5},
	shorttitle = {From {Symptom} to {Cause}},
	url = {http://doi.acm.org/10.1145/604131.604140},
	doi = {10.1145/604131.604140},
	abstract = {There is significant room for improving users' experiences with model checking tools. An error trace produced by a model checker can be lengthy and is indicative of a symptom of an error. As a result, users can spend considerable time examining an error trace in order to understand the cause of the error. Moreover, even state-of-the-art model checkers provide an experience akin to that provided by parsers before syntactic error recovery was invented: they report a single error trace per run. The user has to fix the error and run the model checker again to find more error traces.We present an algorithm that exploits the existence of correct traces in order to localize the error cause in an error trace, report a single error trace per error cause, and generate multiple error traces having independent causes. We have implemented this algorithm in the context of slam, a software model checker that automatically verifies temporal safety properties of C programs, and report on our experience using it to find and localize errors in device drivers. The algorithm typically narrows the location of a cause down to a few lines, even in traces consisting of hundreds of statements.},
	urldate = {2015-06-18},
	booktitle = {Proceedings of the 30th {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Ball, Thomas and Naik, Mayur and Rajamani, Sriram K.},
	year = {2003},
	keywords = {debugging, Software model checking},
	pages = {97--105},
	file = {ball_et_al_2003_from_symptom_to_cause.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/R7T4TDNJ/ball_et_al_2003_from_symptom_to_cause.pdf:application/pdf}
}

@inproceedings{kennedy_generalized_2005,
	address = {New York, NY, USA},
	series = {{OOPSLA} '05},
	title = {Generalized {Algebraic} {Data} {Types} and {Object}-oriented {Programming}},
	isbn = {978-1-59593-031-6},
	url = {http://doi.acm.org/10.1145/1094811.1094814},
	doi = {10.1145/1094811.1094814},
	abstract = {Generalized algebraic data types (GADTs) have received much attention recently in the functional programming community. They generalize the (type) parameterized algebraic datatypes (PADTs) of ML and Haskell by permitting value constructors to return specific, rather than parametric, type-instantiations of their own datatype. GADTs have a number of applications, including strongly-typed evaluators, generic pretty-printing, generic traversals and queries, and typed LR parsing. We show that existing object-oriented programming languages such as Java and C\# can express GADT definitions, and a large class of GADT-manipulating programs, through the use of generics, subclassing, and virtual dispatch. However, some programs can be written only through the use of redundant runtime casts. Moreover, instantiation-specific, yet safe, operations on ordinary PADTs only admit indirect cast-free implementations, via higher-order encodings. We propose a generalization of the type constraint mechanisms of C\# and Java to both avoid the need for casts in GADT programs and higher-order contortions in PADT programs; we present a Visitor pattern for GADTs, and describe a refined switch construct as an alternative to virtual dispatch on datatypes. We formalize both extensions and prove type soundness.},
	urldate = {2015-08-29},
	booktitle = {Proceedings of the 20th {Annual} {ACM} {SIGPLAN} {Conference} on {Object}-oriented {Programming}, {Systems}, {Languages}, and {Applications}},
	publisher = {ACM},
	author = {Kennedy, Andrew and Russo, Claudio V.},
	year = {2005},
	keywords = {constraints, generalized algebraic data types, generics},
	pages = {21--40},
	file = {kennedy_russo_2005_generalized_algebraic_data_types_and_object-oriented_programming.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/6CTD7NH9/kennedy_russo_2005_generalized_algebraic_data_types_and_object-oriented_programming.pdf:application/pdf}
}

@inproceedings{morris_simple_2014,
	address = {New York, NY, USA},
	series = {Haskell '14},
	title = {A {Simple} {Semantics} for {Haskell} {Overloading}},
	isbn = {978-1-4503-3041-1},
	url = {http://doi.acm.org/10.1145/2633357.2633364},
	doi = {10.1145/2633357.2633364},
	abstract = {As originally proposed, type classes provide overloading and ad-hoc definition, but can still be understood (and implemented) in terms of strictly parametric calculi. This is not true of subsequent extensions of type classes. Functional dependencies and equality constraints allow the satisfiability of predicates to refine typing; this means that the interpretations of equivalent qualified types may not be interconvertible. Overlapping instances and instance chains allow predicates to be satisfied without determining the implementations of their associated class methods, introducing truly non-parametric behavior. We propose a new approach to the semantics of type classes, interpreting polymorphic expressions by the behavior of each of their ground instances, but without requiring that those behaviors be parametrically determined. We argue that this approach both matches the intuitive meanings of qualified types and accurately models the behavior of programs.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 2014 {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Morris, J. Garrett},
	year = {2014},
	keywords = {overloading, semantics, type classes},
	pages = {107--118},
	file = {morris_2014_a_simple_semantics_for_haskell_overloading.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/UTIRWGHA/morris_2014_a_simple_semantics_for_haskell_overloading.pdf:application/pdf}
}

@inproceedings{leung_interactive_2015,
	address = {New York, NY, USA},
	series = {{PLDI} 2015},
	title = {Interactive {Parser} {Synthesis} by {Example}},
	isbn = {978-1-4503-3468-6},
	url = {http://doi.acm.org/10.1145/2737924.2738002},
	doi = {10.1145/2737924.2738002},
	abstract = {Despite decades of research on parsing, the construction of parsers remains a painstaking, manual process prone to subtle bugs and pitfalls. We present a programming-by-example framework called Parsify that is able to synthesize a parser from input/output examples. The user does not write a single line of code. To achieve this, Parsify provides: (a) an iterative algorithm for synthesizing and refining a grammar one example at a time, (b) an interface that provides immediate visual feedback in response to changes in the grammar being refined, and (c) a graphical mechanism for specifying example parse trees using only textual selections. We empirically demonstrate the viability of our approach by using Parsify to construct parsers for source code drawn from Verilog, SQL, Apache, and Tiger.},
	urldate = {2015-06-15},
	booktitle = {Proceedings of the 36th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Leung, Alan and Sarracino, John and Lerner, Sorin},
	year = {2015},
	keywords = {parsing, Programming by Example, Program Synthesis},
	pages = {565--574},
	file = {leung_et_al_2015_interactive_parser_synthesis_by_example.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/SSFU6NSU/leung_et_al_2015_interactive_parser_synthesis_by_example.pdf:application/pdf}
}

@inproceedings{dunfield_refined_2007,
	address = {New York, NY, USA},
	series = {{PLPV} '07},
	title = {Refined {Typechecking} with {Stardust}},
	isbn = {978-1-59593-677-6},
	url = {http://doi.acm.org/10.1145/1292597.1292602},
	doi = {10.1145/1292597.1292602},
	abstract = {We present Stardust, an implementation of a type system for a subset of ML with type refinements, intersection types, and union types, enabling programmers to legibly specify certain classes of program invariants that are verified at compile time. This is the first implementation of unrestricted intersection and union types in a mainstream functional programming setting, as well as the first implementation of a system with both datasort and index refinements. The system-with the assistance of external constraint solvers-supports integer, Boolean and dimensional index refinements; we apply both value refinements (to check red-black tree invariants) and invaluable refinements (to check dimensional consistency). While typechecking with intersection and union types is intrinsically complex, our experience so far suggests that it can be practical in many instances.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 2007 {Workshop} on {Programming} {Languages} {Meets} {Program} {Verification}},
	publisher = {ACM},
	author = {Dunfield, Joshua},
	year = {2007},
	keywords = {dependent types, intersection types, type refinements, union types},
	pages = {21--32},
	file = {dunfield_2007_refined_typechecking_with_stardust.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/MQ975F6C/dunfield_2007_refined_typechecking_with_stardust.pdf:application/pdf}
}

@incollection{tsushima_embedded_2013,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {An {Embedded} {Type} {Debugger}},
	copyright = {©2013 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-41581-4 978-3-642-41582-1},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-41582-1_12},
	abstract = {This paper presents how to build a type debugger without implementing any dedicated type inferencer. Previous type debuggers required their own type inferencers apart from the compiler’s type inferencer. The advantage of our approach is threefold. First, by not implementing a type inferencer, it is guaranteed that the debugger’s type inference never disagrees with the compiler’s type inference. Secondly, we can avoid the pointless reproduction of a type inferencer that should work precisely as the compiler’s type inferencer. Thirdly, our approach is robust to updates of the underlying language. The key observation of our approach is that the interactive type debugging, as proposed by Chitil, does not require a type inference tree but only a tree with a certain simple property. We identify the property and present how to construct a tree that satisfies this property using the compiler’s type inferencer. The property guides us how to build a type debugger for various language constructs. In this paper, we describe our idea and first apply it to the simply-typed lambda calculus. After that, we extend it with let-polymorphism and objects to see how our technique scales.},
	language = {en},
	number = {8241},
	urldate = {2015-06-26},
	booktitle = {Implementation and {Application} of {Functional} {Languages}},
	publisher = {Springer Berlin Heidelberg},
	author = {Tsushima, Kanae and Asai, Kenichi},
	editor = {Hinze, Ralf},
	year = {2013},
	keywords = {Information Systems Applications (incl. Internet), Logics and Meanings of Programs, Mathematical Logic and Formal Languages, Programming Languages, Compilers, Interpreters, Programming Techniques, Software Engineering},
	pages = {190--206},
	file = {tsushima_asai_2013_an_embedded_type_debugger.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/NVSVNI9I/tsushima_asai_2013_an_embedded_type_debugger.pdf:application/pdf}
}

@inproceedings{cadar_exe:_2006,
	address = {New York, NY, USA},
	series = {{CCS} '06},
	title = {{EXE}: {Automatically} {Generating} {Inputs} of {Death}},
	isbn = {1-59593-518-5},
	shorttitle = {{EXE}},
	url = {http://doi.acm.org/10.1145/1180405.1180445},
	doi = {10.1145/1180405.1180445},
	abstract = {This paper presents EXE, an effective bug-finding tool that automatically generates inputs that crash real code. Instead of running code on manually or randomly constructed input, EXE runs it on symbolic input initially allowed to be "anything." As checked code runs, EXE tracks the constraints on each symbolic (i.e., input-derived) memory location. If a statement uses a symbolic value, EXE does not run it, but instead adds it as an input-constraint; all other statements run as usual. If code conditionally checks a symbolic expression, EXE forks execution, constraining the expression to be true on the true branch and false on the other. Because EXE reasons about all possible values on a path, it has much more power than a traditional runtime tool: (1) it can force execution down any feasible program path and (2) at dangerous operations (e.g., a pointer dereference), it detects if the current path constraints allow any value that causes a bug.When a path terminates or hits a bug, EXE automatically generates a test case by solving the current path constraints to find concrete values using its own co-designed constraint solver, STP. Because EXE's constraints have no approximations, feeding this concrete input to an uninstrumented version of the checked code will cause it to follow the same path and hit the same bug (assuming deterministic code).EXE works well on real code, finding bugs along with inputs that trigger them in: the BSD and Linux packet filter implementations, the udhcpd DHCP server, the pcre regular expression library, and three Linux file systems.},
	urldate = {2015-01-22},
	booktitle = {Proceedings of the 13th {ACM} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Cadar, Cristian and Ganesh, Vijay and Pawlowski, Peter M. and Dill, David L. and Engler, Dawson R.},
	year = {2006},
	keywords = {attack generation, bug finding, constraint solving, dynamic analysis, symbolic execution, test case generation},
	pages = {322--335},
	file = {cadar_et_al_2006_exe.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/M938IQV7/cadar_et_al_2006_exe.pdf:application/pdf}
}

@article{bennedsen_bluej_2010,
	title = {{BlueJ} {Visual} {Debugger} for {Learning} the {Execution} of {Object}-{Oriented} {Programs}?},
	volume = {10},
	issn = {1946-6226},
	url = {http://doi.acm.org/10.1145/1789934.1789938},
	doi = {10.1145/1789934.1789938},
	abstract = {This article reports on an experiment undertaken in order to evaluate the effect of a program visualization tool for helping students to better understand the dynamics of object-oriented programs. The concrete tool used was BlueJ’s debugger and object inspector. The study was done as a control-group experiment in an introductory programming course. The results of the experiment show that the students who used BlueJ’s debugger did not perform statistically significantly better than the students not using it; both groups profited about the same amount from the exercises given in the experiment. We discuss possible reasons for and implications of this result.},
	number = {2},
	urldate = {2015-10-29},
	journal = {Trans. Comput. Educ.},
	author = {Bennedsen, Jens and Schulte, Carsten},
	month = jun,
	year = {2010},
	keywords = {BlueJ, CS1, debugger, learning program execution, object inspector, object orientation, tools, visualization},
	pages = {8:1--8:22},
	file = {bennedsen_schulte_2010_bluej_visual_debugger_for_learning_the_execution_of_object-oriented_programs.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/AT4FAUH4/bennedsen_schulte_2010_bluej_visual_debugger_for_learning_the_execution_of_object-oriented_programs.pdf:application/pdf}
}

@inproceedings{khalek_testera:_2011,
	title = {Testera: {A} tool for testing java programs using alloy specifications},
	shorttitle = {Testera},
	url = {http://dl.acm.org/citation.cfm?id=2190145},
	urldate = {2014-09-09},
	booktitle = {Proceedings of the 2011 26th {IEEE}/{ACM} international conference on automated software engineering},
	publisher = {IEEE Computer Society},
	author = {Khalek, Shadi Abdul and Yang, Guowei and Zhang, Lingming and Marinov, Darko and Khurshid, Sarfraz},
	year = {2011},
	keywords = {\_tablet},
	pages = {608--611},
	file = {khalek_et_al_2011_testera.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/FA9A5ZHP/khalek_et_al_2011_testera.pdf:application/pdf}
}

@incollection{vazou_abstract_2013,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Abstract {Refinement} {Types}},
	copyright = {©2013 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-642-37035-9 978-3-642-37036-6},
	url = {http://link.springer.com/chapter/10.1007/978-3-642-37036-6_13},
	abstract = {We present abstract refinement types which enable quantification over the refinements of data- and function-types. Our key insight is that we can avail of quantification while preserving SMT-based decidability, simply by encoding refinement parameters as uninterpreted propositions within the refinement logic. We illustrate how this mechanism yields a variety of sophisticated means for reasoning about programs, including: parametric refinements for reasoning with type classes, index-dependent refinements for reasoning about key-value maps, recursive refinements for reasoning about recursive data types, and inductive refinements for reasoning about higher-order traversal routines. We have implemented our approach in a refinement type checker for Haskell and present experiments using our tool to verify correctness invariants of various programs.},
	language = {en},
	number = {7792},
	urldate = {2015-06-26},
	booktitle = {Programming {Languages} and {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Vazou, Niki and Rondon, Patrick M. and Jhala, Ranjit},
	editor = {Felleisen, Matthias and Gardner, Philippa},
	year = {2013},
	keywords = {Logics and Meanings of Programs, Programming Languages, Compilers, Interpreters, Programming Techniques, Software Engineering},
	pages = {209--228},
	file = {vazou_et_al_2013_abstract_refinement_types.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/ENWW3G4Q/vazou_et_al_2013_abstract_refinement_types.pdf:application/pdf}
}

@inproceedings{chakravarty_foreign_2014,
	address = {New York, NY, USA},
	series = {Haskell '14},
	title = {Foreign {Inline} {Code}: {Systems} {Demonstration}},
	isbn = {978-1-4503-3041-1},
	shorttitle = {Foreign {Inline} {Code}},
	url = {http://doi.acm.org/10.1145/2633357.2633372},
	doi = {10.1145/2633357.2633372},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 2014 {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Chakravarty, Manuel M.T.},
	year = {2014},
	keywords = {inline code, interoperability, template meta-programming},
	pages = {119--120},
	file = {chakravarty_2014_foreign_inline_code.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/P3Q5FSXJ/chakravarty_2014_foreign_inline_code.pdf:application/pdf}
}

@incollection{thatte_type_1988,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Type inference with partial types},
	copyright = {©1988 Springer-Verlag},
	isbn = {978-3-540-19488-0 978-3-540-39291-0},
	url = {http://link.springer.com/chapter/10.1007/3-540-19488-6_146},
	abstract = {This paper introduces a new form of type expressions which represent partial type information. These expressions are meant to capture the type information statically derivable from heterogeneous objects. The new monotypes form a lattice of subtypes and require type inference based on inclusion constraints. We discuss the existence and form of principle types under this extension and present a semi-decision procedure for the well-typing problem which can be restricted to a form that terminates for most practical programs. The partial type information derivable for heterogeneous entities is not sufficient to guarantee type-correctness for many of their uses. We therefore introduce a notion of statically generated dynamic type checks. Finally, all these elements are pulled together to sketch the derivation of a static system for "plausibility checking" which identifies the applications which may require a dynamic check and catches many type errors.},
	language = {en},
	number = {317},
	urldate = {2015-05-27},
	booktitle = {Automata, {Languages} and {Programming}},
	publisher = {Springer Berlin Heidelberg},
	author = {Thatte, Satish},
	editor = {Lepistö, Timo and Salomaa, Arto},
	year = {1988},
	keywords = {Algorithm Analysis and Problem Complexity, Programming Languages, Compilers, Interpreters, Programming Techniques},
	pages = {615--629},
	file = {thatte_1988_type_inference_with_partial_types.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/62XD2THK/thatte_1988_type_inference_with_partial_types.pdf:application/pdf}
}

@inproceedings{marlow_lightweight_2007,
	address = {New York, NY, USA},
	series = {Haskell '07},
	title = {A {Lightweight} {Interactive} {Debugger} for {Haskell}},
	isbn = {978-1-59593-674-5},
	url = {http://doi.acm.org/10.1145/1291201.1291204},
	doi = {10.1145/1291201.1291204},
	abstract = {This paper describes the design and construction of a Haskell source-level debugger built into the GHCi interactive environment. We have taken a pragmatic approach: the debugger is based on the traditional stop-examine-continue model of online debugging, which is simple and intuitive, but has traditionally been shunned in the context of Haskell because it exposes the lazy evaluation order. We argue that this drawback is not as severe as it may seem, and in some cases is an advantage. The design focuses on availability: our debugger is intended to work on all programs that can be compiled with GHC, and without requiring the programmer to jump through additional hoops to debug their program. The debugger has a novel approach for reconstructing the type of runtime values in a polymorphic context. Our implementation is light on complexity, and was integrated into GHC without significant upheaval.},
	urldate = {2015-05-29},
	booktitle = {Proceedings of the {ACM} {SIGPLAN} {Workshop} on {Haskell} {Workshop}},
	publisher = {ACM},
	author = {Marlow, Simon and Iborra, José and Pope, Bernard and Gill, Andy},
	year = {2007},
	keywords = {debugging, lazy evaluation},
	pages = {13--24},
	file = {marlow_et_al_2007_a_lightweight_interactive_debugger_for_haskell.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/DUTU75W2/marlow_et_al_2007_a_lightweight_interactive_debugger_for_haskell.pdf:application/pdf}
}

@inproceedings{bucur_prototyping_2014,
	address = {New York, NY, USA},
	series = {{ASPLOS} '14},
	title = {Prototyping {Symbolic} {Execution} {Engines} for {Interpreted} {Languages}},
	isbn = {978-1-4503-2305-5},
	url = {http://doi.acm.org/10.1145/2541940.2541977},
	doi = {10.1145/2541940.2541977},
	abstract = {Symbolic execution is being successfully used to automatically test statically compiled code. However, increasingly more systems and applications are written in dynamic interpreted languages like Python. Building a new symbolic execution engine is a monumental effort, and so is keeping it up-to-date as the target language evolves. Furthermore, ambiguous language specifications lead to their implementation in a symbolic execution engine potentially differing from the production interpreter in subtle ways. We address these challenges by flipping the problem and using the interpreter itself as a specification of the language semantics. We present a recipe and tool (called Chef) for turning a vanilla interpreter into a sound and complete symbolic execution engine. Chef symbolically executes the target program by symbolically executing the interpreter's binary while exploiting inferred knowledge about the program's high-level structure. Using Chef, we developed a symbolic execution engine for Python in 5 person-days and one for Lua in 3 person-days. They offer complete and faithful coverage of language features in a way that keeps up with future language versions at near-zero cost. Chef-produced engines are up to 1000 times more performant than if directly executing the interpreter symbolically without Chef.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}},
	publisher = {ACM},
	author = {Bucur, Stefan and Kinder, Johannes and Candea, George},
	year = {2014},
	keywords = {interpreter instrumentation, software analysis optimizations, state selection strategies},
	pages = {239--254},
	file = {bucur_et_al_2014_prototyping_symbolic_execution_engines_for_interpreted_languages.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/4IVJ78GD/bucur_et_al_2014_prototyping_symbolic_execution_engines_for_interpreted_languages.pdf:application/pdf}
}

@incollection{chitil_freja_2001,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Freja, {Hat} and {Hood} - {A} {Comparative} {Evaluation} of {Three} {Systems} for {Tracing} and {Debugging} {Lazy} {Functional} {Programs}},
	copyright = {©2001 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-41919-8 978-3-540-45361-1},
	url = {http://link.springer.com/chapter/10.1007/3-540-45361-X_11},
	abstract = {In this paper we compare three systems for tracing and debugging Haskell programs: Freja, Hat and Hood. We evaluate their usefulness in practice by applying them to a number of moderately complex programs in which errors had deliberately been introduced. We identify the strengths and weaknesses of each system and then form ideas on how the systems can be improved further.},
	language = {en},
	number = {2011},
	urldate = {2015-01-08},
	booktitle = {Implementation of {Functional} {Languages}},
	publisher = {Springer Berlin Heidelberg},
	author = {Chitil, Olaf and Runciman, Colin and Wallace, Malcolm},
	editor = {Mohnen, Markus and Koopman, Pieter},
	month = jan,
	year = {2001},
	keywords = {Logics and Meanings of Programs, Programming Languages, Compilers, Interpreters, Programming Techniques},
	pages = {176--193},
	file = {chitil_et_al_2001_freja,_hat_and_hood_-_a_comparative_evaluation_of_three_systems_for_tracing_and.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/JCHQMRBA/chitil_et_al_2001_freja,_hat_and_hood_-_a_comparative_evaluation_of_three_systems_for_tracing_and.pdf:application/pdf}
}

@inproceedings{wang_draining_2015,
	address = {Dagstuhl, Germany},
	series = {Leibniz {International} {Proceedings} in {Informatics} ({LIPIcs})},
	title = {Draining the {Swamp}: {Micro} {Virtual} {Machines} as {Solid} {Foundation} for {Language} {Development}},
	volume = {32},
	isbn = {978-3-939897-80-4},
	url = {http://drops.dagstuhl.de/opus/volltexte/2015/5034},
	doi = {http://dx.doi.org/10.4230/LIPIcs.SNAPL.2015.321},
	booktitle = {1st {Summit} on {Advances} in {Programming} {Languages} ({SNAPL} 2015)},
	publisher = {Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik},
	author = {Wang, Kunshan and Lin, Yi and Blackburn, Stephen M. and Norrish, Michael and Hosking, Antony L.},
	editor = {Ball, Thomas and Bodik, Rastislav and Krishnamurthi, Shriram and Lerner, Benjamin S. and Morrisett, Greg},
	year = {2015},
	pages = {321--336},
	file = {wang_et_al_2015_draining_the_swamp.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/Z2J8R685/wang_et_al_2015_draining_the_swamp.pdf:application/pdf}
}

@inproceedings{marlow_there_2014,
	address = {New York, NY, USA},
	series = {{ICFP} '14},
	title = {There is {No} {Fork}: {An} {Abstraction} for {Efficient}, {Concurrent}, and {Concise} {Data} {Access}},
	isbn = {978-1-4503-2873-9},
	shorttitle = {There is {No} {Fork}},
	url = {http://doi.acm.org/10.1145/2628136.2628144},
	doi = {10.1145/2628136.2628144},
	abstract = {We describe a new programming idiom for concurrency, based on Applicative Functors, where concurrency is implicit in the Applicative {\textless}*{\textgreater} operator. The result is that concurrent programs can be written in a natural applicative style, and they retain a high degree of clarity and modularity while executing with maximal concurrency. This idiom is particularly useful for programming against external data sources, where the application code is written without the use of explicit concurrency constructs, while the implementation is able to batch together multiple requests for data from the same source, and fetch data from multiple sources concurrently. Our abstraction uses a cache to ensure that multiple requests for the same data return the same result, which frees the programmer from having to arrange to fetch data only once, which in turn leads to greater modularity. While it is generally applicable, our technique was designed with a particular application in mind: an internal service at Facebook that identifies particular types of content and takes actions based on it. Our application has a large body of business logic that fetches data from several different external sources. The framework described in this paper enables the business logic to execute efficiently by automatically fetching data concurrently; we present some preliminary results.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 19th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Marlow, Simon and Brandy, Louis and Coens, Jonathan and Purdy, Jon},
	year = {2014},
	keywords = {applicative, concurrency, data-fetching, distributed, haskell, monad},
	pages = {325--337},
	file = {marlow_et_al_2014_there_is_no_fork.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/42XZBSEK/marlow_et_al_2014_there_is_no_fork.pdf:application/pdf}
}

@inproceedings{jackson_alcoa:_2000,
	title = {Alcoa: the {Alloy} constraint analyzer},
	isbn = {1-58113-206-9},
	shorttitle = {Alcoa},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=870482},
	doi = {10.1109/ICSE.2000.870482},
	urldate = {2015-06-26},
	publisher = {ACM},
	author = {Jackson, D. and Schechter, I. and Shlyakhter, I.},
	year = {2000},
	pages = {730--733}
}

@techreport{majumdar_latest_2007,
	title = {{LATEST} : {Lazy} {Dynamic} {Test} {Input} {Generation}},
	url = {http://www.eecs.berkeley.edu/Pubs/TechRpts/2007/EECS-2007-36.html},
	abstract = {We present lazy expansion, a new algorithm for scalable test input generation using directed concolic execution. Lazy expansion is an instantiation of the counterexample-guided refinement paradigm from static software verification in the context of testing. Our algorithm works in two phases. It first explores, using concolic execution, an abstraction of the function under test by replacing each called function with an unconstrained input. Second, for each (possibly spurious) trace generated by this abstraction, it attempts to expand the trace to a concretely realizable execution by recursively expanding the called functions and finding concrete executions in the called functions that can be stitched together with the original trace to form a complete program execution. Thus, it reduces the burden of symbolic reasoning about interprocedural paths to reasoning about intraprocedural paths (in the exploration phase), together with a localized and constrained search through functions (in the concretization phase). Lazy expansion is particularly effective in testing functions that make more-or-less independent calls to lower level library functions (that have already been unit tested), by only exploring relevant paths in the function under test. We have implemented our algorithm on top of the CUTE concolic execution tool for C and applied it to testing parser code in small compilers. In preliminary experiments, our tool, called LATEST, outperformed CUTE by an order of magnitude in terms of the time taken to generate inputs, and in contrast to CUTE, produced many syntactically valid input strings which exercised interesting paths through the compiler (rather than only the parser error handling code).},
	number = {UCB/EECS-2007-36},
	institution = {EECS Department, University of California, Berkeley},
	author = {Majumdar, Rupak and Sen, Koushik},
	month = mar,
	year = {2007},
	file = {majumdar_sen_2007_latest.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/G4SZKE4S/majumdar_sen_2007_latest.pdf:application/pdf}
}

@inproceedings{serrano_type_2015,
	address = {New York, NY, USA},
	series = {Haskell 2015},
	title = {Type {Families} with {Class}, {Type} {Classes} with {Family}},
	isbn = {978-1-4503-3808-0},
	url = {http://doi.acm.org/10.1145/2804302.2804304},
	doi = {10.1145/2804302.2804304},
	abstract = {Type classes and type families are key ingredients in Haskell programming. Type classes were introduced to deal with ad-hoc polymorphism, although with the introduction of functional dependencies, their use expanded to type-level programming. Type families also allow encoding type-level functions, but more directly in the form of rewrite rules. In this paper we show that type families are powerful enough to simulate type classes (without overlapping instances), and we provide a formal proof of the soundness and completeness of this simulation. Encoding instance constraints as type families eases the path to proposed extensions to type classes, like closed sets of instances, instance chains, and control over the search procedure. The only feature which type families cannot simulate is elaboration, that is, generating code from the derivation of a rewriting. We look at ways to solve this problem in current Haskell, and propose an extension to allow elaboration during the rewriting phase.},
	urldate = {2015-09-03},
	booktitle = {Proceedings of the 8th {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Serrano, Alejandro and Hage, Jurriaan and Bahr, Patrick},
	year = {2015},
	keywords = {Directives, elaboration, Functional dependencies, haskell, type classes, type families},
	pages = {129--140},
	file = {serrano_et_al_2015_type_families_with_class,_type_classes_with_family.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/PERANI6I/serrano_et_al_2015_type_families_with_class,_type_classes_with_family.pdf:application/pdf}
}

@inproceedings{heeren_helium_2003,
	address = {New York, NY, USA},
	series = {Haskell '03},
	title = {Helium, for {Learning} {Haskell}},
	isbn = {1-58113-758-3},
	url = {http://doi.acm.org/10.1145/871895.871902},
	doi = {10.1145/871895.871902},
	abstract = {Helium is a user-friendly compiler designed especially for learning the functional programming language Haskell. The quality of the error messages has been the main concern both in the choice of the language features and in the implementation of the compiler. Helium implements almost full Haskell, where the most notable difference is the absence of type classes. Our goal is to let students learn functional programming more quickly and with more fun. The compiler has been successfully employed in two introductory programming courses at Utrecht University.},
	urldate = {2015-05-21},
	booktitle = {Proceedings of the 2003 {ACM} {SIGPLAN} {Workshop} on {Haskell}},
	publisher = {ACM},
	author = {Heeren, Bastiaan and Leijen, Daan and van IJzendoorn, Arjan},
	year = {2003},
	keywords = {education, error logging, error messages, learning Haskell, type inference},
	pages = {62--71},
	file = {heeren_et_al_2003_helium,_for_learning_haskell.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/EVR8R9KT/heeren_et_al_2003_helium,_for_learning_haskell.pdf:application/pdf}
}

@inproceedings{lattner_llvm:_2004,
	title = {{LLVM}: a compilation framework for lifelong program analysis transformation},
	shorttitle = {{LLVM}},
	doi = {10.1109/CGO.2004.1281665},
	abstract = {We describe LLVM (low level virtual machine), a compiler framework designed to support transparent, lifelong program analysis and transformation for arbitrary programs, by providing high-level information to compiler transformations at compile-time, link-time, run-time, and in idle time between runs. LLVM defines a common, low-level code representation in static single assignment (SSA) form, with several novel features: a simple, language-independent type-system that exposes the primitives commonly used to implement high-level language features; an instruction for typed address arithmetic; and a simple mechanism that can be used to implement the exception handling features of high-level languages (and setjmp/longjmp in C) uniformly and efficiently. The LLVM compiler framework and code representation together provide a combination of key capabilities that are important for practical, lifelong analysis and transformation of programs. To our knowledge, no existing compilation approach provides all these capabilities. We describe the design of the LLVM representation and compiler framework, and evaluate the design in three ways: (a) the size and effectiveness of the representation, including the type information it provides; (b) compiler performance for several interprocedural problems; and (c) illustrative examples of the benefits LLVM provides for several challenging compiler problems.},
	booktitle = {International {Symposium} on {Code} {Generation} and {Optimization}, 2004. {CGO} 2004},
	author = {Lattner, C. and Adve, V.},
	month = mar,
	year = {2004},
	keywords = {Algorithm design and analysis, Application software, Arithmetic, C language, code representation, compiler framework, compiler performance, compiler transformations, exception handling, High level languages, Information analysis, language-independent type-system, low level virtual machine, optimising compilers, Performance analysis, program analysis, program diagnostics, Program processors, program transformation, Runtime, Software safety, static single assignment form, typed address arithmetic, virtual machines, Virtual machining},
	pages = {75--86},
	file = {lattner_adve_2004_llvm.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/7TKE4IKJ/lattner_adve_2004_llvm.pdf:application/pdf}
}

@inproceedings{klein_randomized_2009,
	title = {Randomized testing in {PLT} {Redex}},
	url = {http://users.eecs.northwestern.edu/~robby/pubs/papers/scheme2009-kf.pdf},
	urldate = {2014-08-19},
	booktitle = {{ACM} {SIGPLAN} {Workshop} on {Scheme} and {Functional} {Programming}},
	author = {Klein, Casey and Findler, Robert Bruce},
	year = {2009},
	keywords = {\_tablet},
	file = {klein_findler_2009_randomized_testing_in_plt_redex.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/X3Z5DTKS/klein_findler_2009_randomized_testing_in_plt_redex.pdf:application/pdf}
}

@article{leavens_preliminary_2006,
	title = {Preliminary {Design} of {JML}: {A} {Behavioral} {Interface} {Specification} {Language} for {Java}},
	volume = {31},
	issn = {0163-5948},
	shorttitle = {Preliminary {Design} of {JML}},
	url = {http://doi.acm.org/10.1145/1127878.1127884},
	doi = {10.1145/1127878.1127884},
	abstract = {JML is a behavioral interface specification language tailored to Java(TM). Besides pre- and postconditions, it also allows assertions to be intermixed with Java code; these aid verification and debugging. JML is designed to be used by working software engineers; to do this it follows Eiffel in using Java expressions in assertions. JML combines this idea from Eiffel with the model-based approach to specifications, typified by VDM and Larch, which results in greater expressiveness. Other expressiveness advantages over Eiffel include quantifiers, specification-only variables, and frame conditions.This paper discusses the goals of JML, the overall approach, and describes the basic features of the language through examples. It is intended for readers who have some familiarity with both Java and behavioral specification using pre- and postconditions.},
	number = {3},
	urldate = {2015-01-24},
	journal = {SIGSOFT Softw. Eng. Notes},
	author = {Leavens, Gary T. and Baker, Albert L. and Ruby, Clyde},
	month = may,
	year = {2006},
	pages = {1--38},
	file = {leavens_et_al_2006_preliminary_design_of_jml.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/BAW2G29E/leavens_et_al_2006_preliminary_design_of_jml.pdf:application/pdf}
}

@inproceedings{muranushi_experience_2014,
	address = {New York, NY, USA},
	series = {Haskell '14},
	title = {Experience {Report}: {Type}-checking {Polymorphic} {Units} for {Astrophysics} {Research} in {Haskell}},
	isbn = {978-1-4503-3041-1},
	shorttitle = {Experience {Report}},
	url = {http://doi.acm.org/10.1145/2633357.2633362},
	doi = {10.1145/2633357.2633362},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 2014 {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Muranushi, Takayuki and Eisenberg, Richard A.},
	year = {2014},
	keywords = {haskell, quantity calculus, type families, type-level computation},
	pages = {31--38},
	file = {muranushi_eisenberg_2014_experience_report.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/8X78XBSA/muranushi_eisenberg_2014_experience_report.pdf:application/pdf}
}

@inproceedings{bernardy_type-theory_2013,
	address = {New York, NY, USA},
	series = {{ICFP} '13},
	title = {Type-theory in {Color}},
	isbn = {978-1-4503-2326-0},
	url = {http://doi.acm.org/10.1145/2500365.2500577},
	doi = {10.1145/2500365.2500577},
	abstract = {Dependent type-theory aims to become the standard way to formalize mathematics at the same time as displacing traditional platforms for high-assurance programming. However, current implementations of type theory are still lacking, in the sense that some obvious truths require explicit proofs, making type-theory awkward to use for many applications, both in formalization and programming. In particular, notions of erasure are poorly supported. In this paper we propose an extension of type-theory with colored terms, color erasure and interpretation of colored types as predicates. The result is a more powerful type-theory: some definitions and proofs may be omitted as they become trivial, it becomes easier to program with precise types, and some parametricity results can be internalized.},
	urldate = {2015-08-18},
	booktitle = {Proceedings of the 18th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Bernardy, Jean-Philippe and Guilhem, Moulin},
	year = {2013},
	keywords = {erasure, parametricity, type-theory},
	pages = {61--72},
	file = {bernardy_guilhem_2013_type-theory_in_color.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/JKVXQ2NI/bernardy_guilhem_2013_type-theory_in_color.pdf:application/pdf}
}

@inproceedings{lerner_seminal:_2006,
	address = {New York, NY, USA},
	series = {{ML} '06},
	title = {Seminal: {Searching} for {ML} {Type}-error {Messages}},
	isbn = {1-59593-483-9},
	shorttitle = {Seminal},
	url = {http://doi.acm.org/10.1145/1159876.1159887},
	doi = {10.1145/1159876.1159887},
	abstract = {We present a new way to generate type-error messages in a polymorphic, implicitly, and strongly typed language (specifically Caml). Our method separates error-message generation from type-checking by taking a fundamentally new approach: we present to programmers small term-level modifications that cause an ill-typed program to become well-typed. This approach aims to improve feedback to programmers with no change to the underlying type-checker nor the compilation of well-typed programs.We have added a prototype implementation of our approach to the Objective Caml system by intercepting type-checker error messages and using the type-checker on candidate changes to see if they succeed. This novel front-end architecture naturally decomposes into (1) enumerating local changes to the abstract syntax tree that may remove type errors, (2) searching for places to try the changes, (3) using the type-checker to evaluate the changes, and (4) ranking the changes and presenting them to the user.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 2006 {Workshop} on {ML}},
	publisher = {ACM},
	author = {Lerner, Benjamin and Grossman, Dan and Chambers, Craig},
	year = {2006},
	keywords = {error messages, objective Caml, seminal, type-checking, type-inference},
	pages = {63--73},
	file = {lerner_et_al_2006_seminal.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/6ZKTKVM7/lerner_et_al_2006_seminal.pdf:application/pdf}
}

@inproceedings{wang_what_2009,
	address = {New York, NY, USA},
	series = {{WGP} '09},
	title = {What {Does} {Aspect}-oriented {Programming} {Mean} for {Functional} {Programmers}?},
	isbn = {978-1-60558-510-9},
	url = {http://doi.acm.org/10.1145/1596614.1596621},
	doi = {10.1145/1596614.1596621},
	abstract = {Aspect-Oriented Programming (AOP) aims at modularising crosscutting concerns that show up in software. The success of AOP has been almost viral and nearly all areas in Software Engineering and Programming Languages have become "infected" by the AOP bug in one way or another. Interestingly the functional programming community (and, in particular, the pure functional programming community) seems to be resistant to the pandemic. The goal of this paper is to debate the possible causes of the functional programming community's resistance and to raise awareness and interest by showcasing the benefits that could be gained from having a functional AOP language. At the same time, we identify the main challenges and explore the possible design-space.},
	urldate = {2015-06-02},
	booktitle = {Proceedings of the 2009 {ACM} {SIGPLAN} {Workshop} on {Generic} {Programming}},
	publisher = {ACM},
	author = {Wang, Meng and Oliveira, Bruno C. d. S.},
	year = {2009},
	keywords = {aspect-oriented programming, functional programming, program extensibility and adaptability, separation of concerns},
	pages = {37--48},
	file = {wang_oliveira_2009_what_does_aspect-oriented_programming_mean_for_functional_programmers.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/VIQ55U6T/wang_oliveira_2009_what_does_aspect-oriented_programming_mean_for_functional_programmers.pdf:application/pdf}
}

@inproceedings{osera_type-and-example-directed_2015,
	address = {New York, NY, USA},
	series = {{PLDI} 2015},
	title = {Type-and-example-directed {Program} {Synthesis}},
	isbn = {978-1-4503-3468-6},
	url = {http://doi.acm.org/10.1145/2737924.2738007},
	doi = {10.1145/2737924.2738007},
	abstract = {This paper presents an algorithm for synthesizing recursive functions that process algebraic datatypes. It is founded on proof-theoretic techniques that exploit both type information and input–output examples to prune the search space. The algorithm uses refinement trees, a data structure that succinctly represents constraints on the shape of generated code. We evaluate the algorithm by using a prototype implementation to synthesize more than 40 benchmarks and several non-trivial larger examples. Our results demonstrate that the approach meets or outperforms the state-of-the-art for this domain, in terms of synthesis time or attainable size of the generated programs.},
	urldate = {2015-06-15},
	booktitle = {Proceedings of the 36th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Osera, Peter-Michael and Zdancewic, Steve},
	year = {2015},
	keywords = {functional programming, Program Syn- thesis, Proof Search, type theory},
	pages = {619--630},
	file = {osera_zdancewic_2015_type-and-example-directed_program_synthesis.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/3UUQ8ZRW/osera_zdancewic_2015_type-and-example-directed_program_synthesis.pdf:application/pdf}
}

@inproceedings{rompf_lightweight_2010,
	address = {New York, NY, USA},
	series = {{GPCE} '10},
	title = {Lightweight {Modular} {Staging}: {A} {Pragmatic} {Approach} to {Runtime} {Code} {Generation} and {Compiled} {DSLs}},
	isbn = {978-1-4503-0154-1},
	shorttitle = {Lightweight {Modular} {Staging}},
	url = {http://doi.acm.org/10.1145/1868294.1868314},
	doi = {10.1145/1868294.1868314},
	abstract = {Software engineering demands generality and abstraction, performance demands specialization and concretization. Generative programming can provide both, but the effort required to develop high-quality program generators likely offsets their benefits, even if a multi-stage programming language is used. We present lightweight modular staging, a library-based multi-stage programming approach that breaks with the tradition of syntactic quasi-quotation and instead uses only types to distinguish between binding times. Through extensive use of component technology, lightweight modular staging makes an optimizing compiler framework available at the library level, allowing programmers to tightly integrate domain-specific abstractions and optimizations into the generation process. We argue that lightweight modular staging enables a form of language virtualization, i.e. allows to go from a pure-library embedded language to one that is practically equivalent to a stand-alone implementation with only modest effort.},
	urldate = {2015-04-29},
	booktitle = {Proceedings of the {Ninth} {International} {Conference} on {Generative} {Programming} and {Component} {Engineering}},
	publisher = {ACM},
	author = {Rompf, Tiark and Odersky, Martin},
	year = {2010},
	keywords = {code generation, domain-specific languages, language virtualization, multi-stage programming},
	pages = {127--136},
	file = {rompf_odersky_2010_lightweight_modular_staging.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/WT32M2CB/rompf_odersky_2010_lightweight_modular_staging.pdf:application/pdf}
}

@article{csallner_jcrasher:_2004,
	title = {{JCrasher}: an automatic robustness tester for {Java}},
	volume = {34},
	copyright = {Copyright © 2004 John Wiley \& Sons, Ltd.},
	issn = {1097-024X},
	shorttitle = {{JCrasher}},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/spe.602/abstract},
	doi = {10.1002/spe.602},
	abstract = {JCrasher is an automatic robustness testing tool for Java code. JCrasher examines the type information of a set of Java classes and constructs code fragments that will create instances of different types to test the behavior of public methods under random data. JCrasher attempts to detect bugs by causing the program under test to ‘crash’, that is, to throw an undeclared runtime exception. Although in general the random testing approach has many limitations, it also has the advantage of being completely automatic: o supervision is required except for off-line inspection of the test ases that have caused a crash. Compared to other similar commercial and research tools, JCrasher offers several novelties: it transitively analyzes methods, determines the size of each tested method's parameter-space and selects parameter combinations and therefore test cases at random, taking into account the time allocated for testing; it defines heuristics for determining whether a Java exception should be considered as a program bug or whether the JCrasher supplied inputs have violated the code's preconditions; it includes support for efficiently undoing all the state changes introduced by previous tests; it produces test files for JUnit, a popular Java testing tool; and it can be integrated in the Eclipse IDE. Copyright © 2004 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {11},
	urldate = {2015-06-26},
	journal = {Software: Practice and Experience},
	author = {Csallner, Christoph and Smaragdakis, Yannis},
	month = sep,
	year = {2004},
	keywords = {Java, random testing, software testing, state re-initialization, test case generation},
	pages = {1025--1050},
	file = {csallner_smaragdakis_2004_jcrasher.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/HF779R5P/csallner_smaragdakis_2004_jcrasher.pdf:application/pdf}
}

@inproceedings{oliveira_functional_2012,
	address = {New York, NY, USA},
	series = {{ICFP} '12},
	title = {Functional {Programming} with {Structured} {Graphs}},
	isbn = {978-1-4503-1054-3},
	url = {http://doi.acm.org/10.1145/2364527.2364541},
	doi = {10.1145/2364527.2364541},
	abstract = {This paper presents a new functional programming model for graph structures called structured graphs. Structured graphs extend conventional algebraic datatypes with explicit definition and manipulation of cycles and/or sharing, and offer a practical and convenient way to program graphs in functional programming languages like Haskell. The representation of sharing and cycles (edges) employs recursive binders and uses an encoding inspired by parametric higher-order abstract syntax. Unlike traditional approaches based on mutable references or node/edge lists, well-formedness of the graph structure is ensured statically and reasoning can be done with standard functional programming techniques. Since the binding structure is generic, we can define many useful generic combinators for manipulating structured graphs. We give applications and show how to reason about structured graphs.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 17th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Oliveira, Bruno C.d.S. and Cook, William R.},
	year = {2012},
	keywords = {graphs, haskell, parametric hoas},
	pages = {77--88},
	file = {oliveira_cook_2012_functional_programming_with_structured_graphs.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/CCK9WTHX/oliveira_cook_2012_functional_programming_with_structured_graphs.pdf:application/pdf}
}

@incollection{berdine_smallfoot:_2006,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Smallfoot: {Modular} {Automatic} {Assertion} {Checking} with {Separation} {Logic}},
	copyright = {©2006 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-36749-9 978-3-540-36750-5},
	shorttitle = {Smallfoot},
	url = {http://link.springer.com/chapter/10.1007/11804192_6},
	abstract = {Separation logic is a program logic for reasoning about programs that manipulate pointer data structures. We describe Smallfoot, a tool for checking certain lightweight separation logic specifications. The assertions describe the shapes of data structures rather than their detailed contents, and this allows reasoning to be fully automatic. The presentation in the paper is tutorial in style. We illustrate what the tool can do via examples which are oriented toward novel aspects of separation logic, namely: avoidance of frame axioms (which say what a procedure does not change); embracement of “dirty” features such as memory disposal and address arithmetic; information hiding in the presence of pointers; and modular reasoning about concurrent programs.},
	language = {en},
	number = {4111},
	urldate = {2015-06-11},
	booktitle = {Formal {Methods} for {Components} and {Objects}},
	publisher = {Springer Berlin Heidelberg},
	author = {Berdine, Josh and Calcagno, Cristiano and O’Hearn, Peter W.},
	editor = {Boer, Frank S. de and Bonsangue, Marcello M. and Graf, Susanne and Roever, Willem-Paul de},
	year = {2006},
	keywords = {Logics and Meanings of Programs, Operating Systems, Programming Languages, Compilers, Interpreters, Software Engineering},
	pages = {115--137},
	file = {berdine_et_al_2006_smallfoot.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/T5UAIVPN/berdine_et_al_2006_smallfoot.pdf:application/pdf}
}

@inproceedings{marceau_mind_2011,
	address = {New York, NY, USA},
	series = {Onward! 2011},
	title = {Mind {Your} {Language}: {On} {Novices}' {Interactions} with {Error} {Messages}},
	isbn = {978-1-4503-0941-7},
	shorttitle = {Mind {Your} {Language}},
	url = {http://doi.acm.org/10.1145/2048237.2048241},
	doi = {10.1145/2048237.2048241},
	abstract = {Error messages are one of the most important tools that a language offers its programmers. For novices, this feed-back is especially critical. Error messages typically contain both a textual description of the problem and an indication of where in the code the error occurred. This paper reports on a series of studies that explore beginning students' inter-actions with the vocabulary and source-expression high-lighting in DrRacket. Our findings demonstrate that the error message significantly fail to convey information accurately to students, while also suggesting alternative designs that might address these problems.},
	urldate = {2015-09-24},
	booktitle = {Proceedings of the 10th {SIGPLAN} {Symposium} on {New} {Ideas}, {New} {Paradigms}, and {Reflections} on {Programming} and {Software}},
	publisher = {ACM},
	author = {Marceau, Guillaume and Fisler, Kathi and Krishnamurthi, Shriram},
	year = {2011},
	keywords = {beginner-friendly ides, error message design, novice programmers, user-studies},
	pages = {3--18},
	file = {marceau_et_al_2011_mind_your_language.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/Q4342API/marceau_et_al_2011_mind_your_language.pdf:application/pdf}
}

@phdthesis{leijen__????,
	title = {The λ abroad},
	url = {http://131.107.65.14/pubs/65216/phd-thesis.pdf},
	urldate = {2015-06-29},
	author = {Leijen, Daniel Johannes Pieter},
	file = {leijen_the_λ_abroad.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/ADUKAGMM/leijen_the_λ_abroad.pdf:application/pdf}
}

@phdthesis{erkok_value_2002,
	type = {Ph.{D}.},
	title = {Value recursion in monadic computations},
	url = {http://digitalcommons.ohsu.edu/etd/164},
	abstract = {This thesis addresses the interaction between recursive declarations and computational effects modeled by monads. More specifically, we present a framework for modeling cyclic definitions resulting from the values of monadic actions. We introduce the term value recursion to capture this kind of recursion. Our model of value recursion relies on the existence of particular fixed-point operators for individual monads, whose behavior is axiomatized via a number of equational properties. These properties regulate the interaction between monadic effects and recursive computations, giving rise to a characterization of the required recursion operation. We present a collection of such operators for monads that are frequently used in functional programming, including those that model exceptions, non-determinism, input-output, and stateful computations. In the context of the programming language Haskell, practical applications of value recursion give rise to the need for a new language construct, providing support for recursive monadic bindings. We discuss the design and implementation of an extension to Haskell's do-notation which allows variables to be bound recursively, eliminating the need for programming with explicit fixed-point operators.},
	school = {Oregon Health \& Science University},
	author = {Erkok, Levent},
	month = oct,
	year = {2002},
	file = {erkok_2002_value_recursion_in_monadic_computations.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/SSCI6WUU/erkok_2002_value_recursion_in_monadic_computations.pdf:application/pdf}
}

@article{gibbons_essence_2009,
	title = {The essence of the {Iterator} pattern},
	volume = {19},
	issn = {1469-7653},
	url = {http://journals.cambridge.org/article_S0956796809007291},
	doi = {10.1017/S0956796809007291},
	abstract = {The Iterator pattern gives a clean interface for element-by-element access to a collection, independent of the collection's shape. Imperative iterations using the pattern have two simultaneous aspects: mapping and accumulating. Various existing functional models of iteration capture one or other of these aspects, but not both simultaneously. We argue that C. McBride and R. Paterson's applicative functors (Applicative programming with effects, J. Funct. Program., 18 (1): 1–13, 2008), and in particular the corresponding traverse operator, do exactly this, and therefore capture the essence of the Iterator pattern. Moreover, they do so in a way that nicely supports modular programming. We present some axioms for traversal, discuss modularity concerns and illustrate with a simple example, the wordcount problem.},
	number = {3-4},
	urldate = {2015-06-02},
	journal = {Journal of Functional Programming},
	author = {Gibbons, Jeremy and Oliveira, BRUNO C. d. S.},
	year = {2009},
	pages = {377--402},
	file = {gibbons_oliveira_2009_the_essence_of_the_iterator_pattern.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/GD75WFBE/gibbons_oliveira_2009_the_essence_of_the_iterator_pattern.pdf:application/pdf}
}

@inproceedings{renieris_almost:_1999,
	address = {New York, NY, USA},
	series = {{NPIVM} '99},
	title = {Almost: {Exploring} {Program} {Traces}},
	isbn = {978-1-58113-254-0},
	shorttitle = {Almost},
	url = {http://doi.acm.org/10.1145/331770.331788},
	doi = {10.1145/331770.331788},
	abstract = {We built a tool to visualize and explore program execution traces. Our goal was to help programmers without any prior knowledge of a program, quickly get enough knowledge about its structure so that they can make small to medium changes. In the process, a number of problems were faced and tackled concerning the efficient use of screen space, interaction with multiple concurrent views, and linking of asymmetric views},
	urldate = {2015-10-29},
	booktitle = {Proceedings of the 1999 {Workshop} on {New} {Paradigms} in {Information} {Visualization} and {Manipulation} in {Conjunction} with the {Eighth} {ACM} {Internation} {Conference} on {Information} and {Knowledge} {Management}},
	publisher = {ACM},
	author = {Renieris, Manos and Reiss, Steven P.},
	year = {1999},
	keywords = {connected views, program traces, software visualization, spiral},
	pages = {70--77},
	file = {renieris_reiss_1999_almost.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/8GDP68VU/renieris_reiss_1999_almost.pdf:application/pdf}
}

@inproceedings{park_kjs:_2015,
	address = {New York, NY, USA},
	series = {{PLDI} 2015},
	title = {{KJS}: {A} {Complete} {Formal} {Semantics} of {JavaScript}},
	isbn = {978-1-4503-3468-6},
	shorttitle = {{KJS}},
	url = {http://doi.acm.org/10.1145/2737924.2737991},
	doi = {10.1145/2737924.2737991},
	abstract = {This paper presents KJS, the most complete and throughly tested formal semantics of JavaScript to date. Being executable, KJS has been tested against the ECMAScript 5.1 conformance test suite, and passes all 2,782 core language tests. Among the existing implementations of JavaScript, only Chrome V8's passes all the tests, and no other semantics passes more than 90\%. In addition to a reference implementation for JavaScript, KJS also yields a simple coverage metric for a test suite: the set of semantic rules it exercises. Our semantics revealed that the ECMAScript 5.1 conformance test suite fails to cover several semantic rules. Guided by the semantics, we wrote tests to exercise those rules. The new tests revealed bugs both in production JavaScript engines (Chrome V8, Safari WebKit, Firefox SpiderMonkey) and in other semantics. KJS is symbolically executable, thus it can be used for formal analysis and verification of JavaScript programs. We verified non-trivial programs and found a known security vulnerability.},
	urldate = {2015-10-23},
	booktitle = {Proceedings of the 36th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Park, Daejun and Stefănescu, Andrei and Roşu, Grigore},
	year = {2015},
	keywords = {javascript, K framework, mechanized semantics},
	pages = {346--356},
	file = {park_et_al_2015_kjs.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/IED23C4F/park_et_al_2015_kjs.pdf:application/pdf}
}

@inproceedings{chitil_practical_2012,
	address = {New York, NY, USA},
	series = {{ICFP} '12},
	title = {Practical {Typed} {Lazy} {Contracts}},
	isbn = {978-1-4503-1054-3},
	url = {http://doi.acm.org/10.1145/2364527.2364539},
	doi = {10.1145/2364527.2364539},
	abstract = {Until now there has been no support for specifying and enforcing contracts within a lazy functional program. That is a shame, because contracts consist of pre- and post-conditions for functions that go beyond the standard static types. This paper presents the design and implementation of a small, easy-to-use, purely functional contract library for Haskell, which, when a contract is violated, also provides more useful information than the classical blaming of one contract partner. From now on lazy functional languages can profit from the assurances in the development of correct programs that contracts provide.},
	urldate = {2015-01-08},
	booktitle = {Proceedings of the 17th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Chitil, Olaf},
	year = {2012},
	keywords = {haskell, lazy, library, purely functional},
	pages = {67--76},
	file = {chitil_2012_practical_typed_lazy_contracts.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/5JI2MTWV/chitil_2012_practical_typed_lazy_contracts.pdf:application/pdf}
}

@inproceedings{bolingbroke_types_2009,
	address = {New York, NY, USA},
	series = {Haskell '09},
	title = {Types {Are} {Calling} {Conventions}},
	isbn = {978-1-60558-508-6},
	url = {http://doi.acm.org/10.1145/1596638.1596640},
	doi = {10.1145/1596638.1596640},
	abstract = {It is common for compilers to derive the calling convention of a function from its type. Doing so is simple and modular but misses many optimisation opportunities, particularly in lazy, higher-order functional languages with extensive use of currying. We restore the lost opportunities by defining Strict Core, a new intermediate language whose type system makes the missing distinctions: laziness is explicit, and functions take multiple arguments and return multiple results.},
	urldate = {2015-08-21},
	booktitle = {Proceedings of the 2Nd {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Bolingbroke, Maximilian C. and Peyton Jones, Simon L.},
	year = {2009},
	keywords = {arity, calling conventions, intermediate language, strictness, unboxing, uncurrying},
	pages = {1--12},
	file = {bolingbroke_peyton_jones_2009_types_are_calling_conventions.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/SKS84FTF/bolingbroke_peyton_jones_2009_types_are_calling_conventions.pdf:application/pdf}
}

@article{chakravarty_risks_2004,
	title = {The risks and benefits of teaching purely functional programming in first year},
	volume = {14},
	issn = {1469-7653},
	url = {http://journals.cambridge.org/article_S0956796803004805},
	doi = {10.1017/S0956796803004805},
	abstract = {We argue that teaching purely functional programming as such in freshman courses is detrimental to both the curriculum as well as to promoting the paradigm. Instead, we need to focus on the more general aims of teaching elementary techniques of programming and essential concepts of computing. We support this viewpoint with experience gained during several semesters of teaching large first-year classes (up to 600 students) in Haskell. These classes consisted of computer science students as well as students from other disciplines. We have systematically gathered student feedback by conducting surveys after each semester. This article contributes an approach to the use of modern functional languages in first year courses and, based on this, advocates the use of functional languages in this setting.},
	number = {01},
	urldate = {2015-03-04},
	journal = {Journal of Functional Programming},
	author = {Chakravarty, Manuel M. T. and Keller, Gabriele},
	month = jan,
	year = {2004},
	keywords = {education, functional programming},
	pages = {113--123},
	file = {chakravarty_keller_2004_the_risks_and_benefits_of_teaching_purely_functional_programming_in_first_year.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/DZ7XWQV6/chakravarty_keller_2004_the_risks_and_benefits_of_teaching_purely_functional_programming_in_first_year.pdf:application/pdf}
}

@inproceedings{meyerovich_empirical_2013,
	address = {New York, NY, USA},
	series = {{OOPSLA} '13},
	title = {Empirical {Analysis} of {Programming} {Language} {Adoption}},
	isbn = {978-1-4503-2374-1},
	url = {http://doi.acm.org/10.1145/2509136.2509515},
	doi = {10.1145/2509136.2509515},
	abstract = {Some programming languages become widely popular while others fail to grow beyond their niche or disappear altogether. This paper uses survey methodology to identify the factors that lead to language adoption. We analyze large datasets, including over 200,000 SourceForge projects, 590,000 projects tracked by Ohloh, and multiple surveys of 1,000-13,000 programmers. We report several prominent findings. First, language adoption follows a power law; a small number of languages account for most language use, but the programming market supports many languages with niche user bases. Second, intrinsic features have only secondary importance in adoption. Open source libraries, existing code, and experience strongly influence developers when selecting a language for a project. Language features such as performance, reliability, and simple semantics do not. Third, developers will steadily learn and forget languages. The overall number of languages developers are familiar with is independent of age. Finally, when considering intrinsic aspects of languages, developers prioritize expressivity over correctness. They perceive static types as primarily helping with the latter, hence partly explaining the popularity of dynamic languages.},
	urldate = {2015-04-29},
	booktitle = {Proceedings of the 2013 {ACM} {SIGPLAN} {International} {Conference} on {Object} {Oriented} {Programming} {Systems} {Languages} \&\#38; {Applications}},
	publisher = {ACM},
	author = {Meyerovich, Leo A. and Rabkin, Ariel S.},
	year = {2013},
	keywords = {programming language adoption, survey research},
	pages = {1--18},
	file = {meyerovich_rabkin_2013_empirical_analysis_of_programming_language_adoption.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/RE9HX8A5/meyerovich_rabkin_2013_empirical_analysis_of_programming_language_adoption.pdf:application/pdf}
}

@inproceedings{cadar_klee:_2008,
	address = {Berkeley, CA, USA},
	series = {{OSDI}'08},
	title = {{KLEE}: {Unassisted} and {Automatic} {Generation} of {High}-coverage {Tests} for {Complex} {Systems} {Programs}},
	shorttitle = {{KLEE}},
	url = {http://dl.acm.org/citation.cfm?id=1855741.1855756},
	abstract = {We present a new symbolic execution tool, KLEE, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment installed on millions of Unix systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage -- on average over 90\% per tool (median: over 94\%) -- and significantly beat the coverage of the developers' own hand-written test suite. When we did the same for 75 equivalent tools in the BUSYBOX embedded system suite, results were even better, including 100\% coverage on 31 of them. We also used KLEE as a bug finding tool, applying it to 452 applications (over 430K total lines of code), where it found 56 serious bugs, including three in COREUTILS that had been missed for over 15 years. Finally, we used KLEE to crosscheck purportedly identical BUSYBOX and COREUTILS utilities, finding functional correctness errors and a myriad of inconsistencies.},
	urldate = {2015-01-23},
	booktitle = {Proceedings of the 8th {USENIX} {Conference} on {Operating} {Systems} {Design} and {Implementation}},
	publisher = {USENIX Association},
	author = {Cadar, Cristian and Dunbar, Daniel and Engler, Dawson},
	year = {2008},
	pages = {209--224},
	file = {cadar_et_al_2008_klee.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/4DAJT3BF/cadar_et_al_2008_klee.pdf:application/pdf}
}

@inproceedings{kawaguchi_deterministic_2012,
	address = {New York, NY, USA},
	series = {{PLDI} '12},
	title = {Deterministic {Parallelism} via {Liquid} {Effects}},
	isbn = {978-1-4503-1205-9},
	url = {http://doi.acm.org/10.1145/2254064.2254071},
	doi = {10.1145/2254064.2254071},
	abstract = {Shared memory multithreading is a popular approach to parallel programming, but also fiendishly hard to get right. We present Liquid Effects, a type-and-effect system based on refinement types which allows for fine-grained, low-level, shared memory multi-threading while statically guaranteeing that a program is deterministic. Liquid Effects records the effect of an expression as a for- mula in first-order logic, making our type-and-effect system highly expressive. Further, effects like Read and Write are recorded in Liquid Effects as ordinary uninterpreted predicates, leaving the effect system open to extension by the user. By building our system as an extension to an existing dependent refinement type system, our system gains precise value- and branch-sensitive reasoning about effects. Finally, our system exploits the Liquid Types refinement type inference technique to automatically infer refinement types and effects. We have implemented our type-and-effect checking techniques in CSOLVE, a refinement type inference system for C programs. We demonstrate how CSOLVE uses Liquid Effects to prove the determinism of a variety of benchmarks.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 33rd {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Kawaguchi, Ming and Rondon, Patrick and Bakst, Alexander and Jhala, Ranjit},
	year = {2012},
	keywords = {c, dependent types, determinism, liquid types, safe parallel programming, type inference},
	pages = {45--54},
	file = {kawaguchi_et_al_2012_deterministic_parallelism_via_liquid_effects.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/PE9V5HKJ/kawaguchi_et_al_2012_deterministic_parallelism_via_liquid_effects.pdf:application/pdf}
}

@book{danvy_refocusing_2004,
	title = {Refocusing in reduction semantics},
	url = {http://repository.readscheme.org/ftp/papers/plsemantics/danvy/BRICS-RS-04-26.pdf},
	urldate = {2015-08-05},
	publisher = {BRICS, Department of Computer Science, Univ.},
	author = {Danvy, Olivier and Nielsen, Lasse R.},
	year = {2004},
	file = {danvy_nielsen_2004_refocusing_in_reduction_semantics.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/X6CTKMMS/danvy_nielsen_2004_refocusing_in_reduction_semantics.pdf:application/pdf}
}

@inproceedings{dillinger_acl2s:_2007,
	address = {Los Alamitos, CA, USA},
	title = {{ACL}2s: "{The} {ACL}2 {Sedan}"},
	volume = {0},
	isbn = {0-7695-2892-9},
	shorttitle = {{ACL}2s},
	doi = {10.1109/ICSECOMPANION.2007.14},
	booktitle = {International {Conference} on {Software} {Engineering} {Companion}},
	publisher = {IEEE Computer Society},
	author = {Dillinger, Peter C. and Manolios, Panagiotis and Vroon, Daron and Moore, J. Strother},
	year = {2007},
	pages = {59--60},
	file = {IEEE Computer Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/FJ3QKXM6/Dillinger et al. - 2007 - ACL2s The ACL2 Sedan.html:text/html}
}

@inproceedings{chitil_typeview:_2000,
	title = {Typeview: a tool for understanding type errors},
	shorttitle = {Typeview},
	abstract = {Abstract. In modern statically typed functional languages, type inference is used to determine the type of each function automatically. Whenever this fails, the compiler emits an error message that is often very complex. Sometimes the expression mentioned in the type error message is not the one that is wrong. We therefore implement an interactive tool that allows programmers to browse through the source code of their program and query the types of each expression. If a variable cannot be typed, we would like to present a set of possible types from which the user can decide which is wrong. This should help finding the origin of type errors without detailed knowledge of type inference on the user side. 1},
	booktitle = {12th {International} {Workshop} on {Implementation} of {Functional} {Languages}, {Aachner} {Informatik}-{Berichte}},
	author = {Chitil, Olaf and Huch, Frank and Simon, Axel},
	year = {2000},
	pages = {63--69},
	file = {chitil_et_al_2000_typeview.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/A6S9JF9Q/chitil_et_al_2000_typeview.pdf:application/pdf}
}

@inproceedings{wadler_comprehending_1990,
	address = {New York, NY, USA},
	series = {{LFP} '90},
	title = {Comprehending {Monads}},
	isbn = {0-89791-368-X},
	url = {http://doi.acm.org/10.1145/91556.91592},
	doi = {10.1145/91556.91592},
	abstract = {Category theorists invented monads in the 1960's to concisely express certain aspects of universal algebra. Functional programmers invented list comprehensions in the 1970's to concisely express certain programs involving lists. This paper shows how list comprehensions may be generalised to an arbitrary monad, and how the resulting programming feature can concisely express in a pure functional language some programs that manipulate state, handle exceptions, parse text, or invoke continuations. A new solution to the old problem of destructive array update is also presented. No knowledge of category theory is assumed.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 1990 {ACM} {Conference} on {LISP} and {Functional} {Programming}},
	publisher = {ACM},
	author = {Wadler, Philip},
	year = {1990},
	pages = {61--78},
	file = {wadler_1990_comprehending_monads.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/Z7GKGUNF/wadler_1990_comprehending_monads.pdf:application/pdf}
}

@misc{_mozilla_????,
	title = {Mozilla {Firefox} {Start} {Page}},
	url = {about:home},
	urldate = {2015-10-26},
	file = {Mozilla Firefox Start Page:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/NECFFVIZ/home.html:application/xhtml+xml}
}

@incollection{hughes_quickcheck_2006,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{QuickCheck} {Testing} for {Fun} and {Profit}},
	copyright = {©2007 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-69608-7 978-3-540-69611-7},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-69611-7_1},
	abstract = {One of the nice things about purely functional languages is that functions often satisfy simple properties, and enjoy simple algebraic relationships. Indeed, if the functions of an API satisfy elegant laws, that in itself is a sign of a good design—the laws not only indicate conceptual simplicity, but are useful in practice for simplifying programs that use the API, by equational reasoning or otherwise.},
	language = {en},
	number = {4354},
	urldate = {2015-01-27},
	booktitle = {Practical {Aspects} of {Declarative} {Languages}},
	publisher = {Springer Berlin Heidelberg},
	author = {Hughes, John},
	editor = {Hanus, Michael},
	year = {2006},
	keywords = {Logics and Meanings of Programs, Programming Languages, Compilers, Interpreters, Programming Techniques, Software Engineering},
	pages = {1--32},
	file = {hughes_2006_quickcheck_testing_for_fun_and_profit.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/GSFQS4D3/hughes_2006_quickcheck_testing_for_fun_and_profit.pdf:application/pdf}
}

@inproceedings{swamy_secure_2011,
	address = {New York, NY, USA},
	series = {{ICFP} '11},
	title = {Secure {Distributed} {Programming} with {Value}-dependent {Types}},
	isbn = {978-1-4503-0865-6},
	url = {http://doi.acm.org/10.1145/2034773.2034811},
	doi = {10.1145/2034773.2034811},
	abstract = {Distributed applications are difficult to program reliably and securely. Dependently typed functional languages promise to prevent broad classes of errors and vulnerabilities, and to enable program verification to proceed side-by-side with development. However, as recursion, effects, and rich libraries are added, using types to reason about programs, specifications, and proofs becomes challenging. We present F*, a full-fledged design and implementation of a new dependently typed language for secure distributed programming. Unlike prior languages, F* provides arbitrary recursion while maintaining a logically consistent core; it enables modular reasoning about state and other effects using affine types; and it supports proofs of refinement properties using a mixture of cryptographic evidence and logical proof terms. The key mechanism is a new kind system that tracks several sub-languages within F* and controls their interaction. F* subsumes two previous languages, F7 and Fine. We prove type soundness (with proofs mechanized in Coq) and logical consistency for F*. We have implemented a compiler that translates F* to .NET bytecode, based on a prototype for Fine. F* provides access to libraries for concurrency, networking, cryptography, and interoperability with C\#, F\#, and the other .NET languages. The compiler produces verifiable binaries with 60\% code size overhead for proofs and types, as much as a 45x improvement over the Fine compiler, while still enabling efficient bytecode verification. To date, we have programmed and verified more than 20,000 lines of F* including (1) new schemes for multi-party sessions; (2) a zero-knowledge privacy-preserving payment protocol; (3) a provenance-aware curated database; (4) a suite of 17 web-browser extensions verified for authorization properties; and (5) a cloud-hosted multi-tier web application with a verified reference monitor.},
	urldate = {2015-01-28},
	booktitle = {Proceedings of the 16th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Swamy, Nikhil and Chen, Juan and Fournet, Cédric and Strub, Pierre-Yves and Bhargavan, Karthikeyan and Yang, Jean},
	year = {2011},
	keywords = {refinement types, security types},
	pages = {266--278},
	file = {swamy_et_al_2011_secure_distributed_programming_with_value-dependent_types.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/ZQXQUE4B/swamy_et_al_2011_secure_distributed_programming_with_value-dependent_types.pdf:application/pdf}
}

@inproceedings{jackson_automating_2000,
	address = {New York, NY, USA},
	series = {{SIGSOFT} '00/{FSE}-8},
	title = {Automating {First}-order {Relational} {Logic}},
	isbn = {1-58113-205-0},
	url = {http://doi.acm.org/10.1145/355045.355063},
	doi = {10.1145/355045.355063},
	abstract = {An automatic analysis method for first-order logic with sets and relations is described. A first-order formula is translated to a quantifier-free boolean formula, which has a model when the original formula has a model within a given scope (that is, involving no more than some finite number of atoms). Because the satisfiable formulas that occur in practice tend to have small models, a small scope usually suffices and the analysis is efficient.
The paper presents a simple logic and gives a compositional translation scheme. It also reports briefly on experience using the Alloy Analyzer, a tool that implements the scheme.},
	urldate = {2015-01-27},
	booktitle = {Proceedings of the 8th {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}: {Twenty}-first {Century} {Applications}},
	publisher = {ACM},
	author = {Jackson, Daniel},
	year = {2000},
	keywords = {automatic analysis, constraint solvers, first-order logic, model finding, object models, relational logic, SAT solvers, Z specification},
	pages = {130--139},
	file = {jackson_2000_automating_first-order_relational_logic.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/3N4JHJDQ/jackson_2000_automating_first-order_relational_logic.pdf:application/pdf}
}

@inproceedings{vazou_bounded_2015,
	address = {New York, NY, USA},
	series = {{ICFP} 2015},
	title = {Bounded {Refinement} {Types}},
	isbn = {978-1-4503-3669-7},
	url = {http://doi.acm.org/10.1145/2784731.2784745},
	doi = {10.1145/2784731.2784745},
	abstract = {We present a notion of bounded quantification for refinement types and show how it expands the expressiveness of refinement typing by using it to develop typed combinators for: (1) relational algebra and safe database access, (2) Floyd-Hoare logic within a state transformer monad equipped with combinators for branching and looping, and (3) using the above to implement a refined IO monad that tracks capabilities and resource usage. This leap in expressiveness comes via a translation to ``ghost" functions, which lets us retain the automated and decidable SMT based checking and inference that makes refinement typing effective in practice.},
	urldate = {2015-09-03},
	booktitle = {Proceedings of the 20th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Vazou, Niki and Bakst, Alexander and Jhala, Ranjit},
	year = {2015},
	keywords = {abstract interpretation, haskell, refinement types},
	pages = {48--61},
	file = {vazou_et_al_2015_bounded_refinement_types.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/ZFAFAHSU/vazou_et_al_2015_bounded_refinement_types.pdf:application/pdf}
}

@inproceedings{claessen_quickcheck:_2000,
	address = {New York, NY, USA},
	series = {{ICFP} '00},
	title = {{QuickCheck}: {A} {Lightweight} {Tool} for {Random} {Testing} of {Haskell} {Programs}},
	isbn = {1-58113-202-6},
	shorttitle = {{QuickCheck}},
	url = {http://doi.acm.org/10.1145/351240.351266},
	doi = {10.1145/351240.351266},
	abstract = {Quick Check is a tool which aids the Haskell programmer in formulating and testing properties of programs. Properties are described as Haskell functions, and can be automatically tested on random input, but it is also possible to define custom test data generators. We present a number of case studies, in which the tool was successfully used, and also point out some pitfalls to avoid. Random testing is especially suitable for functional programs because properties can be stated at a fine grain. When a function is built from separately tested components, then random testing suffices to obtain good coverage of the definition under test.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the {Fifth} {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Claessen, Koen and Hughes, John},
	year = {2000},
	pages = {268--279},
	file = {claessen_hughes_2000_quickcheck.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/25N4HZQJ/claessen_hughes_2000_quickcheck.pdf:application/pdf}
}

@inproceedings{gabriel_ultra-large-scale_2006,
	address = {New York, NY, USA},
	series = {{OOPSLA} '06},
	title = {Ultra-large-scale {Systems}},
	isbn = {1-59593-491-X},
	url = {http://doi.acm.org/10.1145/1176617.1176645},
	doi = {10.1145/1176617.1176645},
	abstract = {Scale changes everything. The trend in the design and development of software-intensive systems today is toward scale that increases in every measurable way. Lines of code, complexity, dependency, communication, bandwidth, memory, datasets, and many other measures for our systems continue to reach and exceed the limits of our ability to produce high-quality systems for all purposes.These systems will be unbounded, integrating internet-scale resources. They will serve diverse stakeholders with competing objectives and at the same time be constrained by policy, regulation, and the behaviors of their users. The lines between development, acquisition, and operations will blur: ULS systems will not die; they will be too large to be replaced and will be inextricably connected to the day-to-day mission. Rather, they will continue to evolve over time with behavior often more emergent than planned. Because complete specifications will not be achievable, sufficient assurance will have to do. ULS systems present "wicked problems," ones for which each attempt to create a solution changes the problem. Some of these characteristics appear in conventional systems, but in ULS systems they will dominate.},
	urldate = {2015-06-26},
	booktitle = {Companion to the 21st {ACM} {SIGPLAN} {Symposium} on {Object}-oriented {Programming} {Systems}, {Languages}, and {Applications}},
	publisher = {ACM},
	author = {Gabriel, Richard P. and Northrop, Linda and Schmidt, Douglas C. and Sullivan, Kevin},
	year = {2006},
	keywords = {design, methodology, systems, ultra-large-scale},
	pages = {632--634},
	file = {gabriel_et_al_2006_ultra-large-scale_systems.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/G5TZD65X/gabriel_et_al_2006_ultra-large-scale_systems.pdf:application/pdf}
}

@article{thompson_functional_1993,
	title = {Functional programming in education – {Introduction}},
	volume = {3},
	issn = {1469-7653},
	url = {http://journals.cambridge.org/article_S0956796800000563},
	doi = {10.1017/S0956796800000563},
	number = {01},
	urldate = {2015-03-04},
	journal = {Journal of Functional Programming},
	author = {Thompson, Simon and Wadler, Philip},
	month = jan,
	year = {1993},
	keywords = {education, functional programming},
	pages = {3--4},
	file = {thompson_wadler_1993_functional_programming_in_education_–_introduction.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/2986CQIH/thompson_wadler_1993_functional_programming_in_education_–_introduction.pdf:application/pdf}
}

@inproceedings{adams_optimizing_2014,
	address = {New York, NY, USA},
	series = {{PEPM} '14},
	title = {Optimizing {SYB} is {Easy}!},
	isbn = {978-1-4503-2619-3},
	url = {http://doi.acm.org/10.1145/2543728.2543730},
	doi = {10.1145/2543728.2543730},
	abstract = {The most widely used generic-programming system in the Haskell community, Scrap Your Boilerplate (SYB), also happens to be one of the slowest. Generic traversals in SYB are often an order of magnitude slower than equivalent handwritten, non-generic traversals. Thus while SYB allows the concise expression of many traversals, its use incurs a significant runtime cost. Existing techniques for optimizing other generic-programming systems are not able to eliminate this overhead. This paper presents an optimization that completely eliminates this cost. Essentially, it is a partial evaluation that takes advantage of domain-specific knowledge about the structure of SYB. It optimizes SYB-style traversals to be as fast as handwritten, non-generic code, and benchmarks show that this optimization improves the speed of SYB-style code by an order of magnitude or more.},
	urldate = {2015-05-21},
	booktitle = {Proceedings of the {ACM} {SIGPLAN} 2014 {Workshop} on {Partial} {Evaluation} and {Program} {Manipulation}},
	publisher = {ACM},
	author = {Adams, Michael D. and Farmer, Andrew and Magalhães, José Pedro},
	year = {2014},
	keywords = {datatype-generic programming, haskell, keywords: optimization, partial evaluation, performance, scrap your boilerplate (syb)},
	pages = {71--82},
	file = {adams_et_al_2014_optimizing_syb_is_easy.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/HZTZ2GR5/adams_et_al_2014_optimizing_syb_is_easy.pdf:application/pdf}
}

@inproceedings{yang_automatically_2006,
	title = {Automatically generating malicious disks using symbolic execution},
	doi = {10.1109/SP.2006.7},
	abstract = {Many current systems allow data produced by potentially malicious sources to be mounted as a file system. File system code must check this data for dangerous values or invariant violations before using it. Because file system code typically runs inside the operating system kernel, even a single unchecked value can crash the machine or lead to an exploit. Unfortunately, validating file system images is complex: they form DAGs with complex dependency relationships across massive amounts of data bound together with intricate, undocumented assumptions. This paper shows how to automatically find bugs in such code using symbolic execution. Rather than running the code on manually-constructed concrete input, we instead run it on symbolic input that is initially allowed to be "anything." As the code runs, it observes (tests) this input and thus constrains its possible values. We generate test cases by solving these constraints for concrete values. The approach works well in practice: we checked the disk mounting code of three widely-used Linux file systems: ext2, ext3, and JFS and found bugs in all of them where malicious data could either cause a kernel panic or form the basis of a buffer overflow attack},
	booktitle = {2006 {IEEE} {Symposium} on {Security} and {Privacy}},
	author = {Yang, Junfeng and Sar, Can and Twohey, P. and Cadar, C. and Engler, D.},
	month = may,
	year = {2006},
	keywords = {Buffer overflow, buffer overflow attack, Computer bugs, Computer crashes, computer viruses, Concrete, disk mounting code, ext2 file system, ext3 file system, file system code, file system image validation, File systems, invariant violations, Kernel, Laboratories, Linux, Linux file systems, malicious disk automatic generation, operating system kernel, Operating Systems, program debugging, symbolic execution, Testing},
	pages = {15 pp.--257},
	file = {yang_et_al_2006_automatically_generating_malicious_disks_using_symbolic_execution.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/E9I9BG2W/yang_et_al_2006_automatically_generating_malicious_disks_using_symbolic_execution.pdf:application/pdf}
}

@article{olmer_evaluating_2014,
	title = {Evaluating {Haskell} expressions in a tutoring environment},
	volume = {170},
	issn = {2075-2180},
	url = {http://arxiv.org/abs/1412.4879v1},
	doi = {10.4204/EPTCS.170.4},
	language = {en},
	urldate = {2015-09-25},
	journal = {Electronic Proceedings in Theoretical Computer Science},
	author = {Olmer, Tim and Heeren, Bastiaan and Jeuring, Johan},
	month = dec,
	year = {2014},
	keywords = {Computer Science - Computers and Society, Computer Science - Programming Languages},
	pages = {50--66},
	file = {arXiv.org Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/6KC5EIXN/1412.html:text/html;olmer_et_al_2014_evaluating_haskell_expressions_in_a_tutoring_environment.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/ZVQ5V4AS/olmer_et_al_2014_evaluating_haskell_expressions_in_a_tutoring_environment.pdf:application/pdf;olmer_et_al_2014_evaluating_haskell_expressions_in_a_tutoring_environment.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/745CU7AD/olmer_et_al_2014_evaluating_haskell_expressions_in_a_tutoring_environment.pdf:application/pdf}
}

@article{ishii_report_2014,
	title = {Report on a {User} {Test} and {Extension} of a {Type} {Debugger} for {Novice} {Programmers}},
	volume = {170},
	issn = {2075-2180},
	url = {http://arxiv.org/abs/1412.4877},
	doi = {10.4204/EPTCS.170.1},
	abstract = {A type debugger interactively detects the expressions that cause type errors. It asks users whether they intend the types of identifiers to be those that the compiler inferred. However, it seems that novice programmers often get in trouble when they think about how to fix type errors by reading the messages given by the type debugger. In this paper, we analyze the user tests of a type debugger and report problems of the current type debugger. We then extend the type debugger to address these problems. Specifically, we introduce expression-specific error messages and language levels. Finally, we show type errors that we think are difficult to explain to novice programmers. The subjects of the user tests were 40 novice students belonging to the department of information science at Ochanomizu University.},
	urldate = {2015-10-29},
	journal = {Electronic Proceedings in Theoretical Computer Science},
	author = {Ishii, Yuki and Asai, Kenichi},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.4877},
	keywords = {Computer Science - Programming Languages, Computer Science - Software Engineering, D.2.5},
	pages = {1--18},
	file = {arXiv.org Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/X5JF6EUD/1412.html:text/html;ishii_asai_2014_report_on_a_user_test_and_extension_of_a_type_debugger_for_novice_programmers.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/ZTFTPHZV/ishii_asai_2014_report_on_a_user_test_and_extension_of_a_type_debugger_for_novice_programmers.pdf:application/pdf}
}

@inproceedings{ramsey_teaching_2014,
	address = {New York, NY, USA},
	series = {{ICFP} '14},
	title = {On {Teaching} *{How} to {Design} {Programs}*: {Observations} from a {Newcomer}},
	isbn = {978-1-4503-2873-9},
	shorttitle = {On {Teaching} *{How} to {Design} {Programs}*},
	url = {http://doi.acm.org/10.1145/2628136.2628137},
	doi = {10.1145/2628136.2628137},
	abstract = {This paper presents a personal, qualitative case study of a first course using How to Design Programs and its functional teaching languages. The paper reconceptualizes the book's six-step design process as an eight-step design process ending in a new "review and refactor" step. It recommends specific approaches to students' difficulties with function descriptions, function templates, data examples, and other parts of the design process. It connects the process to interactive "world programs." It recounts significant, informative missteps in course design and delivery. Finally, it identifies some unsolved teaching problems and some potential solutions.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 19th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Ramsey, Norman},
	year = {2014},
	keywords = {how to design programs, introductory programming course, program by design, racket, reflective practice},
	pages = {153--166},
	file = {ramsey_2014_on_teaching_how_to_design_programs.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/NVQTGSG6/ramsey_2014_on_teaching_how_to_design_programs.pdf:application/pdf}
}

@article{cadar_symbolic_2013,
	title = {Symbolic {Execution} for {Software} {Testing}: {Three} {Decades} {Later}},
	volume = {56},
	issn = {0001-0782},
	shorttitle = {Symbolic {Execution} for {Software} {Testing}},
	url = {http://doi.acm.org/10.1145/2408776.2408795},
	doi = {10.1145/2408776.2408795},
	abstract = {The challenges---and great promise---of modern symbolic execution techniques, and the tools to help implement them.},
	number = {2},
	urldate = {2015-01-22},
	journal = {Commun. ACM},
	author = {Cadar, Cristian and Sen, Koushik},
	month = feb,
	year = {2013},
	pages = {82--90},
	file = {cadar_sen_2013_symbolic_execution_for_software_testing.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/CPHVWKMB/cadar_sen_2013_symbolic_execution_for_software_testing.pdf:application/pdf}
}

@misc{edwards_two-way_????,
	title = {Two-way {Dataflow}},
	url = {https://vimeo.com/106073134},
	abstract = {Subtext is an experiment to radically simplify application programming. The goal is to combine the power of frameworks like Rails and iOS with the simplicity of…},
	urldate = {2015-09-17},
	author = {Edwards, Johnathan},
	file = {Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/FZVIKVCB/106073134.html:text/html}
}

@inproceedings{elliott_guilt_2015,
	address = {New York, NY, USA},
	series = {Haskell 2015},
	title = {Guilt {Free} {Ivory}},
	isbn = {978-1-4503-3808-0},
	url = {http://doi.acm.org/10.1145/2804302.2804318},
	doi = {10.1145/2804302.2804318},
	abstract = {Ivory is a language that enforces memory safety and avoids most undefined behaviors while providing low-level control of memory- manipulation. Ivory is embedded in a modern variant of Haskell, as implemented by the GHC compiler. The main contributions of the paper are two-fold. First, we demonstrate how to embed the type-system of a safe-C language into the type extensions of GHC. Second, Ivory is of interest in its own right, as a powerful language for writing high-assurance embedded programs. Beyond invariants enforced by its type-system, Ivory has direct support for model-checking, theorem-proving, and property-based testing. Ivory’s semantics have been formalized and proved to guarantee memory safety.},
	urldate = {2015-09-03},
	booktitle = {Proceedings of the 8th {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Elliott, Trevor and Pike, Lee and Winwood, Simon and Hickey, Pat and Bielman, James and Sharp, Jamey and Seidel, Eric and Launchbury, John},
	year = {2015},
	keywords = {embedded domain specific languages, embedded systems},
	pages = {189--200},
	file = {elliott_et_al_2015_guilt_free_ivory.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/TFEIBWWH/elliott_et_al_2015_guilt_free_ivory.pdf:application/pdf}
}

@inproceedings{cook_understanding_2009,
	address = {New York, NY, USA},
	series = {{OOPSLA} '09},
	title = {On {Understanding} {Data} {Abstraction}, {Revisited}},
	isbn = {978-1-60558-766-0},
	url = {http://doi.acm.org/10.1145/1640089.1640133},
	doi = {10.1145/1640089.1640133},
	abstract = {In 1985 Luca Cardelli and Peter Wegner, my advisor, published an ACM Computing Surveys paper called "On understanding types, data abstraction, and polymorphism". Their work kicked off a flood of research on semantics and type theory for object-oriented programming, which continues to this day. Despite 25 years of research, there is still widespread confusion about the two forms of data abstraction, abstract data types and objects. This essay attempts to explain the differences and also why the differences matter.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 24th {ACM} {SIGPLAN} {Conference} on {Object} {Oriented} {Programming} {Systems} {Languages} and {Applications}},
	publisher = {ACM},
	author = {Cook, William R.},
	year = {2009},
	keywords = {abstract data type, ADT, class, object},
	pages = {557--572},
	file = {cook_2009_on_understanding_data_abstraction,_revisited.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/89EQDVEK/cook_2009_on_understanding_data_abstraction,_revisited.pdf:application/pdf}
}

@inproceedings{bierman_essence_2002,
	title = {The essence of data access in cω: {The} power is in the dot},
	shorttitle = {The essence of data access in cω},
	abstract = {Abstract. In this paper we describe the data access features of Cω, an experimental programming language based on C ♯ currently under development at Microsoft Research. Cω targets distributed, data-intensive applications and accordingly extends C ♯ ’s support of both data and control. In the data dimension it provides a type-theoretic integration of the three prevalent data models, namely the object, relational, and semi-structured models of data. In the control dimension Cω provides elegant primitives for asynchronous communication. In this paper we concentrate on the data dimension. Our aim is to describe the essence of these extensions; by which we mean we identify, exemplify and formalize their essential features. Our tool is a small core language, FCω, which is a valid subset of the full Cω language. Using this core language we are able to formalize both the type system and the operational semantics of the data access fragment of Cω. 1},
	booktitle = {In {ECOOP} ’02},
	author = {Bierman, Gavin and Meijer, Erik and Schulte, Wolfram},
	year = {2002},
	file = {bierman_et_al_2002_the_essence_of_data_access_in_cω.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/JG8GNT4V/bierman_et_al_2002_the_essence_of_data_access_in_cω.pdf:application/pdf}
}

@inproceedings{torlak_growing_2013,
	address = {New York, NY, USA},
	series = {Onward! '13},
	title = {Growing {Solver}-aided {Languages} with {Rosette}},
	isbn = {978-1-4503-2472-4},
	url = {http://doi.acm.org/10.1145/2509578.2509586},
	doi = {10.1145/2509578.2509586},
	urldate = {2014-11-10},
	booktitle = {Proceedings of the 2013 {ACM} {International} {Symposium} on {New} {Ideas}, {New} {Paradigms}, and {Reflections} on {Programming} \&\#38; {Software}},
	publisher = {ACM},
	author = {Torlak, Emina and Bodik, Rastislav},
	year = {2013},
	keywords = {languages, solver-aided},
	pages = {135--152},
	file = {torlak_bodik_2013_growing_solver-aided_languages_with_rosette.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/7ER9DNQ3/torlak_bodik_2013_growing_solver-aided_languages_with_rosette.pdf:application/pdf}
}

@inproceedings{mainland_explicitly_2012,
	address = {New York, NY, USA},
	series = {{ICFP} '12},
	title = {Explicitly {Heterogeneous} {Metaprogramming} with {MetaHaskell}},
	isbn = {978-1-4503-1054-3},
	url = {http://doi.acm.org/10.1145/2364527.2364572},
	doi = {10.1145/2364527.2364572},
	abstract = {Languages with support for metaprogramming, like MetaOCaml, offer a principled approach to code generation by guaranteeing that well-typed metaprograms produce well-typed programs. However, many problem domains where metaprogramming can fruitfully be applied require generating code in languages like C, CUDA, or assembly. Rather than resorting to add-hoc code generation techniques, these applications should be directly supported by explicitly heterogeneous metaprogramming languages. We present MetaHaskell, an extension of Haskell 98 that provides modular syntactic and type system support for type safe metaprogramming with multiple object languages. Adding a new object language to MetaHaskell requires only minor modifications to the host language to support type-level quantification over object language types and propagation of type equality constraints. We demonstrate the flexibility of our approach through three object languages: a core ML language, a linear variant of the core ML language, and a subset of C. All three languages support metaprogramming with open terms and guarantee that well-typed MetaHaskell programs will only produce closed object terms that are well-typed. The essence of MetaHaskell is captured in a type system for a simplified metalanguage. MetaHaskell, as well as all three object languages, are fully implemented in the mhc bytecode compiler.},
	urldate = {2015-01-08},
	booktitle = {Proceedings of the 17th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Mainland, Geoffrey},
	year = {2012},
	keywords = {linear languages, metaprogramming, open terms, quasiquotation, type systems},
	pages = {311--322},
	file = {mainland_2012_explicitly_heterogeneous_metaprogramming_with_metahaskell.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/546RAQ2N/mainland_2012_explicitly_heterogeneous_metaprogramming_with_metahaskell.pdf:application/pdf}
}

@article{chambers_function_2012,
	title = {The {Function}, and {Dysfunction}, of {Information} {Sources} in {Learning} {Functional} {Programming}},
	volume = {28},
	issn = {1937-4771},
	url = {http://dl.acm.org/citation.cfm?id=2379703.2379745},
	abstract = {Programmers experienced in using imperative languages can increasingly benefit from also knowing how to use functional languages. However, even if programmers have already mastered general programming constructs such as types and recursion, actually expressing these in a functional language can be challenging. In this paper, we present an observational study investigating what information sources imperative programmers use when they encounter these problems, as well as how well different information sources enable them to overcome problems. By highlighting the central role that external information sources play as students learn functional programming, our results reveal opportunities for more effectively supporting the learning process.},
	number = {1},
	urldate = {2015-10-22},
	journal = {J. Comput. Sci. Coll.},
	author = {Chambers, Christopher and Chen, Sheng and Le, Duc and Scaffidi, Christopher},
	month = oct,
	year = {2012},
	pages = {220--226},
	file = {chambers_et_al_2012_the_function,_and_dysfunction,_of_information_sources_in_learning_functional.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/QQDAFTBV/chambers_et_al_2012_the_function,_and_dysfunction,_of_information_sources_in_learning_functional.pdf:application/pdf}
}

@inproceedings{lerner_searching_2007,
	address = {New York, NY, USA},
	series = {{PLDI} '07},
	title = {Searching for {Type}-error {Messages}},
	isbn = {978-1-59593-633-2},
	url = {http://doi.acm.org/10.1145/1250734.1250783},
	doi = {10.1145/1250734.1250783},
	abstract = {Advanced type systems often need some form of type inference to reduce the burden of explicit typing, but type inference often leads to poor error messages for ill-typed programs. This work pursues a new approach to constructing compilers and presenting type-error messages in which the type-checker itself does not produce the messages. Instead, it is an oracle for a search procedure that finds similar programs that do type-check. Our two-fold goal is to improve error messages while simplifying compiler construction. Our primary implementation and evaluation is for Caml, a language with full type inference. We also present a prototype for C++ template functions, where type instantiation is implicit. A key extension is making our approach robust even when the program has multiple independent type errors.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 28th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Lerner, Benjamin S. and Flower, Matthew and Grossman, Dan and Chambers, Craig},
	year = {2007},
	keywords = {error messages, objective Caml, seminal, type-checking, type-inference},
	pages = {425--434},
	file = {lerner_et_al_2007_searching_for_type-error_messages.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/FMIQF8AW/lerner_et_al_2007_searching_for_type-error_messages.pdf:application/pdf}
}

@inproceedings{zhu_learning_2015,
	address = {New York, NY, USA},
	series = {{ICFP} 2015},
	title = {Learning {Refinement} {Types}},
	isbn = {978-1-4503-3669-7},
	url = {http://doi.acm.org/10.1145/2784731.2784766},
	doi = {10.1145/2784731.2784766},
	abstract = {We propose the integration of a random test generation system (capable of discovering program bugs) and a refinement type system (capable of expressing and verifying program invariants), for higher-order functional programs, using a novel lightweight learning algorithm as an effective intermediary between the two. Our approach is based on the well-understood intuition that useful, but difficult to infer, program properties can often be observed from concrete program states generated by tests; these properties act as likely invariants, which if used to refine simple types, can have their validity checked by a refinement type checker. We describe an implementation of our technique for a variety of benchmarks written in ML, and demonstrate its effectiveness in inferring and proving useful invariants for programs that express complex higher-order control and dataflow.},
	urldate = {2015-09-03},
	booktitle = {Proceedings of the 20th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Zhu, He and Nori, Aditya V. and Jagannathan, Suresh},
	year = {2015},
	keywords = {Higher-Order Verification, Learning, refinement types, Testing},
	pages = {400--411},
	file = {Learning Dependent Types from Tests.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/3Z6KE9XV/Learning Dependent Types from Tests.pdf:application/pdf;zhu_et_al_2015_learning_refinement_types.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/7KSDHUTJ/zhu_et_al_2015_learning_refinement_types.pdf:application/pdf}
}

@article{leroy_coinductive_2009,
	series = {Special issue on {Structural} {Operational} {Semantics} ({SOS})},
	title = {Coinductive big-step operational semantics},
	volume = {207},
	issn = {0890-5401},
	url = {http://www.sciencedirect.com/science/article/pii/S0890540108001296},
	doi = {10.1016/j.ic.2007.12.004},
	abstract = {Using a call-by-value functional language as an example, this article illustrates the use of coinductive definitions and proofs in big-step operational semantics, enabling it to describe diverging evaluations in addition to terminating evaluations. We formalize the connections between the coinductive big-step semantics and the standard small-step semantics, proving that both semantics are equivalent. We then study the use of coinductive big-step semantics in proofs of type soundness and proofs of semantic preservation for compilers. A methodological originality of this paper is that all results have been proved using the Coq proof assistant. We explain the proof-theoretic presentation of coinductive definitions and proofs offered by Coq, and show that it facilitates the discovery and the presentation of the results.},
	number = {2},
	urldate = {2015-08-04},
	journal = {Information and Computation},
	author = {Leroy, Xavier and Grall, Hervé},
	month = feb,
	year = {2009},
	keywords = {Big-step semantics, Coinduction, Compiler correctness, Mechanized proofs, Natural semantics, Operational semantics, Reduction semantics, Small-step semantics, The Coq proof assistant, Type soundness},
	pages = {284--304},
	file = {ScienceDirect Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/XP3P825T/Leroy and Grall - 2009 - Coinductive big-step operational semantics.html:text/html}
}

@inproceedings{vytiniotis_equality_2012,
	address = {New York, NY, USA},
	series = {{ICFP} '12},
	title = {Equality {Proofs} and {Deferred} {Type} {Errors}: {A} {Compiler} {Pearl}},
	isbn = {978-1-4503-1054-3},
	shorttitle = {Equality {Proofs} and {Deferred} {Type} {Errors}},
	url = {http://doi.acm.org/10.1145/2364527.2364554},
	doi = {10.1145/2364527.2364554},
	abstract = {The Glasgow Haskell Compiler is an optimizing compiler that expresses and manipulates first-class equality proofs in its intermediate language. We describe a simple, elegant technique that exploits these equality proofs to support deferred type errors. The technique requires us to treat equality proofs as possibly-divergent terms; we show how to do so without losing either soundness or the zero-overhead cost model that the programmer expects.},
	urldate = {2015-10-27},
	booktitle = {Proceedings of the 17th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Vytiniotis, Dimitrios and Peyton Jones, Simon and Magalhães, José Pedro},
	year = {2012},
	keywords = {deferred type errors, system fc, type equalities},
	pages = {341--352},
	file = {vytiniotis_et_al_2012_equality_proofs_and_deferred_type_errors.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/38JQ79NW/vytiniotis_et_al_2012_equality_proofs_and_deferred_type_errors.pdf:application/pdf}
}

@article{kishon_semantics_1995,
	title = {Semantics directed program execution monitoring},
	volume = {5},
	issn = {1469-7653},
	url = {http://journals.cambridge.org/article_S0956796800001465},
	doi = {10.1017/S0956796800001465},
	abstract = {Monitoring semantics is a formal model of program execution which captures ‘monitoring activity’ as found in profilers, tracers, debuggers, etc. Beyond its theoretical interest, this formalism provides a new methodology for implementing a large family of source-level monitoring activities for sequential deterministic programming languages. In this article we explore the use of monitoring semantics in the specification and implementation of a variety of monitors: profilers, tracers, collecting interpreters, and, most importantly, interactive source-level debuggers. Although we consider such monitors only for (both strict and non-strict) functional languages, the methodology extends easily to imperative languages, since it begins with a continuation semantics specification.In addition, using standard partial evaluation techniques as an optimization strategy, we show that the methodology forms a practical basis for building real monitors. Our system can be optimized at two levels of specialization: specializing the interpreter with respect to a monitor specification automatically yields an instrumented interpreter; further specializing this instrumented interpreter with respect to a source program yields an instrumented program, i.e. one in which the extra code to perform monitoring has been automatically embedded into the program.},
	number = {04},
	urldate = {2015-10-29},
	journal = {Journal of Functional Programming},
	author = {Kishon, Amir and Hudak, Paul},
	year = {1995},
	pages = {501--547},
	file = {Cambridge Journals Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/I2JMT7GU/displayAbstract.html:text/html;kishon_hudak_1995_semantics_directed_program_execution_monitoring.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/KAEEKP2K/kishon_hudak_1995_semantics_directed_program_execution_monitoring.pdf:application/pdf}
}

@inproceedings{gibbons_just_2011,
	address = {New York, NY, USA},
	series = {{ICFP} '11},
	title = {Just {Do} {It}: {Simple} {Monadic} {Equational} {Reasoning}},
	isbn = {978-1-4503-0865-6},
	shorttitle = {Just {Do} {It}},
	url = {http://doi.acm.org/10.1145/2034773.2034777},
	doi = {10.1145/2034773.2034777},
	abstract = {One of the appeals of pure functional programming is that it is so amenable to equational reasoning. One of the problems of pure functional programming is that it rules out computational effects. Moggi and Wadler showed how to get round this problem by using monads to encapsulate the effects, leading in essence to a phase distinction - a pure functional evaluation yielding an impure imperative computation. Still, it has not been clear how to reconcile that phase distinction with the continuing appeal of functional programming; does the impure imperative part become inaccessible to equational reasoning? We think not; and to back that up, we present a simple axiomatic approach to reasoning about programs with computational effects.},
	urldate = {2015-05-21},
	booktitle = {Proceedings of the 16th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Gibbons, Jeremy and Hinze, Ralf},
	year = {2011},
	keywords = {algebraic specification, equational reasoning, lawvere theories, monads},
	pages = {2--14},
	file = {gibbons_hinze_2011_just_do_it.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/Z7QW7DT2/gibbons_hinze_2011_just_do_it.pdf:application/pdf}
}

@inproceedings{ko_designing_2004,
	address = {New York, NY, USA},
	series = {{CHI} '04},
	title = {Designing the {Whyline}: {A} {Debugging} {Interface} for {Asking} {Questions} {About} {Program} {Behavior}},
	isbn = {1-58113-702-8},
	shorttitle = {Designing the {Whyline}},
	url = {http://doi.acm.org/10.1145/985692.985712},
	doi = {10.1145/985692.985712},
	abstract = {Debugging is still among the most common and costly of programming activities. One reason is that current debugging tools do not directly support the inquisitive nature of the activity. Interrogative Debugging is a new debugging paradigm in which programmers can ask why did and even why didn't questions directly about their program's runtime failures. The Whyline is a prototype Interrogative Debugging interface for the Alice programming environment that visualizes answers in terms of runtime events directly relevant to a programmer's question. Comparisons of identical debugging scenarios from user tests with and without the Whyline showed that the Whyline reduced debugging time by nearly a factor of 8, and helped programmers complete 40\% more tasks.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Ko, Andrew J. and Myers, Brad A.},
	year = {2004},
	keywords = {Alice, debugging, program slicing},
	pages = {151--158},
	file = {ko_myers_2004_designing_the_whyline.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/QC9D6U8I/ko_myers_2004_designing_the_whyline.pdf:application/pdf}
}

@inproceedings{hindle_naturalness_2012,
	address = {Piscataway, NJ, USA},
	series = {{ICSE} '12},
	title = {On the {Naturalness} of {Software}},
	isbn = {978-1-4673-1067-3},
	url = {http://dl.acm.org/citation.cfm?id=2337223.2337322},
	abstract = {Natural languages like English are rich, complex, and powerful. The highly creative and graceful use of languages like English and Tamil, by masters like Shakespeare and Avvaiyar, can certainly delight and inspire. But in practice, given cognitive constraints and the exigencies of daily life, most human utterances are far simpler and much more repetitive and predictable. In fact, these utterances can be very usefully modeled using modern statistical methods. This fact has led to the phenomenal success of statistical approaches to speech recognition, natural language translation, question-answering, and text mining and comprehension. We begin with the conjecture that most software is also natural, in the sense that it is created by humans at work, with all the attendant constraints and limitations---and thus, like natural language, it is also likely to be repetitive and predictable. We then proceed to ask whether a) code can be usefully modeled by statistical language models and b) such models can be leveraged to support software engineers. Using the widely adopted n-gram model, we provide empirical evidence supportive of a positive answer to both these questions. We show that code is also very repetitive, and in fact even more so than natural languages. As an example use of the model, we have developed a simple code completion engine for Java that, despite its simplicity, already improves Eclipse's completion capability. We conclude the paper by laying out a vision for future research in this area.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Hindle, Abram and Barr, Earl T. and Su, Zhendong and Gabel, Mark and Devanbu, Premkumar},
	year = {2012},
	pages = {837--847},
	file = {hindle_et_al_2012_on_the_naturalness_of_software.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/TFXNRHHR/hindle_et_al_2012_on_the_naturalness_of_software.pdf:application/pdf}
}

@inproceedings{lindblad_property_2007,
	series = {Trends in {Functional} {Programming}},
	title = {Property {Directed} {Generation} of {First}-{Order} {Test} {Data}},
	volume = {8},
	isbn = {978-1-84150-196-3},
	booktitle = {Proceedings of the {Eighth} {Symposium} on {Trends} in {Functional} {Programming}, {TFP} 2007, {New} {York} {City}, {New} {York}, {USA}, {April} 2-4. 2007},
	publisher = {Intellect},
	author = {Lindblad, Fredrik},
	editor = {Morazán, Marco T.},
	year = {2007},
	keywords = {\_tablet, research-exam},
	pages = {105--123},
	file = {lindblad_2007_property_directed_generation_of_first-order_test_data.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/QJPQ3XB5/lindblad_2007_property_directed_generation_of_first-order_test_data.pdf:application/pdf}
}

@inproceedings{ishii_tangible_1997,
	address = {New York, NY, USA},
	series = {{CHI} '97},
	title = {Tangible {Bits}: {Towards} {Seamless} {Interfaces} {Between} {People}, {Bits} and {Atoms}},
	isbn = {0-89791-802-9},
	shorttitle = {Tangible {Bits}},
	url = {http://doi.acm.org/10.1145/258549.258715},
	doi = {10.1145/258549.258715},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the {ACM} {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Ishii, Hiroshi and Ullmer, Brygg},
	year = {1997},
	keywords = {ambient media, augmented reality, center and periphery, foreground and background, graspable user interface, tangible user interface, ubiquitous computing},
	pages = {234--241},
	file = {ishii_ullmer_1997_tangible_bits.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/M2DC8VPC/ishii_ullmer_1997_tangible_bits.pdf:application/pdf}
}

@incollection{barnett_spec_2005,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {The {Spec}\# {Programming} {System}: {An} {Overview}},
	copyright = {©2005 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-24287-1 978-3-540-30569-9},
	shorttitle = {The {Spec}\# {Programming} {System}},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-30569-9_3},
	abstract = {The Spec\# programming system is a new attempt at a more cost effective way to develop and maintain high-quality software. This paper describes the goals and architecture of the Spec\# programming system, consisting of the object-oriented Spec\# programming language, the Spec\# compiler, and the Boogie static program verifier. The language includes constructs for writing specifications that capture programmer intentions about how methods and data are to be used, the compiler emits run-time checks to enforce these specifications, and the verifier can check the consistency between a program and its specifications.},
	language = {en},
	number = {3362},
	urldate = {2015-06-26},
	booktitle = {Construction and {Analysis} of {Safe}, {Secure}, and {Interoperable} {Smart} {Devices}},
	publisher = {Springer Berlin Heidelberg},
	author = {Barnett, Mike and Leino, K. Rustan M. and Schulte, Wolfram},
	editor = {Barthe, Gilles and Burdy, Lilian and Huisman, Marieke and Lanet, Jean-Louis and Muntean, Traian},
	year = {2005},
	keywords = {Logics and Meanings of Programs, Operating Systems, Programming Languages, Compilers, Interpreters, Programming Techniques, Software Engineering, Special Purpose and Application-Based Systems},
	pages = {49--69},
	file = {barnett_et_al_2005_the_spec#_programming_system.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/8FCBAIGV/barnett_et_al_2005_the_spec#_programming_system.pdf:application/pdf}
}

@book{jackson_software_2006,
	title = {Software {Abstractions}: {Logic}, {Language}, and {Analysis}},
	isbn = {0-262-10114-9},
	shorttitle = {Software {Abstractions}},
	publisher = {The MIT Press},
	author = {Jackson, Daniel},
	year = {2006}
}

@inproceedings{stolarek_injective_2015,
	address = {New York, NY, USA},
	series = {Haskell 2015},
	title = {Injective {Type} {Families} for {Haskell}},
	isbn = {978-1-4503-3808-0},
	url = {http://doi.acm.org/10.1145/2804302.2804314},
	doi = {10.1145/2804302.2804314},
	abstract = {Haskell, as implemented by the Glasgow Haskell Compiler (GHC), allows expressive type-level programming. The most popular type-level programming extension is TypeFamilies, which allows users to write functions on types. Yet, using type functions can cripple type inference in certain situations. In particular, lack of injectivity in type functions means that GHC can never infer an instantiation of a type variable appearing only under type functions. In this paper, we describe a small modification to GHC that allows type functions to be annotated as injective. GHC naturally must check validity of the injectivity annotations. The algorithm to do so is surprisingly subtle. We prove soundness for a simplification of our algorithm, and state and prove a completeness property, though the algorithm is not fully complete. As much of our reasoning surrounds functions defined by a simple pattern-matching structure, we believe our results extend beyond just Haskell. We have implemented our solution on a branch of GHC and plan to make it available to regular users with the next stable release of the compiler.},
	urldate = {2015-09-03},
	booktitle = {Proceedings of the 8th {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Stolarek, Jan and Peyton Jones, Simon and Eisenberg, Richard A.},
	year = {2015},
	keywords = {Functional dependencies, haskell, injectivity, type families, type-level programming},
	pages = {118--128},
	file = {stolarek_et_al_2015_injective_type_families_for_haskell.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/BW78BVX4/stolarek_et_al_2015_injective_type_families_for_haskell.pdf:application/pdf}
}

@inproceedings{xu_static_2009,
	title = {Static {Contract} {Checking} for {Haskell}},
	abstract = {Program errors are hard to detect and are costly both to programmers who spend significant efforts in debugging, and for systems that are guarded by runtime checks. Static verification techniques have been applied to imperative and object-oriented languages, like Java and C\#, but few have been applied to a higher-order lazy functional language, like Haskell. In this paper, we describe a sound and automatic static verification framework for Haskell, that is based on contracts and symbolic execution. Our approach is modular and gives precise blame assignments at compile-time in the presence of higher-order functions and laziness. D.3 [Software]: Program-},
	booktitle = {In {Proceedings} of the 36 th {Annual} {ACM} {Symposium} on the {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Xu, Dana N. and Jones, Simon Peyton and Claessen, Koen},
	year = {2009},
	pages = {41--52},
	file = {xu_et_al_2009_static_contract_checking_for_haskell.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/ZEQ4UZT4/xu_et_al_2009_static_contract_checking_for_haskell.pdf:application/pdf}
}

@inproceedings{bayne_always-available_2011,
	address = {New York, NY, USA},
	series = {{ICSE} '11},
	title = {Always-available {Static} and {Dynamic} {Feedback}},
	isbn = {978-1-4503-0445-0},
	url = {http://doi.acm.org/10.1145/1985793.1985864},
	doi = {10.1145/1985793.1985864},
	abstract = {Developers who write code in a statically typed language are denied the ability to obtain dynamic feedback by executing their code during periods when it fails the static type checker. They are further confined to the static typing discipline during times in the development process where it does not yield the highest productivity. If they opt instead to use a dynamic language, they forgo the many benefits of static typing, including machine-checked documentation, improved correctness and reliability, tool support (such as for refactoring), and better runtime performance. We present a novel approach to giving developers the benefits of both static and dynamic typing, throughout the development process, and without the burden of manually separating their program into statically and dynamically-typed parts. Our approach, which is intended for temporary use during the development process, relaxes the static type system and provides a semantics for many type-incorrect programs. It defers type errors to run time, or suppresses them if they do not affect runtime semantics. We implemented our approach in a publicly available tool, DuctileJ, for the Java language. In case studies, DuctileJ conferred benefits both during prototyping and during the evolution of existing code.},
	urldate = {2015-10-27},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Software} {Engineering}},
	publisher = {ACM},
	author = {Bayne, Michael and Cook, Richard and Ernst, Michael D.},
	year = {2011},
	keywords = {dynamic typing, gradual typing, hybrid typing, productivity, prototyping, refactoring, static typing, type error},
	pages = {521--530},
	file = {bayne_et_al_2011_always-available_static_and_dynamic_feedback.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/UHE57TMX/bayne_et_al_2011_always-available_static_and_dynamic_feedback.pdf:application/pdf}
}

@inproceedings{marinov_testera:_2001,
	title = {{TestEra}: {A} novel framework for automated testing of {Java} programs},
	shorttitle = {{TestEra}},
	url = {http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=989787},
	urldate = {2014-09-09},
	booktitle = {Automated {Software} {Engineering}, 2001.({ASE} 2001). {Proceedings}. 16th {Annual} {International} {Conference} on},
	publisher = {IEEE},
	author = {Marinov, Darko and Khurshid, Sarfraz},
	year = {2001},
	keywords = {\_tablet},
	pages = {22--31},
	file = {marinov_khurshid_2001_testera.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/VMQIRMWZ/marinov_khurshid_2001_testera.pdf:application/pdf}
}

@phdthesis{nelson_techniques_1980,
	address = {Stanford, CA, USA},
	title = {Techniques for {Program} {Verification}},
	school = {Stanford University},
	author = {Nelson, Charles Gregory},
	year = {1980},
	note = {AAI8011683}
}

@inproceedings{majumdar_hybrid_2007,
	address = {Washington, DC, USA},
	series = {{ICSE} '07},
	title = {Hybrid {Concolic} {Testing}},
	isbn = {0-7695-2828-7},
	url = {http://dx.doi.org/10.1109/ICSE.2007.41},
	doi = {10.1109/ICSE.2007.41},
	abstract = {We present hybrid concolic testing, an algorithm that interleaves random testing with concolic execution to obtain both a deep and a wide exploration of program state space. Our algorithm generates test inputs automatically by interleaving random testing until saturation with bounded exhaustive symbolic exploration of program points. It thus combines the ability of random search to reach deep program states quickly together with the ability of concolic testing to explore states in a neighborhood exhaustively. We have implemented our algorithm on top of CUTE and applied it to obtain better branch coverage for an editor implementation (VIM 5.7, 150K lines of code) as well as a data structure implementation in C. Our experiments suggest that hybrid concolic testing can handle large programs and provide, for the same testing budget, almost 4× the branch coverage than random testing and almost 2× that of concolic testing.},
	urldate = {2014-06-16},
	booktitle = {Proceedings of the 29th {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Majumdar, Rupak and Sen, Koushik},
	year = {2007},
	keywords = {\_tablet, concolic testing., directed random testing, research-exam},
	pages = {416--426},
	file = {majumdar_sen_2007_hybrid_concolic_testing.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/J89UPJ22/majumdar_sen_2007_hybrid_concolic_testing.pdf:application/pdf}
}

@inproceedings{kim_automatic_2013,
	address = {Piscataway, NJ, USA},
	series = {{ICSE} '13},
	title = {Automatic {Patch} {Generation} {Learned} from {Human}-written {Patches}},
	isbn = {978-1-4673-3076-3},
	url = {http://dl.acm.org/citation.cfm?id=2486788.2486893},
	abstract = {Patch generation is an essential software maintenance task because most software systems inevitably have bugs that need to be fixed. Unfortunately, human resources are often insufficient to fix all reported and known bugs. To address this issue, several automated patch generation techniques have been proposed. In particular, a genetic-programming-based patch generation technique, GenProg, proposed by Weimer et al., has shown promising results. However, these techniques can generate nonsensical patches due to the randomness of their mutation operations. To address this limitation, we propose a novel patch generation approach, Pattern-based Automatic program Repair (PAR), using fix patterns learned from existing human-written patches. We manually inspected more than 60,000 human-written patches and found there are several common fix patterns. Our approach leverages these fix patterns to generate program patches automatically. We experimentally evaluated PAR on 119 real bugs. In addition, a user study involving 89 students and 164 developers confirmed that patches generated by our approach are more acceptable than those generated by GenProg. PAR successfully generated patches for 27 out of 119 bugs, while GenProg was successful for only 16 bugs.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 2013 {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Press},
	author = {Kim, Dongsun and Nam, Jaechang and Song, Jaewoo and Kim, Sunghun},
	year = {2013},
	pages = {802--811},
	file = {kim_et_al_2013_automatic_patch_generation_learned_from_human-written_patches.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/NABXI3XF/kim_et_al_2013_automatic_patch_generation_learned_from_human-written_patches.pdf:application/pdf}
}

@inproceedings{godefroid_compositional_2007,
	address = {New York, NY, USA},
	series = {{POPL} '07},
	title = {Compositional {Dynamic} {Test} {Generation}},
	isbn = {1-59593-575-4},
	url = {http://doi.acm.org/10.1145/1190216.1190226},
	doi = {10.1145/1190216.1190226},
	abstract = {Dynamic test generation is a form of dynamic program analysis that attempts to compute test inputs to drive a program along a specific program path. Directed Automated Random Testing, or DART for short, blends dynamic test generation with model checking techniques with the goal of systematically executing all feasible program paths of a program while detecting various types of errors using run-time checking tools (like Purify, for instance). Unfortunately, systematically executing all feasible program paths does not scale to large, realistic programs.This paper addresses this major limitation and proposes to perform dynamic test generation compositionally, by adapting known techniques for interprocedural static analysis. Specifically, we introduce a new algorithm, dubbed SMART for Systematic Modular Automated Random Testing, that extends DART by testing functions in isolation, encoding test results as function summaries expressed using input preconditions and output postconditions, and then re-using those summaries when testing higher-level functions. We show that, for a fixed reasoning capability, our compositional approach to dynamic test generation (SMART) is both sound and complete compared to monolithic dynamic test generation (DART). In other words, SMART can perform dynamic test generation compositionally without any reduction in program path coverage. We also show that, given a bound on the maximum number of feasible paths in individual program functions, the number of program executions explored by SMART is linear in that bound, while the number of program executions explored by DART can be exponential in that bound. We present examples of C programs and preliminary experimental results that illustrate and validate empirically these properties.},
	urldate = {2015-01-23},
	booktitle = {Proceedings of the 34th {Annual} {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Godefroid, Patrice},
	year = {2007},
	keywords = {automatic test generation, compositional program analysis, program verification, scalability, software testing},
	pages = {47--54},
	file = {godefroid_2007_compositional_dynamic_test_generation.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/3V8679H2/godefroid_2007_compositional_dynamic_test_generation.pdf:application/pdf}
}

@inproceedings{rondon_liquid_2008,
	address = {New York, NY, USA},
	series = {{PLDI} '08},
	title = {Liquid {Types}},
	isbn = {978-1-59593-860-2},
	url = {http://doi.acm.org/10.1145/1375581.1375602},
	doi = {10.1145/1375581.1375602},
	abstract = {We present Logically Qualified Data Types, abbreviated to Liquid Types, a system that combines Hindley-Milner type inference with Predicate Abstraction to automatically infer dependent types precise enough to prove a variety of safety properties. Liquid types allow programmers to reap many of the benefits of dependent types, namely static verification of critical properties and the elimination of expensive run-time checks, without the heavy price of manual annotation. We have implemented liquid type inference in DSOLVE, which takes as input an OCAML program and a set of logical qualifiers and infers dependent types for the expressions in the OCAML program. To demonstrate the utility of our approach, we describe experiments using DSOLVE to statically verify the safety of array accesses on a set of OCAML benchmarks that were previously annotated with dependent types as part of the DML project. We show that when used in conjunction with a fixed set of array bounds checking qualifiers, DSOLVE reduces the amount of manual annotation required for proving safety from 31\% of program text to under 1\%.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 29th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Rondon, Patrick M. and Kawaguci, Ming and Jhala, Ranjit},
	year = {2008},
	keywords = {dependent types, hindley-milner, predicate abstraction, type inference},
	pages = {159--169},
	file = {rondon_et_al_2008_liquid_types.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/ZH2BBMVF/rondon_et_al_2008_liquid_types.pdf:application/pdf}
}

@inproceedings{okabe_systems_2014,
	address = {New York, NY, USA},
	series = {Haskell '14},
	title = {Systems {Demonstration}: {Writing} {NetBSD} {Sound} {Drivers} in {Haskell}},
	isbn = {978-1-4503-3041-1},
	shorttitle = {Systems {Demonstration}},
	url = {http://doi.acm.org/10.1145/2633357.2633370},
	doi = {10.1145/2633357.2633370},
	abstract = {Most strongly typed, functional programming languages are not equipped with a reentrant garbage collector. Therefore such languages are not used for operating systems programming, where the virtues of types are most desired. We propose the use of Context-Local Heaps (CLHs) to achieve reentrancy, which also increasing the speed of garbage collection. We have implemented CLHs in Ajhc, a Haskell compiler derived from jhc, rewritten some NetBSD sound drivers using Ajhc, and benchmarked them. The reentrant, faster garbage collection that CLHs provide opens the path to type-assisted operating systems programming.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 2014 {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Okabe, Kiwamu and Muranushi, Takayuki},
	year = {2014},
	keywords = {languages, performance},
	pages = {77--78},
	file = {okabe_muranushi_2014_systems_demonstration.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/6KSWDKKN/okabe_muranushi_2014_systems_demonstration.pdf:application/pdf}
}

@inproceedings{chen_error-tolerant_2012,
	address = {New York, NY, USA},
	series = {{ICFP} '12},
	title = {An {Error}-tolerant {Type} {System} for {Variational} {Lambda} {Calculus}},
	isbn = {978-1-4503-1054-3},
	url = {http://doi.acm.org/10.1145/2364527.2364535},
	doi = {10.1145/2364527.2364535},
	abstract = {Conditional compilation and software product line technologies make it possible to generate a huge number of different programs from a single software project. Typing each of these programs individually is usually impossible due to the sheer number of possible variants. Our previous work has addressed this problem with a type system for variational lambda calculus (VLC), an extension of lambda calculus with basic constructs for introducing and organizing variation. Although our type inference algorithm is more efficient than the brute-force strategy of inferring the types of each variant individually, it is less robust since type inference will fail for the entire variational expression if any one variant contains a type error. In this work, we extend our type system to operate on VLC expressions containing type errors. This extension directly supports locating ill-typed variants and the incremental development of variational programs. It also has many subtle implications for the unification of variational types. We show that our extended type system possesses a principal typing property and that the underlying unification problem is unitary. Our unification algorithm computes partial unifiers that lead to result types that (1) contain errors in as few variants as possible and (2) are most general. Finally, we perform an empirical evaluation to determine the overhead of this extension compared to our previous work, to demonstrate the improvements over the brute-force approach, and to explore the effects of various error distributions on the inference process.},
	urldate = {2015-05-26},
	booktitle = {Proceedings of the 17th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Chen, Sheng and Erwig, Martin and Walkingshaw, Eric},
	year = {2012},
	keywords = {error-tolerant type systems, variational lambda calculus, variational type inference, variational types},
	pages = {29--40},
	file = {chen_et_al_2012_an_error-tolerant_type_system_for_variational_lambda_calculus.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/JKZ9ZB7W/chen_et_al_2012_an_error-tolerant_type_system_for_variational_lambda_calculus.pdf:application/pdf}
}

@article{blackburn_wake_2008,
	title = {Wake {Up} and {Smell} the {Coffee}: {Evaluation} {Methodology} for the 21st {Century}},
	volume = {51},
	issn = {0001-0782},
	shorttitle = {Wake {Up} and {Smell} the {Coffee}},
	url = {http://doi.acm.org/10.1145/1378704.1378723},
	doi = {10.1145/1378704.1378723},
	number = {8},
	urldate = {2015-04-29},
	journal = {Commun. ACM},
	author = {Blackburn, Stephen M. and McKinley, Kathryn S. and Garner, Robin and Hoffmann, Chris and Khan, Asjad M. and Bentzur, Rotem and Diwan, Amer and Feinberg, Daniel and Frampton, Daniel and Guyer, Samuel Z. and Hirzel, Martin and Hosking, Antony and Jump, Maria and Lee, Han and Moss, J. Eliot B. and Phansalkar, Aashish and Stefanovik, Darko and VanDrunen, Thomas and von Dincklage, Daniel and Wiedermann, Ben},
	month = aug,
	year = {2008},
	pages = {83--89},
	file = {blackburn_et_al_2008_wake_up_and_smell_the_coffee.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/5D9SX67Z/blackburn_et_al_2008_wake_up_and_smell_the_coffee.pdf:application/pdf}
}

@misc{_spoofax_????,
	title = {The {Spoofax} {Language} {Workbench}},
	url = {http://metaborg.org/spoofax/},
	abstract = {All about Spoofax},
	urldate = {2015-03-16},
	journal = {MetaBorg},
	file = {Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/ATP5S6PU/spoofax.html:text/html}
}

@inproceedings{perera_functional_2012,
	address = {New York, NY, USA},
	series = {{ICFP} '12},
	title = {Functional {Programs} {That} {Explain} {Their} {Work}},
	isbn = {978-1-4503-1054-3},
	url = {http://doi.acm.org/10.1145/2364527.2364579},
	doi = {10.1145/2364527.2364579},
	abstract = {We present techniques that enable higher-order functional computations to "explain" their work by answering questions about how parts of their output were calculated. As explanations, we consider the traditional notion of program slices, which we show can be inadequate, and propose a new notion: trace slices. We present techniques for specifying flexible and rich slicing criteria based on partial expressions, parts of which have been replaced by holes. We characterise program slices in an algorithm-independent fashion and show that a least slice for a given criterion exists. We then present an algorithm, called unevaluation, for computing least program slices from computations reified as traces. Observing a limitation of program slices, we develop a notion of trace slice as another form of explanation and present an algorithm for computing them. The unevaluation algorithm can be applied to any subtrace of a trace slice to compute a program slice whose evaluation generates that subtrace. This close correspondence between programs, traces, and their slices can enable the programmer to understand a computation interactively, in terms of the programming language in which the computation is expressed. We present an implementation in the form of a tool, discuss some important practical implementation concerns and present some techniques for addressing them.},
	urldate = {2015-01-08},
	booktitle = {Proceedings of the 17th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Perera, Roly and Acar, Umut A. and Cheney, James and Levy, Paul Blain},
	year = {2012},
	keywords = {debugging, program slicing, provenance},
	pages = {365--376},
	file = {perera_et_al_2012_functional_programs_that_explain_their_work.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/T49TERWH/perera_et_al_2012_functional_programs_that_explain_their_work.pdf:application/pdf;perera_et_al_2012_functional_programs_that_explain_their_work.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/RCZUAB42/perera_et_al_2012_functional_programs_that_explain_their_work.pdf:application/pdf}
}

@inproceedings{ray_large_2014,
	address = {New York, NY, USA},
	series = {{FSE} 2014},
	title = {A {Large} {Scale} {Study} of {Programming} {Languages} and {Code} {Quality} in {Github}},
	isbn = {978-1-4503-3056-5},
	url = {http://doi.acm.org/10.1145/2635868.2635922},
	doi = {10.1145/2635868.2635922},
	abstract = {What is the effect of programming languages on software quality? This question has been a topic of much debate for a very long time. In this study, we gather a very large data set from GitHub (729 projects, 80 Million SLOC, 29,000 authors, 1.5 million commits, in 17 languages) in an attempt to shed some empirical light on this question. This reasonably large sample size allows us to use a mixed-methods approach, combining multiple regression modeling with visualization and text analytics, to study the effect of language features such as static v.s. dynamic typing, strong v.s. weak typing on software quality. By triangulating findings from different methods, and controlling for confounding effects such as team size, project size, and project history, we report that language design does have a significant, but modest effect on software quality. Most notably, it does appear that strong typing is modestly better than weak typing, and among functional languages, static typing is also somewhat better than dynamic typing. We also find that functional languages are somewhat better than procedural languages. It is worth noting that these modest effects arising from language design are overwhelmingly dominated by the process factors such as project size, team size, and commit size. However, we hasten to caution the reader that even these modest effects might quite possibly be due to other, intangible process factors, e.g., the preference of certain personality types for functional, static and strongly typed languages.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 22Nd {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Ray, Baishakhi and Posnett, Daryl and Filkov, Vladimir and Devanbu, Premkumar},
	year = {2014},
	keywords = {bug fix, code quality, empirical research, programming language, regression analysis, software domain, type system},
	pages = {155--165},
	file = {ray_et_al_2014_a_large_scale_study_of_programming_languages_and_code_quality_in_github.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/524XHH3W/ray_et_al_2014_a_large_scale_study_of_programming_languages_and_code_quality_in_github.pdf:application/pdf}
}

@inproceedings{allwood_finding_2009,
	address = {New York, NY, USA},
	series = {Haskell '09},
	title = {Finding the {Needle}: {Stack} {Traces} for {GHC}},
	isbn = {978-1-60558-508-6},
	shorttitle = {Finding the {Needle}},
	url = {http://doi.acm.org/10.1145/1596638.1596654},
	doi = {10.1145/1596638.1596654},
	abstract = {Even Haskell programs can occasionally go wrong. Programs calling head on an empty list, and incomplete patterns in function definitions can cause program crashes, reporting little more than the precise location where error was ultimately called. Being told that one application of the head function in your program went wrong, without knowing which use of head went wrong can be infuriating. We present our work on adding the ability to get stack traces out of GHC, for example that our crashing head was used during the evaluation of foo, which was called during the evaluation of bar, during the evaluation of main. We provide a transformation that converts GHC Core programs into ones that pass a stack around, and a stack library that ensures bounded heap usage despite the highly recursive nature of Haskell. We call our extension to GHC StackTrace.},
	urldate = {2015-05-29},
	booktitle = {Proceedings of the 2Nd {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Allwood, Tristan O.R. and Peyton Jones, Simon and Eisenbach, Susan},
	year = {2009},
	keywords = {debugging, stack trace},
	pages = {129--140},
	file = {allwood_et_al_2009_finding_the_needle.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/CNCU4A77/allwood_et_al_2009_finding_the_needle.pdf:application/pdf}
}

@inproceedings{manevich_pse:_2004,
	address = {New York, NY, USA},
	series = {{SIGSOFT} '04/{FSE}-12},
	title = {{PSE}: {Explaining} {Program} {Failures} via {Postmortem} {Static} {Analysis}},
	isbn = {1-58113-855-5},
	shorttitle = {{PSE}},
	url = {http://doi.acm.org/10.1145/1029894.1029907},
	doi = {10.1145/1029894.1029907},
	abstract = {In this paper, we describe PSE (Postmortem Symbolic Evaluation), a static analysis algorithm that can be used by programmers to diagnose software failures. The algorithm requires minimal information about a failure, namely its kind (e.g. NULL dereference), and its location in the program's source code. It produces a set of execution traces along which the program can be driven to the given failure. PSE tracks the flow of a single value of interest from the point in the program where the failure occurred back to the points in the program where the value may have originated. The algorithm combines a novel dataflow analysis and memory alias analysis in a manner that allows for precise exploration of the program's behavior in polynomial time. We have applied PSE to the problem of diagnosing potential NULL-dereference errors in a suite of C programs, including several SPEC benchmarks and a large commercial operating system. In most cases, the analysis is able to either validate a pointer dereference, or find precise error traces demonstrating a NULL value for the pointer, in less than a second.},
	urldate = {2015-06-23},
	booktitle = {Proceedings of the 12th {ACM} {SIGSOFT} {Twelfth} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Manevich, Roman and Sridharan, Manu and Adams, Stephen and Das, Manuvir and Yang, Zhe},
	year = {2004},
	keywords = {alias analysis, postmortem analysis, typestate, value flow},
	pages = {63--72},
	file = {manevich_et_al_2004_pse.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/7ZJX4H8A/manevich_et_al_2004_pse.pdf:application/pdf}
}

@incollection{sparud_complete_1997,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Complete and partial redex trails of functional computations},
	copyright = {©1998 Springer-Verlag},
	isbn = {978-3-540-64849-9 978-3-540-68528-9},
	url = {http://link.springer.com/chapter/10.1007/BFb0055430},
	abstract = {Redex trails are histories of functional computations by graph reduction; their main application is fault-tracing. A prototype implementation of a tracer based on redex trails [8] demonstrated the promise of the technique, but was limited in two respects: (1) trails did not record every reduction, only those constructing a new value; (2) even so computing trails was very expensive, particularly in terms of the memory space they occupied. In this paper, we address both problems: complete redex trails provide a full computational record; partial versions of these trails exclude all but selected details, greatly reducing memory costs. We include results of experiments tracing several applications, including a compiler.},
	language = {en},
	number = {1467},
	urldate = {2015-10-29},
	booktitle = {Implementation of {Functional} {Languages}},
	publisher = {Springer Berlin Heidelberg},
	author = {Sparud, Jan and Runciman, Colin},
	editor = {Clack, Chris and Hammond, Kevin and Davie, Tony},
	month = sep,
	year = {1997},
	note = {DOI: 10.1007/BFb0055430},
	keywords = {Logics and Meanings of Programs, Programming Languages, Compilers, Interpreters, Programming Techniques},
	pages = {160--177},
	file = {Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/WXUMCQS3/10.html:text/html;sparud_runciman_1997_complete_and_partial_redex_trails_of_functional_computations.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/E2G9GJ83/sparud_runciman_1997_complete_and_partial_redex_trails_of_functional_computations.pdf:application/pdf}
}

@incollection{distefano_local_2006,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Local} {Shape} {Analysis} {Based} on {Separation} {Logic}},
	copyright = {©2006 Springer Berlin Heidelberg},
	isbn = {978-3-540-33056-1 978-3-540-33057-8},
	url = {http://link.springer.com/chapter/10.1007/11691372_19},
	abstract = {We describe a program analysis for linked list programs where the abstract domain uses formulae from separation logic.},
	language = {en},
	number = {3920},
	urldate = {2015-06-11},
	booktitle = {Tools and {Algorithms} for the {Construction} and {Analysis} of {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Distefano, Dino and O’Hearn, Peter W. and Yang, Hongseok},
	editor = {Hermanns, Holger and Palsberg, Jens},
	year = {2006},
	keywords = {Algorithm Analysis and Problem Complexity, Computer Communication Networks, Logics and Meanings of Programs, Software Engineering},
	pages = {287--302},
	file = {distefano_et_al_2006_a_local_shape_analysis_based_on_separation_logic.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/4HWMHHWV/distefano_et_al_2006_a_local_shape_analysis_based_on_separation_logic.pdf:application/pdf}
}

@inproceedings{chugh_nested_2012,
	address = {New York, NY, USA},
	series = {{POPL} '12},
	title = {Nested {Refinements}: {A} {Logic} for {Duck} {Typing}},
	isbn = {978-1-4503-1083-3},
	shorttitle = {Nested {Refinements}},
	url = {http://doi.acm.org/10.1145/2103656.2103686},
	doi = {10.1145/2103656.2103686},
	abstract = {Programs written in dynamic languages make heavy use of features --- run-time type tests, value-indexed dictionaries, polymorphism, and higher-order functions --- that are beyond the reach of type systems that employ either purely syntactic or purely semantic reasoning. We present a core calculus, System D, that merges these two modes of reasoning into a single powerful mechanism of nested refinement types wherein the typing relation is itself a predicate in the refinement logic. System D coordinates SMT-based logical implication and syntactic subtyping to automatically typecheck sophisticated dynamic language programs. By coupling nested refinements with McCarthy's theory of finite maps, System D can precisely reason about the interaction of higher-order functions, polymorphism, and dictionaries. The addition of type predicates to the refinement logic creates a circularity that leads to unique technical challenges in the metatheory, which we solve with a novel stratification approach that we use to prove the soundness of System D.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 39th {Annual} {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Chugh, Ravi and Rondon, Patrick M. and Jhala, Ranjit},
	year = {2012},
	keywords = {dynamic languages, refinement types},
	pages = {231--244},
	file = {chugh_et_al_2012_nested_refinements.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/FWCHI7ZI/chugh_et_al_2012_nested_refinements.pdf:application/pdf}
}

@incollection{christiansen_easycheck_2008,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{EasyCheck} — {Test} {Data} for {Free}},
	copyright = {©2008 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-78968-0 978-3-540-78969-7},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-78969-7_23},
	abstract = {We present a lightweight, automated tool for specification-based testing of declarative programs written in the functional logic programming language Curry and emphasize the usefulness of logic features in its implementation and use. Free variables, nondeterminism and encapsulated search turn out to be elegant and powerful means to express test-data generation.},
	language = {en},
	number = {4989},
	urldate = {2015-06-26},
	booktitle = {Functional and {Logic} {Programming}},
	publisher = {Springer Berlin Heidelberg},
	author = {Christiansen, Jan and Fischer, Sebastian},
	editor = {Garrigue, Jacques and Hermenegildo, Manuel V.},
	year = {2008},
	keywords = {Artificial Intelligence (incl. Robotics), Curry, Encapsulated Search, Logics and Meanings of Programs, Mathematical Logic and Formal Languages, Nondeterminism, Programming Languages, Compilers, Interpreters, Programming Techniques, Testing},
	pages = {322--336},
	file = {christiansen_fischer_2008_easycheck_—_test_data_for_free.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/DR9ES75U/christiansen_fischer_2008_easycheck_—_test_data_for_free.pdf:application/pdf}
}

@inproceedings{van_noort_ad-hoc_2010,
	address = {New York, NY, USA},
	series = {{WGP} '10},
	title = {Ad-hoc {Polymorphism} and {Dynamic} {Typing} in a {Statically} {Typed} {Functional} {Language}},
	isbn = {978-1-4503-0251-7},
	url = {http://doi.acm.org/10.1145/1863495.1863505},
	doi = {10.1145/1863495.1863505},
	abstract = {Static typing in functional programming languages such as Clean, Haskell, and ML is highly beneficial: it prevents erroneous behaviour at run time and provides opportunities for optimisations. However, dynamic typing is just as important as sometimes types are not known until run time. Examples are exchanging values between applications by deserialisation from disk, input provided by a user, or obtaining values via a network connection. Ideally, a static typing system works in close harmony with an orthogonal dynamic typing system; not discriminating between statically and dynamically typed values. In contrast to Haskell's minimal support for dynamic typing, Clean has an extensive dynamic typing; it adopted ML's support for monomorphism and parametric polymorphism and added the notion of type dependencies. Unfortunately, ad-hoc polymorphism has been left out of the equation over the years. While both ad-hoc polymorphism and dynamic typing have been studied in-depth earlier, their interaction in a statically typed functional language has not been studied before. In this paper we explore the design space of their interactions.},
	urldate = {2015-05-29},
	booktitle = {Proceedings of the 6th {ACM} {SIGPLAN} {Workshop} on {Generic} {Programming}},
	publisher = {ACM},
	author = {van Noort, Thomas and Achten, Peter and Plasmeijer, Rinus},
	year = {2010},
	keywords = {ad-hoc polymorphism, dynamic typing},
	pages = {73--84},
	file = {van_noort_et_al_2010_ad-hoc_polymorphism_and_dynamic_typing_in_a_statically_typed_functional_language.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/EZTJ7W3F/van_noort_et_al_2010_ad-hoc_polymorphism_and_dynamic_typing_in_a_statically_typed_functional_language.pdf:application/pdf}
}

@inproceedings{allen_component_2010,
	title = {Component specification in the {Cactus} {Framework}: {The} {Cactus} {Configuration} {Language}},
	isbn = {978-1-4244-9347-0},
	url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5698008},
	doi = {10.1109/GRID.2010.5698008},
	abstract = {Component frameworks are complex systems that rely on many layers of abstraction to function properly. One essential requirement is a consistent means of describing each individual component and how it relates to both other components and the whole f...},
	booktitle = {{GRID}},
	publisher = {IEEE},
	author = {Allen, Gabrielle and Löffler, Frank and Schnetter, Erik and Seidel, Eric L},
	year = {2010},
	keywords = {\_tablet},
	pages = {359--368},
	file = {allen_et_al_2010_component_specification_in_the_cactus_framework.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/Q5FN9BP4/allen_et_al_2010_component_specification_in_the_cactus_framework.pdf:application/pdf}
}

@inproceedings{kaki_relational_2014,
	address = {New York, NY, USA},
	series = {{ICFP} '14},
	title = {A {Relational} {Framework} for {Higher}-order {Shape} {Analysis}},
	isbn = {978-1-4503-2873-9},
	url = {http://doi.acm.org/10.1145/2628136.2628159},
	doi = {10.1145/2628136.2628159},
	abstract = {We propose the integration of a relational specification framework within a dependent type system capable of verifying complex invariants over the shapes of algebraic datatypes. Our approach is based on the observation that structural properties of such datatypes can often be naturally expressed as inductively-defined relations over the recursive structure evident in their definitions. By interpreting constructor applications (abstractly) in a relational domain, we can define expressive relational abstractions for a variety of complex data structures, whose structural and shape invariants can be automatically verified. Our specification language also allows for definitions of parametricrelations for polymorphic data types that enable highly composable specifications and naturally generalizes to higher-order polymorphic functions. We describe an algorithm that translates relational specifications into a decidable fragment of first-order logic that can be efficiently discharged by an SMT solver. We have implemented these ideas in a type checker called CATALYST that is incorporated within the MLton SML compiler. Experimental results and case studies indicate that our verification strategy is both practical and effective.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 19th {ACM} {SIGPLAN} {International} {Conference} on {Functional} {Programming}},
	publisher = {ACM},
	author = {Kaki, Gowtham and Jagannathan, Suresh},
	year = {2014},
	keywords = {decidability, dependent types, inductive relations, parametric relations, relational specifications, standard ml},
	pages = {311--324},
	file = {kaki_jagannathan_2014_a_relational_framework_for_higher-order_shape_analysis.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/UXCDKBSQ/kaki_jagannathan_2014_a_relational_framework_for_higher-order_shape_analysis.pdf:application/pdf}
}

@incollection{griswold_coping_2001,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Coping with {Crosscutting} {Software} {Changes} {Using} {Information} {Transparency}},
	copyright = {©2001 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-42618-9 978-3-540-45429-8},
	url = {http://link.springer.com/chapter/10.1007/3-540-45429-2_17},
	abstract = {Designers are often unsuccessful in designing for change using traditional modularity techniques. A complementary modularity technique called information transparency can improve a designer’s ability to simplify changes by exposing the interdependence of dispersed program elements that must be changed together for correctness. Information transparency represents modules via similarity and architecture, rather than locality and abstraction.With these, a programmer can create locality with a software tool, easing change in much the same way as traditional modularity. When combined with information hiding, then, more complex module structures can be represented. Information transparency techniques include naming conventions, formatting style, and ordering of code in a file. Transparency can be increased by better matching tool capabilities and programming style. We discuss applications of information transparency and introduce design principles for software designers and tool designers.},
	language = {en},
	number = {2192},
	urldate = {2015-06-26},
	booktitle = {Metalevel {Architectures} and {Separation} of {Crosscutting} {Concerns}},
	publisher = {Springer Berlin Heidelberg},
	author = {Griswold, William G.},
	editor = {Yonezawa, Akinori and Matsuoka, Satoshi},
	year = {2001},
	keywords = {Computer Communication Networks, design, implementation techniques, Logics and Meanings of Programs, Modularity, Operating Systems, Programming Languages, Compilers, Interpreters, programming methodology, Programming Techniques, Software Engineering, software evolution, software maintenance},
	pages = {250--265},
	file = {griswold_2001_coping_with_crosscutting_software_changes_using_information_transparency.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/CHQ47VNF/griswold_2001_coping_with_crosscutting_software_changes_using_information_transparency.pdf:application/pdf}
}

@book{bourque_swebok:_2014,
	address = {[Los Alamitos, CA]},
	title = {{SWEBOK}: guide to the software engineering body of knowledge},
	isbn = {978-0-7695-5166-1 0-7695-5166-1},
	shorttitle = {{SWEBOK}},
	language = {English},
	publisher = {IEEE Computer Society},
	author = {Bourque, Pierre and Fairley, R. E and {IEEE Computer Society}},
	year = {2014},
	file = {bourque_et_al_2014_swebok.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/D9ERMRXK/bourque_et_al_2014_swebok.pdf:application/pdf}
}

@article{tirronen_understanding_2015,
	title = {Understanding beginners' mistakes with {Haskell}},
	volume = {25},
	issn = {1469-7653},
	url = {http://journals.cambridge.org/article_S0956796815000179},
	doi = {10.1017/S0956796815000179},
	abstract = {This article presents an overview of student difficulties in an introductory functional programming (FP) course taught in Haskell. The motivation for this study stems from our belief that many student difficulties can be alleviated by understanding the underlying causes of errors and by modifying the educational approach and, possibly, the teaching language accordingly. We analyze students' exercise submissions and categorize student errors according to compiler error messages and then manually according to the observed underlying cause. Our study complements earlier studies on the topic by applying computer and manual analysis while focusing on providing descriptive statistics of difficulties specific to FP languages. We conclude that the majority of student errors, regardless of cause, are reported by three different compiler error messages that are not well understood by students. In addition, syntactic features, such as precedence, the syntax of function application, and deeply nested statements, cause difficulties throughout the course.},
	urldate = {2015-10-22},
	journal = {Journal of Functional Programming},
	author = {Tirronen, Ville and Uusi-Mäkelä, Samuel and Isomöttönen, Ville},
	year = {2015},
	file = {Cambridge Journals Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/BVI4NMAC/displayAbstract.html:text/html;tirronen_et_al_2015_understanding_beginners'_mistakes_with_haskell.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/I4SDJCQJ/tirronen_et_al_2015_understanding_beginners'_mistakes_with_haskell.pdf:application/pdf}
}

@incollection{mycroft_polymorphic_1984,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Polymorphic type schemes and recursive definitions},
	copyright = {©1984 Springer-Verlag},
	isbn = {978-3-540-12925-7 978-3-540-38809-8},
	url = {http://link.springer.com/chapter/10.1007/3-540-12925-1_41},
	abstract = {An extension to Milner's polymorphic type system is proposed and proved correct. Such an extension appears to be necessary for the class of languages with mutually recursive top-level definitions. We can now ascribe a more general type to such definitions than before.},
	language = {en},
	number = {167},
	urldate = {2015-07-17},
	booktitle = {International {Symposium} on {Programming}},
	publisher = {Springer Berlin Heidelberg},
	author = {Mycroft, Alan},
	editor = {Paul, M. and Robinet, B.},
	year = {1984},
	keywords = {Programming Languages, Compilers, Interpreters, Software Engineering},
	pages = {217--228},
	file = {mycroft_1984_polymorphic_type_schemes_and_recursive_definitions.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/Q77K6XNQ/mycroft_1984_polymorphic_type_schemes_and_recursive_definitions.pdf:application/pdf;Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/GSNRBKFX/3-540-12925-1_41.html:text/html}
}

@inproceedings{ekblad_seamless_2014,
	address = {New York, NY, USA},
	series = {Haskell '14},
	title = {A {Seamless}, {Client}-centric {Programming} {Model} for {Type} {Safe} {Web} {Applications}},
	isbn = {978-1-4503-3041-1},
	url = {http://doi.acm.org/10.1145/2633357.2633367},
	doi = {10.1145/2633357.2633367},
	abstract = {We propose a new programming model for web applications which is (1) seamless; one program and one language is used to produce code for both client and server, (2) client-centric; the programmer takes the viewpoint of the client that runs code on the server rather than the other way around, (3) functional and type-safe, and (4) portable; everything is implemented as a Haskell library that implicitly takes care of all networking code. Our aim is to improve the painful and error-prone experience of today's standard development methods, in which clients and servers are coded in different languages and communicate with each other using ad-hoc protocols. We present the design of our library called Haste.App, an example web application that uses it, and discuss the implementation and the compiler technology on which it depends.},
	urldate = {2015-06-26},
	booktitle = {Proceedings of the 2014 {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Ekblad, Anton and Claessen, Koen},
	year = {2014},
	keywords = {distributed systems, network communication, web applications},
	pages = {79--89},
	file = {ekblad_claessen_2014_a_seamless,_client-centric_programming_model_for_type_safe_web_applications.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/MGSP7UQV/ekblad_claessen_2014_a_seamless,_client-centric_programming_model_for_type_safe_web_applications.pdf:application/pdf}
}

@inproceedings{faddegon_algorithmic_2015,
	address = {New York, NY, USA},
	series = {{PLDI} 2015},
	title = {Algorithmic {Debugging} of {Real}-world {Haskell} {Programs}: {Deriving} {Dependencies} from the {Cost} {Centre} {Stack}},
	isbn = {978-1-4503-3468-6},
	shorttitle = {Algorithmic {Debugging} of {Real}-world {Haskell} {Programs}},
	url = {http://doi.acm.org/10.1145/2737924.2737985},
	doi = {10.1145/2737924.2737985},
	abstract = {Existing algorithmic debuggers for Haskell require a transformation of all modules in a program, even libraries that the user does not want to debug and which may use language features not supported by the debugger. This is a pity, because a promising approach to debugging is therefore not applicable to many real-world programs. We use the cost centre stack from the Glasgow Haskell Compiler profiling environment together with runtime value observations as provided by the Haskell Object Observation Debugger (HOOD) to collect enough information for algorithmic debugging. Program annotations are in suspected modules only. With this technique algorithmic debugging is applicable to a much larger set of Haskell programs. This demonstrates that for functional languages in general a simple stack trace extension is useful to support tasks such as profiling and debugging.},
	urldate = {2015-06-15},
	booktitle = {Proceedings of the 36th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}},
	publisher = {ACM},
	author = {Faddegon, Maarten and Chitil, Olaf},
	year = {2015},
	keywords = {algorithmic debugging, haskell, lazy evaluation, tracing},
	pages = {33--42},
	file = {faddegon_chitil_2015_algorithmic_debugging_of_real-world_haskell_programs.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/ISGETC9E/faddegon_chitil_2015_algorithmic_debugging_of_real-world_haskell_programs.pdf:application/pdf}
}

@inproceedings{foner_functional_2015,
	address = {New York, NY, USA},
	series = {Haskell 2015},
	title = {Functional {Pearl}: {Getting} a {Quick} {Fix} on {Comonads}},
	isbn = {978-1-4503-3808-0},
	shorttitle = {Functional {Pearl}},
	url = {http://doi.acm.org/10.1145/2804302.2804310},
	doi = {10.1145/2804302.2804310},
	abstract = {A piece of functional programming folklore due to Piponi provides Löb's theorem from modal provability logic with a computational interpretation as an unusual fixed point. Interpreting modal necessity as an arbitrary Functor in Haskell, the "type" of Löb's theorem is inhabited by a fixed point function allowing each part of a structure to refer to the whole. However, Functor's logical interpretation may be used to prove Löb's theorem only by relying on its implicit functorial strength, an axiom not available in the provability modality. As a result, the well known loeb fixed point "cheats" by using functorial strength to implement its recursion. Rather than Functor, a closer Curry analogue to modal logic's Howard inspiration is a closed (semi-)comonad, of which Haskell's ComonadApply typeclass provides analogous structure. Its computational interpretation permits the definition of a novel fixed point function allowing each part of a structure to refer to its own context within the whole. This construction further guarantees maximal sharing and asymptotic efficiency superior to loeb for locally contextual computations upon a large class of structures. With the addition of a distributive law, closed comonads may be composed into spaces of arbitrary dimensionality while preserving the performance guarantees of this new fixed point. From these elements, we construct a small embedded domain-specific language to elegantly express and evaluate multidimensional "spreadsheet-like" recurrences for a variety of cellular automata.},
	urldate = {2015-09-03},
	booktitle = {Proceedings of the 8th {ACM} {SIGPLAN} {Symposium} on {Haskell}},
	publisher = {ACM},
	author = {Foner, Kenneth},
	year = {2015},
	keywords = {closed comonads, haskell, modal logic, spreadsheets},
	pages = {106--117},
	file = {foner_2015_functional_pearl.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/EQARVK8N/foner_2015_functional_pearl.pdf:application/pdf}
}

@inproceedings{christiansen_reflect_2014,
	title = {Reflect on your mistakes! {Lightweight} domain-specific error messages},
	booktitle = {Preproceedings of the 15th {Symposium} on {Trends} in {Functional} {Programming}},
	author = {Christiansen, David Raymond},
	year = {2014},
	file = {christiansen_2014_reflect_on_your_mistakes.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/QD9U3QX5/christiansen_2014_reflect_on_your_mistakes.pdf:application/pdf}
}

@article{cornelissen_controlled_2011,
	title = {A {Controlled} {Experiment} for {Program} {Comprehension} through {Trace} {Visualization}},
	volume = {37},
	issn = {0098-5589},
	doi = {10.1109/TSE.2010.47},
	abstract = {Software maintenance activities require a sufficient level of understanding of the software at hand that unfortunately is not always readily available. Execution trace visualization is a common approach in gaining this understanding, and among our own efforts in this context is Extravis, a tool for the visualization of large traces. While many such tools have been evaluated through case studies, there have been no quantitative evaluations to the present day. This paper reports on the first controlled experiment to quantitatively measure the added value of trace visualization for program comprehension. We designed eight typical tasks aimed at gaining an understanding of a representative subject system, and measured how a control group (using the Eclipse IDE) and an experimental group (using both Eclipse and Extravis) performed these tasks in terms of time spent and solution correctness. The results are statistically significant in both regards, showing a 22 percent decrease in time requirements and a 43 percent increase in correctness for the group using trace visualization.},
	number = {3},
	journal = {IEEE Transactions on Software Engineering},
	author = {Cornelissen, B. and Zaidman, A. and van Deursen, A.},
	month = may,
	year = {2011},
	keywords = {Computer Society, controlled experiment., Control systems, data visualisation, Documentation, dynamic analysis, execution trace visualization, Gain measurement, Performance evaluation, program comprehension, Programming, scalability, software maintenance, Time measurement, visualization},
	pages = {341--355},
	file = {cornelissen_et_al_2011_a_controlled_experiment_for_program_comprehension_through_trace_visualization.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/A2NZBB59/cornelissen_et_al_2011_a_controlled_experiment_for_program_comprehension_through_trace_visualization.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/B6582BC5/abs_all.html:text/html}
}

@article{gill_worker/wrapper_2009,
	title = {The worker/wrapper transformation},
	volume = {19},
	issn = {1469-7653},
	url = {http://journals.cambridge.org/article_S0956796809007175},
	doi = {10.1017/S0956796809007175},
	abstract = {The worker/wrapper transformation is a technique for changing the type of a computation, usually with the aim of improving its performance. It has been used by compiler writers for many years, but the technique is little known in the wider functional programming community, and has never been described precisely. In this article we explain, formalise and explore the generality of the worker/wrapper transformation. We also provide a systematic recipe for its use as an equational reasoning technique for improving the performance of programs, and illustrate the power of this recipe using a range of examples.},
	number = {02},
	urldate = {2015-10-22},
	journal = {Journal of Functional Programming},
	author = {Gill, Andy and Hutton, Graham},
	year = {2009},
	pages = {227--251},
	file = {Cambridge Journals Snapshot:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/5FEAKFKN/displayAbstract.html:text/html;gill_hutton_2009_the_worker-wrapper_transformation.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/HU4W7FWF/gill_hutton_2009_the_worker-wrapper_transformation.pdf:application/pdf}
}

@inproceedings{tucker_opium:_2007,
	address = {Washington, DC, USA},
	series = {{ICSE} '07},
	title = {{OPIUM}: {Optimal} {Package} {Install}/{Uninstall} {Manager}},
	isbn = {0-7695-2828-7},
	shorttitle = {{OPIUM}},
	url = {http://dx.doi.org/10.1109/ICSE.2007.59},
	doi = {10.1109/ICSE.2007.59},
	abstract = {Linux distributions often include package management tools such as apt-get in Debian or yum in RedHat. Using information about package dependencies and conflicts, such tools can determine how to install a new package (and its dependencies) on a system of already installed packages. Using off-the-shelf SAT solvers, pseudo-boolean solvers, and Integer Linear Programming solvers, we have developed a new package-management tool, called Opium, that improves on current tools in two ways: (1) Opium is complete, in that if there is a solution, Opium is guaranteed to find it, and (2) Opium can optimize a user-provided objective function, which could for example state that smaller packages should be preferred over larger ones. We performed a comparative study of our tool against Debian's apt-get on 600 traces of real-world package installations. We show that Opium runs fast enough to be usable, and that its completeness and optimality guarantees provide concrete benefits to end users.},
	urldate = {2014-06-12},
	booktitle = {Proceedings of the 29th {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Tucker, Chris and Shuffelton, David and Jhala, Ranjit and Lerner, Sorin},
	year = {2007},
	keywords = {\_tablet},
	pages = {178--188},
	file = {tucker_et_al_2007_opium.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/SP9M2ECT/tucker_et_al_2007_opium.pdf:application/pdf}
}

@incollection{ganesh_decision_2007,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {A {Decision} {Procedure} for {Bit}-{Vectors} and {Arrays}},
	copyright = {©2007 Springer-Verlag Berlin Heidelberg},
	isbn = {978-3-540-73367-6 978-3-540-73368-3},
	url = {http://link.springer.com/chapter/10.1007/978-3-540-73368-3_52},
	abstract = {STP is a decision procedure for the satisfiability of quantifier-free formulas in the theory of bit-vectors and arrays that has been optimized for large problems encountered in software analysis applications. The basic architecture of the procedure consists of word-level pre-processing algorithms followed by translation to SAT. The primary bottlenecks in software verification and bug finding applications are large arrays and linear bit-vector arithmetic. New algorithms based on the abstraction-refinement paradigm are presented for reasoning about large arrays. A solver for bit-vector linear arithmetic is presented that eliminates variables and parts of variables to enable other transformations, and reduce the size of the problem that is eventually received by the SAT solver. These and other algorithms have been implemented in STP, which has been heavily tested over thousands of examples obtained from several real-world applications. Experimental results indicate that the above mix of algorithms along with the overall architecture is far more effective, for a variety of applications, than a direct translation of the original formula to SAT or other comparable decision procedures.},
	language = {en},
	number = {4590},
	urldate = {2015-01-23},
	booktitle = {Computer {Aided} {Verification}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ganesh, Vijay and Dill, David L.},
	editor = {Damm, Werner and Hermanns, Holger},
	year = {2007},
	keywords = {Artificial Intelligence (incl. Robotics), Logic Design, Logics and Meanings of Programs, Mathematical Logic and Formal Languages, Software Engineering},
	pages = {519--531},
	file = {ganesh_dill_2007_a_decision_procedure_for_bit-vectors_and_arrays.pdf:/Users/gridaphobe/Library/Application Support/Firefox/Profiles/f18czy5l.default/zotero/storage/BT5X3GJG/ganesh_dill_2007_a_decision_procedure_for_bit-vectors_and_arrays.pdf:application/pdf}
}
